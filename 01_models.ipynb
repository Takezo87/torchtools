{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from torchtools.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn as nn\n",
    "import torch as torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7616])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.tensor([-2.])) * (1 - -1) + -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@gmail.com based on:\n",
    "\n",
    "# Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... & Petitjean, F. (2019). InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.\n",
    "# Official InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime\n",
    "\n",
    "\n",
    "def noop(x):\n",
    "    return x\n",
    "\n",
    "def shortcut(c_in, c_out):\n",
    "    return nn.Sequential(*[nn.Conv1d(c_in, c_out, kernel_size=1), \n",
    "                           nn.BatchNorm1d(c_out)])\n",
    "    \n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, c_in, bottleneck=32, ks=40, nb_filters=32):\n",
    "\n",
    "        super().__init__()\n",
    "        self.bottleneck = nn.Conv1d(c_in, bottleneck, 1) if bottleneck and c_in > 1 else noop\n",
    "        mts_feat = bottleneck or c_in\n",
    "        conv_layers = []\n",
    "        kss = [ks // (2**i) for i in range(3)]\n",
    "        # ensure odd kss until nn.Conv1d with padding='same' is available in pytorch 1.3\n",
    "        kss = [ksi if ksi % 2 != 0 else ksi - 1 for ksi in kss]  \n",
    "        for i in range(len(kss)):\n",
    "            conv_layers.append(\n",
    "                nn.Conv1d(mts_feat, nb_filters, kernel_size=kss[i], padding=kss[i] // 2))\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.maxpool = nn.MaxPool1d(3, stride=1, padding=1)\n",
    "        self.conv = nn.Conv1d(c_in, nb_filters, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm1d(nb_filters * 4)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = x.to(torch.float)\n",
    "        x = self.bottleneck(input_tensor)\n",
    "        for i in range(3):\n",
    "            out_ = self.conv_layers[i](x)\n",
    "            if i == 0: out = out_\n",
    "            else: out = torch.cat((out, out_), 1)\n",
    "        mp = self.conv(self.maxpool(input_tensor))\n",
    "        inc_out = torch.cat((out, mp), 1)\n",
    "        return self.act(self.bn(inc_out))\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self,c_in,bottleneck=32,ks=40,nb_filters=32,residual=True,depth=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = residual\n",
    "        self.depth = depth\n",
    "\n",
    "        #inception & residual layers\n",
    "        inc_mods = []\n",
    "        res_layers = []\n",
    "        res = 0\n",
    "        for d in range(depth):\n",
    "            inc_mods.append(\n",
    "                Inception(c_in if d == 0 else nb_filters * 4, bottleneck=bottleneck if d > 0 else 0,ks=ks,\n",
    "                          nb_filters=nb_filters))\n",
    "            if self.residual and d % 3 == 2:\n",
    "                res_layers.append(shortcut(c_in if res == 0 else nb_filters * 4, nb_filters * 4))\n",
    "                res += 1\n",
    "            else: res_layer = res_layers.append(None)\n",
    "        self.inc_mods = nn.ModuleList(inc_mods)\n",
    "        self.res_layers = nn.ModuleList(res_layers)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for d, l in enumerate(range(self.depth)):\n",
    "            x = self.inc_mods[d](x)\n",
    "            if self.residual and d % 3 == 2:\n",
    "                res = self.res_layers[d](res)\n",
    "                x += res\n",
    "                res = x\n",
    "                x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "class InceptionTime(nn.Module):\n",
    "    def __init__(self,c_in,c_out,bottleneck=32,ks=40,nb_filters=32,residual=True,depth=6):\n",
    "        super().__init__()\n",
    "        self.block = InceptionBlock(c_in,bottleneck=bottleneck,ks=ks,nb_filters=nb_filters,\n",
    "                                    residual=residual,depth=depth)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(nb_filters * 4, c_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Sigmoid(nn.Module):\n",
    "    '''\n",
    "    sigmoid layer\n",
    "    '''\n",
    "    def __init__(self, low, high):\n",
    "        super().__init__()\n",
    "        self.high, self.low = high, low\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x)*(self.high-self.low)+self.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeSgmOld(nn.Module):\n",
    "    '''\n",
    "    add a sigmoid layer to InceptionTime to get the ouput in a certain range\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        nn.Sequential()\n",
    "        self.inc = InceptionTime(n_in, n_out)\n",
    "        self.low, self.high = -1., 1.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.inc(x)) * (self.high - self.low) + self.low\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeSgm(nn.Module):\n",
    "    '''\n",
    "    add a sigmoid layer to InceptionTime to get the ouput in a certain range\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out, range=(-1,1)):\n",
    "        super().__init__()\n",
    "        self.mod = nn.Sequential(InceptionTime(n_in, n_out), Sigmoid(*range))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        return self.mod(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeD(nn.Module):\n",
    "    '''\n",
    "    add a sigmoid layer to InceptionTime to get the ouput in a certain range\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.mod = nn.Sequential(InceptionTime(n_in, n_out), Sigmoid(-1., 1.))\n",
    "        \n",
    "    def forward(self, xc, xd):\n",
    "        x = torch.cat([xc.float(), xd.float()], dim=-2)\n",
    "        x = x.float()\n",
    "#         print(f'InceptionTimeSgm dtype {x.dtype}')\n",
    "        return self.mod(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeVar(nn.Module):\n",
    "    '''\n",
    "    output mean and variance\n",
    "    regression model, sigmoid for the mean output optional\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out, meanrange=None):\n",
    "        super().__init__()\n",
    "        models  = [InceptionTime(n_in, n_out+1)]\n",
    "        if meanrange:\n",
    "            self.sigmoid = Sigmoid(*meanrange)\n",
    "        self.mod = nn.Sequential(*models)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        output = self.mod(x)\n",
    "        ## enforce positivity of sigma^2\n",
    "        ##output_sig_pos = tf.log(1 + tf.exp(output_sig)) + 1e-06\n",
    "        output[:,-1] = (output[:,-1].exp()+1).log_() + 1e-06\n",
    "        if getattr(self, 'sigmoid', None): output[:,:-1] = self.sigmoid(output[:,:-1])\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model sanity checks\n",
    "xb = torch.randn((128,10,100))\n",
    "yb = torch.rand(128,1)\n",
    "model = InceptionTimeVar(10,1)\n",
    "\n",
    "preds = model(xb)\n",
    "assert preds.shape == (128,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def nll_regression(preds, y_true, c=5):\n",
    "    '''\n",
    "    negative log likelihood loss for regression, both mu and sigma are predicted\n",
    "    \n",
    "    Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles\n",
    "    Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell, DeepMind\n",
    "\n",
    "    '''\n",
    "    \n",
    "    s1 = 0.5*preds[:,1].log() \n",
    "    s2 = 0.5*(yb.squeeze()-preds[:,0]).pow(2).div(preds[:,1])\n",
    "    loss = (s1+s2).mean() + c\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def nll_leaky_loss(preds, y_true, c=5, alpha=0.5):\n",
    "    '''\n",
    "    leaky_loss with variance\n",
    "    \n",
    "    Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles\n",
    "    Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell, DeepMind\n",
    "\n",
    "    '''\n",
    "    \n",
    "    s1 = 0.5*preds[:,1].log() \n",
    "    l1 = -F.leaky_relu(preds[:,0], alpha)*y_true.float().squeeze()\n",
    "    s2 = 0.5*(l1.div(preds[:,1]+1)) ## +1 to prevent optimizing for variance, maybe just an artifical problem\n",
    "    loss = (s1+s2).mean() + c\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9918, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nll_leaky_loss(preds, yb)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = InceptionTimeVar(10,1,meanrange=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7907, grad_fn=<AddBackward0>)\n",
      "tensor(3.7414, grad_fn=<AddBackward0>)\n",
      "tensor(3.1807, grad_fn=<AddBackward0>)\n",
      "tensor(2.5960, grad_fn=<AddBackward0>)\n",
      "tensor(1.9984, grad_fn=<AddBackward0>)\n",
      "tensor(1.3569, grad_fn=<AddBackward0>)\n",
      "tensor(0.5428, grad_fn=<AddBackward0>)\n",
      "tensor(-0.8033, grad_fn=<AddBackward0>)\n",
      "tensor(-3.9921, grad_fn=<AddBackward0>)\n",
      "tensor(-15.4558, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#simple training loop\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "loss_fn = nll_regression\n",
    "m = model_var\n",
    "loss_fn = nll_leaky_loss\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for p in m.parameters():\n",
    "            p.sub_(lr*p.grad)\n",
    "    m.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.2576)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unweighted_profit(m(xb)[:,0], yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = InceptionTimeVar(10,1)\n",
    "opt = Adam(model_var.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8916.1064, grad_fn=<AddBackward0>)\n",
      "tensor(7501.5732, grad_fn=<AddBackward0>)\n",
      "tensor(6539.9932, grad_fn=<AddBackward0>)\n",
      "tensor(5825.1211, grad_fn=<AddBackward0>)\n",
      "tensor(5261.4102, grad_fn=<AddBackward0>)\n",
      "tensor(4800.5986, grad_fn=<AddBackward0>)\n",
      "tensor(4416.3896, grad_fn=<AddBackward0>)\n",
      "tensor(4092.2979, grad_fn=<AddBackward0>)\n",
      "tensor(3815.1409, grad_fn=<AddBackward0>)\n",
      "tensor(3575.7354, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = model_var\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = nll_regression(preds, yb)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5254, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.0768, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-1.6221, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-3.1501, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-4.6924, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-6.2771, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-7.9320, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-9.6852, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-11.5683, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-13.6197, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-15.8777, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-18.3844, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-21.1874, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-24.3242, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-27.8178, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-31.7143, grad_fn=<AddBackward0>)\n",
      "tensor(-0.1123)\n",
      "tensor(-36.0276, grad_fn=<AddBackward0>)\n",
      "tensor(-0.2247)\n",
      "tensor(-40.7211, grad_fn=<AddBackward0>)\n",
      "tensor(-0.5617)\n",
      "tensor(-45.8238, grad_fn=<AddBackward0>)\n",
      "tensor(-0.8425)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-433-ee2b5322477f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munweighted_profit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nbdev/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nbdev/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_var = InceptionTimeVar(10,1,meanrange=(-10,5))\n",
    "m = model_var\n",
    "loss_fn = partial(nll_leaky_loss, alpha=0.5)\n",
    "epochs=20\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    print(loss)\n",
    "    print(unweighted_profit(preds[:,0], yb))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unweighted_profit(m(xb)[:,0],yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.85,  0.  ],\n",
       "       [-0.81,  0.  ],\n",
       "       [-0.81,  0.01],\n",
       "       [-0.85,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.81,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.86,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.82,  0.01],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.81,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.86,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.8 ,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.85,  0.  ],\n",
       "       [-0.81,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.8 ,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.8 ,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.81,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.85,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.81,  0.  ],\n",
       "       [-0.85,  0.  ],\n",
       "       [-0.82,  0.01],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.85,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.81,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.81,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.8 ,  0.01],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.81,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.81,  0.  ],\n",
       "       [-0.85,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.85,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.8 ,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.8 ,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.86,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.81,  0.  ],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.82,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.8 ,  0.01],\n",
       "       [-0.84,  0.  ],\n",
       "       [-0.83,  0.  ],\n",
       "       [-0.83,  0.  ]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(m(xb).detach().numpy(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.zeros(128,1)\n",
    "x2 = torch.ones(128,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_p = torch.cat([x1,x2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4584)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_regression(x_p, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9803, 1.0653, 1.1313, 1.2034, 1.0856, 1.1149, 0.9548, 1.2650, 1.1225,\n",
       "        1.0263, 1.1108, 1.1613, 1.0342, 1.1508, 1.1242, 1.0461, 1.1593, 1.0856,\n",
       "        1.0512, 1.1492, 1.0317, 1.1062, 1.0516, 1.2052, 1.0718, 1.2382, 1.0609,\n",
       "        0.9224, 1.2014, 1.1770, 1.2056, 1.0383, 0.9821, 1.0533, 1.1642, 1.0345,\n",
       "        1.0596, 1.0757, 1.0150, 1.1848, 0.9880, 1.1780, 1.0803, 0.9860, 1.0961,\n",
       "        1.2026, 1.0471, 1.1474, 1.0199, 0.9712, 1.0827, 1.1966, 0.9657, 1.1112,\n",
       "        1.1972, 1.0153, 1.0547, 1.1979, 1.0110, 1.1318, 0.9660, 1.1317, 1.0288,\n",
       "        1.0343, 1.1463, 1.1485, 1.2269, 1.1639, 1.0283, 1.0596, 1.1076, 1.0431,\n",
       "        1.1613, 1.1897, 1.0978, 1.0384, 1.0461, 1.1684, 1.1513, 1.1601, 0.9469,\n",
       "        1.1904, 1.1193, 1.2421, 1.1287, 1.2092, 1.0505, 0.9479, 1.1306, 1.2492,\n",
       "        0.9976, 1.1380, 1.1620, 1.0202, 1.1837, 1.1023, 1.0340, 1.1576, 1.1534,\n",
       "        1.1139])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=torch.randn(100)\n",
    "torch.sigmoid(t)*(1.3-0.9)+0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5913, 0.7275, 0.6183, 0.5189, 0.6616, 0.6775, 0.7000, 0.7109, 0.5841,\n",
       "        0.6315, 0.7232, 0.5411, 0.6829, 0.7191, 0.5508, 0.5162, 0.7131, 0.7207,\n",
       "        0.5158, 0.7128, 0.6950, 0.5704, 0.6903, 0.7302, 0.5538, 0.5747, 0.6909,\n",
       "        0.6565, 0.6839, 0.5790, 0.7144, 0.6901, 0.7021, 0.5765, 0.6108, 0.6747,\n",
       "        0.7227, 0.7173, 0.7185, 0.5146, 0.6119, 0.7285, 0.6731, 0.7155, 0.5086,\n",
       "        0.6942, 0.6788, 0.5290, 0.5611, 0.7158, 0.6941, 0.6132, 0.6277, 0.6278,\n",
       "        0.6790, 0.6894, 0.5219, 0.5740, 0.7114, 0.7063, 0.6639, 0.5869, 0.6681,\n",
       "        0.6533, 0.6566, 0.6462, 0.6430, 0.7135, 0.6631, 0.5016, 0.6098, 0.6257,\n",
       "        0.7183, 0.7010, 0.7022, 0.6265, 0.6993, 0.7100, 0.6913, 0.5921, 0.6228,\n",
       "        0.7095, 0.5008, 0.6969, 0.5941, 0.5423, 0.7106, 0.6369, 0.6663, 0.6127,\n",
       "        0.6994, 0.5936, 0.6483, 0.5113, 0.5580, 0.5715, 0.5325, 0.6815, 0.6862,\n",
       "        0.6572])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_random_results(size):\n",
    "    high, low = 1.2, 0.9\n",
    "    res=torch.randn(size)\n",
    "    res = torch.sigmoid(res)*(high-low)+low\n",
    "    res *= -100\n",
    "    res[torch.rand(size)>0.5] = 100.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = _create_random_results((128,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(38.1166, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_leaky_loss(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.1893)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = InceptionTimeVar(10,1,meanrange=(-1,1))\n",
    "m = model_var\n",
    "loss_fn = partial(nll_leaky_loss, alpha=0.5)\n",
    "epochs=20\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.8582, grad_fn=<AddBackward0>)\n",
      "tensor(-3.5385)\n",
      "tensor(-6.6025, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4823)\n",
      "tensor(-7.3244, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-8.0178, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-8.6825, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-9.3129, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-9.9017, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-10.4434, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-10.9383, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-11.3915, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-11.8113, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-12.2062, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-12.5798, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-12.9334, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-13.2676, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-13.5844, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-13.8866, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-14.1788, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-14.4650, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n",
      "tensor(-14.7484, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4261)\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    print(loss)\n",
    "    print(unweighted_profit(preds[:,0], yb))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9317.9912, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7070)\n",
      "tensor(-30340.9004, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7070)\n",
      "tensor(-90235.2656, grad_fn=<AddBackward0>)\n",
      "tensor(-3.8755)\n",
      "tensor(-214640.3750, grad_fn=<AddBackward0>)\n",
      "tensor(-3.9878)\n",
      "tensor(-359202.5312, grad_fn=<AddBackward0>)\n",
      "tensor(-4.0440)\n",
      "tensor(-397443.5000, grad_fn=<AddBackward0>)\n",
      "tensor(-4.1001)\n",
      "tensor(-444806.8125, grad_fn=<AddBackward0>)\n",
      "tensor(-4.1001)\n",
      "tensor(-444799.9375, grad_fn=<AddBackward0>)\n",
      "tensor(-4.1001)\n",
      "tensor(-444794.8438, grad_fn=<AddBackward0>)\n",
      "tensor(-4.1001)\n",
      "tensor(-444791.0625, grad_fn=<AddBackward0>)\n",
      "tensor(-4.1001)\n",
      "tensor(-444788.1875, grad_fn=<AddBackward0>)\n",
      "tensor(-4.1001)\n",
      "tensor(-444786., grad_fn=<AddBackward0>)\n",
      "tensor(-4.1563)\n",
      "tensor(-444784.2500, grad_fn=<AddBackward0>)\n",
      "tensor(-4.1563)\n",
      "tensor(-444782.7812, grad_fn=<AddBackward0>)\n",
      "tensor(-4.3248)\n",
      "tensor(-444781.6250, grad_fn=<AddBackward0>)\n",
      "tensor(-4.3248)\n",
      "tensor(-444780.6250, grad_fn=<AddBackward0>)\n",
      "tensor(-4.4933)\n",
      "tensor(-444779.7500, grad_fn=<AddBackward0>)\n",
      "tensor(-4.6056)\n",
      "tensor(-444778.9688, grad_fn=<AddBackward0>)\n",
      "tensor(-4.6618)\n",
      "tensor(-444778.1875, grad_fn=<AddBackward0>)\n",
      "tensor(-4.6618)\n",
      "tensor(-444777.6250, grad_fn=<AddBackward0>)\n",
      "tensor(-4.8303)\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    print(loss)\n",
    "    print(unweighted_profit(preds[:,0], yb))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8102,  0.3178],\n",
       "        [-0.5239,  0.6837],\n",
       "        [ 0.7969,  0.3428],\n",
       "        [ 0.8238,  0.3040],\n",
       "        [-0.6941,  0.6855],\n",
       "        [-0.5750,  0.6674],\n",
       "        [ 0.8599,  0.2828],\n",
       "        [-0.6396,  0.6581],\n",
       "        [-0.6178,  0.6803],\n",
       "        [-0.6704,  0.7044],\n",
       "        [ 0.7746,  0.3350],\n",
       "        [-0.4117,  0.6518],\n",
       "        [ 0.7976,  0.3109],\n",
       "        [-0.6089,  0.6810],\n",
       "        [ 0.7839,  0.3432],\n",
       "        [ 0.8260,  0.3253],\n",
       "        [-0.6338,  0.7247],\n",
       "        [ 0.8098,  0.2983],\n",
       "        [-0.7259,  0.6800],\n",
       "        [ 0.7886,  0.3059],\n",
       "        [-0.4271,  0.6483],\n",
       "        [ 0.8405,  0.2760],\n",
       "        [ 0.8067,  0.3065],\n",
       "        [ 0.7801,  0.3093],\n",
       "        [-0.6602,  0.6911],\n",
       "        [ 0.7929,  0.3536],\n",
       "        [ 0.7634,  0.3492],\n",
       "        [-0.7213,  0.6978],\n",
       "        [-0.6269,  0.7063],\n",
       "        [ 0.7730,  0.3216],\n",
       "        [ 0.8197,  0.2993],\n",
       "        [ 0.8387,  0.2986],\n",
       "        [ 0.8390,  0.2937],\n",
       "        [ 0.7963,  0.3075],\n",
       "        [-0.6434,  0.7037],\n",
       "        [-0.5897,  0.6951],\n",
       "        [ 0.8069,  0.3289],\n",
       "        [-0.5431,  0.6712],\n",
       "        [-0.5118,  0.6954],\n",
       "        [ 0.8070,  0.3409],\n",
       "        [-0.5894,  0.6192],\n",
       "        [ 0.8343,  0.2936],\n",
       "        [-0.5271,  0.6456],\n",
       "        [-0.6154,  0.6583],\n",
       "        [ 0.8333,  0.3005],\n",
       "        [-0.6157,  0.7255],\n",
       "        [-0.5690,  0.6801],\n",
       "        [ 0.8136,  0.2901],\n",
       "        [ 0.8361,  0.3272],\n",
       "        [-0.5298,  0.7013],\n",
       "        [ 0.8339,  0.3044],\n",
       "        [-0.6459,  0.6985],\n",
       "        [-0.6424,  0.7266],\n",
       "        [-0.5261,  0.6575],\n",
       "        [ 0.8036,  0.3322],\n",
       "        [ 0.8275,  0.3105],\n",
       "        [ 0.8149,  0.3106],\n",
       "        [ 0.8105,  0.2903],\n",
       "        [-0.4714,  0.6741],\n",
       "        [ 0.8174,  0.3127],\n",
       "        [ 0.7837,  0.3433],\n",
       "        [-0.6115,  0.6627],\n",
       "        [-0.6071,  0.7099],\n",
       "        [ 0.8061,  0.3094],\n",
       "        [ 0.8387,  0.3063],\n",
       "        [-0.6945,  0.7227],\n",
       "        [ 0.8292,  0.2883],\n",
       "        [ 0.8283,  0.2970],\n",
       "        [ 0.8274,  0.2986],\n",
       "        [ 0.8054,  0.3035],\n",
       "        [ 0.8054,  0.3292],\n",
       "        [-0.5444,  0.6650],\n",
       "        [-0.6767,  0.7035],\n",
       "        [-0.5492,  0.6872],\n",
       "        [-0.5918,  0.6855],\n",
       "        [-0.6481,  0.6900],\n",
       "        [ 0.8205,  0.3029],\n",
       "        [-0.5748,  0.6860],\n",
       "        [ 0.8184,  0.2873],\n",
       "        [-0.5876,  0.6837],\n",
       "        [ 0.7957,  0.3320],\n",
       "        [ 0.8325,  0.2979],\n",
       "        [-0.5924,  0.7199],\n",
       "        [ 0.8353,  0.3017],\n",
       "        [ 0.8656,  0.2744],\n",
       "        [-0.5516,  0.6911],\n",
       "        [-0.6387,  0.6578],\n",
       "        [ 0.8391,  0.3027],\n",
       "        [-0.5607,  0.6783],\n",
       "        [-0.5742,  0.6996],\n",
       "        [-0.5799,  0.6655],\n",
       "        [-0.5217,  0.6808],\n",
       "        [ 0.8239,  0.3124],\n",
       "        [-0.5923,  0.6208],\n",
       "        [-0.6012,  0.6716],\n",
       "        [ 0.8322,  0.2753],\n",
       "        [-0.5909,  0.6835],\n",
       "        [-0.4380,  0.6229],\n",
       "        [ 0.7928,  0.3090],\n",
       "        [-0.5745,  0.6928],\n",
       "        [ 0.7712,  0.3466],\n",
       "        [ 0.8064,  0.3226],\n",
       "        [-0.6104,  0.6598],\n",
       "        [ 0.8143,  0.3111],\n",
       "        [-0.5438,  0.6740],\n",
       "        [ 0.7976,  0.3137],\n",
       "        [-0.4920,  0.6633],\n",
       "        [-0.7181,  0.7156],\n",
       "        [-0.5889,  0.6741],\n",
       "        [ 0.7896,  0.3479],\n",
       "        [ 0.7899,  0.3399],\n",
       "        [-0.6214,  0.6625],\n",
       "        [ 0.8369,  0.2983],\n",
       "        [-0.5505,  0.6481],\n",
       "        [ 0.8312,  0.3085],\n",
       "        [-0.6113,  0.6070],\n",
       "        [-0.6210,  0.6519],\n",
       "        [-0.5778,  0.6665],\n",
       "        [-0.6429,  0.6874],\n",
       "        [ 0.8278,  0.2942],\n",
       "        [-0.6555,  0.7006],\n",
       "        [-0.5862,  0.6934],\n",
       "        [-0.5371,  0.7025],\n",
       "        [-0.6234,  0.6196],\n",
       "        [-0.6825,  0.6794],\n",
       "        [ 0.7964,  0.3077],\n",
       "        [-0.6964,  0.6912],\n",
       "        [ 0.8083,  0.2941]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1196, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.1918, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.2503, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.3002, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.3442, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.3833, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.4182, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.4494, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.4771, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.5019, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.5238, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.5433, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.5605, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.5757, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.5891, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.6010, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.6116, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.6209, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.6293, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.6368, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "model_mean = InceptionTimeSgm(10,1, range=(-1,1))\n",
    "m = model_mean\n",
    "loss_fn = partial(leaky_loss, alpha=0.1)\n",
    "# loss_fn = F.mse_loss\n",
    "\n",
    "epochs=20\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    print(loss)\n",
    "    print(unweighted_profit(preds, yb))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59.69],\n",
       "       [-44.33],\n",
       "       [ 32.03],\n",
       "       [ 48.34],\n",
       "       [-71.35],\n",
       "       [-90.21],\n",
       "       [ 31.71],\n",
       "       [-74.66],\n",
       "       [-63.18],\n",
       "       [-85.05],\n",
       "       [ 27.3 ],\n",
       "       [-40.44],\n",
       "       [ 22.68],\n",
       "       [-72.75],\n",
       "       [ 16.64],\n",
       "       [ 59.72],\n",
       "       [-79.17],\n",
       "       [ 51.5 ],\n",
       "       [-74.85],\n",
       "       [ 15.23],\n",
       "       [-46.47],\n",
       "       [ 54.3 ],\n",
       "       [ 39.65],\n",
       "       [ 40.41],\n",
       "       [-78.55],\n",
       "       [ 23.05],\n",
       "       [ 19.68],\n",
       "       [-73.84],\n",
       "       [-63.5 ],\n",
       "       [ 15.34],\n",
       "       [ 32.78],\n",
       "       [ 43.81],\n",
       "       [ 47.92],\n",
       "       [ 21.04],\n",
       "       [-72.07],\n",
       "       [-62.32],\n",
       "       [ 55.13],\n",
       "       [-50.59],\n",
       "       [-22.92],\n",
       "       [ 25.04],\n",
       "       [-52.25],\n",
       "       [ 58.79],\n",
       "       [-80.96],\n",
       "       [-63.03],\n",
       "       [ 45.97],\n",
       "       [-51.07],\n",
       "       [-44.96],\n",
       "       [ 36.12],\n",
       "       [ 38.38],\n",
       "       [-35.93],\n",
       "       [ 48.57],\n",
       "       [-81.71],\n",
       "       [-75.16],\n",
       "       [-14.51],\n",
       "       [ 18.78],\n",
       "       [ 45.45],\n",
       "       [ 64.61],\n",
       "       [ 21.41],\n",
       "       [-47.83],\n",
       "       [ 25.56],\n",
       "       [ 29.67],\n",
       "       [-45.24],\n",
       "       [-74.35],\n",
       "       [ 51.18],\n",
       "       [ 70.01],\n",
       "       [-66.95],\n",
       "       [ 50.7 ],\n",
       "       [ 31.65],\n",
       "       [ 73.51],\n",
       "       [ 45.24],\n",
       "       [ 15.79],\n",
       "       [-46.97],\n",
       "       [-66.19],\n",
       "       [-52.27],\n",
       "       [-31.99],\n",
       "       [-62.17],\n",
       "       [ 47.18],\n",
       "       [-61.29],\n",
       "       [ 51.05],\n",
       "       [-81.69],\n",
       "       [ 36.47],\n",
       "       [ 60.53],\n",
       "       [-59.38],\n",
       "       [ 49.58],\n",
       "       [ 55.02],\n",
       "       [-46.87],\n",
       "       [-52.48],\n",
       "       [ 68.52],\n",
       "       [-59.73],\n",
       "       [-65.89],\n",
       "       [-70.77],\n",
       "       [-42.74],\n",
       "       [ 25.96],\n",
       "       [-74.5 ],\n",
       "       [-51.23],\n",
       "       [ 45.24],\n",
       "       [-54.63],\n",
       "       [-35.25],\n",
       "       [ 22.76],\n",
       "       [-82.95],\n",
       "       [ 40.13],\n",
       "       [ 42.83],\n",
       "       [-79.73],\n",
       "       [  9.94],\n",
       "       [-79.8 ],\n",
       "       [ 56.69],\n",
       "       [-59.82],\n",
       "       [-77.28],\n",
       "       [-72.12],\n",
       "       [ 25.76],\n",
       "       [ 33.63],\n",
       "       [-40.13],\n",
       "       [ 39.33],\n",
       "       [-19.84],\n",
       "       [ 59.07],\n",
       "       [-38.63],\n",
       "       [-71.31],\n",
       "       [-52.19],\n",
       "       [-67.08],\n",
       "       [ 41.86],\n",
       "       [-78.22],\n",
       "       [-73.28],\n",
       "       [-64.8 ],\n",
       "       [-40.85],\n",
       "       [-87.33],\n",
       "       [ 46.48],\n",
       "       [-61.11],\n",
       "       [ 50.97]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(preds.detach().numpy(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8.0606, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_profit(preds, yb, threshold=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 0.5*preds[:,1].log() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.4077, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = F.leaky_relu(preds[:,0], 0.5)*yb.float().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50.2426, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.0440)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unweighted_profit(preds[:,0], yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.1925e-01,  2.2035e-01,  7.7802e-01,  7.5209e-01, -9.6370e-03,\n",
       "        -5.4069e-03,  8.3000e-01, -5.6366e-03, -9.5591e-03,  1.5291e-02,\n",
       "         8.0178e-01,  1.9101e-01,  6.7007e-01, -9.8845e-03,  5.9902e-01,\n",
       "         7.5299e-01, -1.4802e-03,  7.7943e-01, -9.4004e-03,  7.8837e-01,\n",
       "        -4.1210e-03,  7.9112e-01,  7.8203e-01,  7.0024e-01, -1.0000e-02,\n",
       "         7.9763e-01,  7.3333e-01, -4.8869e-03, -6.0198e-03,  5.9659e-01,\n",
       "         6.4868e-01,  8.3074e-01,  8.6674e-01,  8.1339e-01, -8.5559e-03,\n",
       "        -8.0417e-03,  8.7010e-01, -7.2772e-03,  2.4229e-01,  6.5914e-01,\n",
       "        -9.9913e-03,  8.0144e-01, -6.2689e-03, -5.1532e-03,  8.4950e-01,\n",
       "        -7.8629e-04, -5.0912e-03,  8.3450e-01,  6.9821e-01, -6.6265e-03,\n",
       "         8.4684e-01, -1.4540e-03, -1.1866e-03, -5.2012e-04,  6.9449e-01,\n",
       "         8.3494e-01,  7.6754e-01,  8.4125e-01, -7.3983e-03,  8.4954e-01,\n",
       "         6.6644e-01, -7.9540e-03, -9.4522e-03,  7.9308e-01,  5.6524e-01,\n",
       "        -4.2100e-03,  4.8191e-01,  8.4140e-01,  8.2408e-01,  8.4799e-01,\n",
       "         7.0922e-01, -2.8092e-03, -9.7226e-03, -2.9480e-04, -1.5744e-03,\n",
       "        -7.0143e-03,  6.9547e-01, -2.8341e-03,  6.0642e-01, -9.1840e-03,\n",
       "         8.6263e-01,  8.9377e-01,  7.6320e-02,  8.4966e-01,  8.5309e-01,\n",
       "        -1.5595e-03,  8.9373e-02,  7.4599e-01, -9.4146e-03, -6.8390e-03,\n",
       "        -6.1355e-03, -6.6092e-03,  8.2144e-01, -8.1095e-03,  5.5224e-03,\n",
       "         8.5346e-01,  4.2325e-02, -2.0239e-03,  8.7500e-01, -3.8894e-03,\n",
       "         6.3018e-01,  8.1290e-01, -9.7854e-03,  7.9471e-01, -1.9305e-03,\n",
       "         6.8138e-01, -9.0449e-03, -9.6631e-03, -3.2488e-03,  7.9622e-01,\n",
       "         8.0945e-01,  1.5524e-02,  8.8755e-01,  2.4855e-01,  6.4081e-01,\n",
       "        -9.5659e-03, -9.2554e-03, -8.2808e-03, -9.9988e-03,  8.0530e-01,\n",
       "         2.1511e-02, -7.4787e-03, -4.0604e-04, -9.1842e-03, -6.4931e-03,\n",
       "         8.6702e-01, -8.8407e-03,  8.4123e-01], grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.leaky_relu(preds[:,0], 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 81.9255, -22.0001,  77.8021,  75.2093,  52.1226,  25.8011,  82.9995,\n",
       "         28.4714,  48.0399,  -1.5908,  80.1777, -19.7501,  67.0073,  55.0419,\n",
       "         59.9020,  75.2989,   7.4788,  77.9432,  54.3332,  78.8372,  20.9156,\n",
       "         79.1120,  78.2034,  70.0237,  56.1900,  79.7627,  73.3333,  27.1952,\n",
       "         33.1130,  59.6591,  64.8677,  83.0743,  86.6744,  81.3389,  45.9245,\n",
       "         42.3194,  87.0103,  41.0345, -25.5582,  65.9136,  55.6011,  80.1443,\n",
       "         29.4647,  28.9230,  84.9501,   3.7070,  26.5617,  83.4499,  69.8207,\n",
       "         33.9378,  84.6835,   7.3470,   5.6708,   2.5431,  69.4486,  83.4943,\n",
       "         76.7540,  84.1252,  37.5089,  84.9543,  66.6443,  41.6723,  44.7566,\n",
       "         79.3078,  56.5244,  21.7490,  48.1913,  84.1402,  82.4076,  84.7987,\n",
       "         70.9222,  15.2145,  51.2575,   1.5143,   7.5915,  38.2872,  69.5466,\n",
       "         15.8526,  60.6424,  48.2346,  86.2634,  89.3767,  -7.8546,  84.9656,\n",
       "         85.3088,   7.4551,  -8.3118,  74.5987,  50.2686,  32.9276,  32.0365,\n",
       "         35.4898,  82.1441,  46.4378,  -0.5498,  85.3464,  -4.3913,   9.9212,\n",
       "         87.5000,  18.6303,  63.0184,  81.2898,  55.2142,  79.4711,  10.0563,\n",
       "         68.1381,  44.9391,  55.2910,  16.6259,  79.6222,  80.9447,  -1.5727,\n",
       "         88.7554, -27.5071,  64.0814,  54.4578,  52.9549,  45.0890,  50.1575,\n",
       "         80.5303,  -2.4074,  38.8905,   2.2394,  49.3734,  36.0351,  86.7023,\n",
       "         49.4738,  84.1235], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = 0.5*(l1.div(preds[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.300000e+01, -8.000000e+00,  9.100000e+01,  4.700000e+01,\n",
       "        5.150000e+02,  2.200000e+01,  4.500000e+01,  2.800000e+01,\n",
       "        3.400000e+02, -1.000000e+00,  9.500000e+01, -7.000000e+00,\n",
       "        4.400000e+01,  1.124000e+03,  5.500000e+01,  3.700000e+01,\n",
       "        3.000000e+00,  4.100000e+01,  3.570000e+02,  9.000000e+01,\n",
       "        1.500000e+01,  8.000000e+01,  1.010000e+02,  9.400000e+01,\n",
       "        3.717994e+06,  5.000000e+01,  4.800000e+01,  3.100000e+01,\n",
       "        3.200000e+01,  4.700000e+01,  4.500000e+01,  4.100000e+01,\n",
       "        4.600000e+01,  7.500000e+01,  1.430000e+02,  7.400000e+01,\n",
       "        6.100000e+01,  4.700000e+01, -6.000000e+00,  5.600000e+01,\n",
       "        1.050800e+04,  6.000000e+01,  5.800000e+01,  2.400000e+01,\n",
       "        4.300000e+01,  2.000000e+00,  2.100000e+01,  5.900000e+01,\n",
       "        4.300000e+01,  4.900000e+01,  4.000000e+01,  3.000000e+00,\n",
       "        3.000000e+00,  1.000000e+00,  4.100000e+01,  4.700000e+01,\n",
       "        4.800000e+01,  1.710000e+02,  5.800000e+01,  7.900000e+01,\n",
       "        4.900000e+01,  1.140000e+02,  3.100000e+02,  4.900000e+01,\n",
       "        2.900000e+01,  1.800000e+01,  6.800000e+01,  3.900000e+01,\n",
       "        3.900000e+01,  5.700000e+01,  3.600000e+01,  9.000000e+00,\n",
       "        5.560000e+02,  1.000000e+00,  4.000000e+00,  5.700000e+01,\n",
       "        5.100000e+01,  7.000000e+00,  7.300000e+01,  1.870000e+02,\n",
       "        5.200000e+01,  4.600000e+01, -3.000000e+00,  4.200000e+01,\n",
       "        4.100000e+01,  4.000000e+00, -2.000000e+00,  6.800000e+01,\n",
       "        2.520000e+02,  4.800000e+01,  5.400000e+01,  3.700000e+01,\n",
       "        9.500000e+01,  1.080000e+02, -0.000000e+00,  4.600000e+01,\n",
       "       -2.000000e+00,  4.000000e+00,  7.200000e+01,  1.500000e+01,\n",
       "        4.500000e+01,  3.300000e+01,  8.200000e+02,  6.800000e+01,\n",
       "        7.000000e+00,  5.000000e+01,  1.490000e+02,  5.370000e+02,\n",
       "        1.000000e+01,  6.700000e+01,  5.500000e+01, -1.000000e+00,\n",
       "        4.700000e+01, -1.100000e+01,  2.800000e+01,  5.210000e+02,\n",
       "        2.570000e+02,  1.210000e+02,  2.937700e+04,  5.300000e+01,\n",
       "       -1.000000e+00,  4.300000e+01,  1.000000e+00,  2.440000e+02,\n",
       "        3.400000e+01,  6.200000e+01,  1.480000e+02,  4.500000e+01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(s2.detach().numpy(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    loss = (s1+s2).mean() + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "with torch.no_grad():\n",
    "    for p in model_v.parameters():\n",
    "        p -= p.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_2 = model_v(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_2 = nll_regression(preds_2, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.6749, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1327, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.5*preds[:,1].log() + 0.5*(yb.squeeze()-preds[:,0]).pow(2).div(preds[:,1].pow(2))).mean() + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = list(model.parameters())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load in some real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbdev]",
   "language": "python",
   "name": "conda-env-nbdev-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
