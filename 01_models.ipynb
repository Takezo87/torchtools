{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torchtools.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn as nn\n",
    "import torch as torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from fastai2.layers import SigmoidRange\n",
    "from fast_tabnet.core import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7616])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.tensor([-2.])) * (1 - -1) + -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8ce02dac10>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "t = torch.arange(-10,10).float()\n",
    "# plt.plot(t)\n",
    "plt.plot(F.softplus(t, beta=1))\n",
    "plt.plot(F.softplus(t, beta=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@gmail.com based on:\n",
    "\n",
    "# Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... & Petitjean, F. (2019). InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.\n",
    "# Official InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime\n",
    "\n",
    "\n",
    "def noop(x):\n",
    "    return x\n",
    "\n",
    "def shortcut(c_in, c_out):\n",
    "    return nn.Sequential(*[nn.Conv1d(c_in, c_out, kernel_size=1), \n",
    "                           nn.BatchNorm1d(c_out)])\n",
    "    \n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, c_in, bottleneck=32, ks=40, nb_filters=32):\n",
    "\n",
    "        super().__init__()\n",
    "        self.bottleneck = nn.Conv1d(c_in, bottleneck, 1) if bottleneck and c_in > 1 else noop\n",
    "        mts_feat = bottleneck or c_in\n",
    "        conv_layers = []\n",
    "        kss = [ks // (2**i) for i in range(3)]\n",
    "        # ensure odd kss until nn.Conv1d with padding='same' is available in pytorch 1.3\n",
    "        kss = [ksi if ksi % 2 != 0 else ksi - 1 for ksi in kss]  \n",
    "        for i in range(len(kss)):\n",
    "            conv_layers.append(\n",
    "                nn.Conv1d(mts_feat, nb_filters, kernel_size=kss[i], padding=kss[i] // 2))\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.maxpool = nn.MaxPool1d(3, stride=1, padding=1)\n",
    "        self.conv = nn.Conv1d(c_in, nb_filters, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm1d(nb_filters * 4)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = x.to(torch.float)\n",
    "        x = self.bottleneck(input_tensor)\n",
    "        for i in range(3):\n",
    "            out_ = self.conv_layers[i](x)\n",
    "            if i == 0: out = out_\n",
    "            else: out = torch.cat((out, out_), 1)\n",
    "        mp = self.conv(self.maxpool(input_tensor))\n",
    "        inc_out = torch.cat((out, mp), 1)\n",
    "        return self.act(self.bn(inc_out))\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self,c_in,bottleneck=32,ks=40,nb_filters=32,residual=True,depth=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = residual\n",
    "        self.depth = depth\n",
    "\n",
    "        #inception & residual layers\n",
    "        inc_mods = []\n",
    "        res_layers = []\n",
    "        res = 0\n",
    "        for d in range(depth):\n",
    "            inc_mods.append(\n",
    "                Inception(c_in if d == 0 else nb_filters * 4, bottleneck=bottleneck if d > 0 else 0,ks=ks,\n",
    "                          nb_filters=nb_filters))\n",
    "            if self.residual and d % 3 == 2:\n",
    "                res_layers.append(shortcut(c_in if res == 0 else nb_filters * 4, nb_filters * 4))\n",
    "                res += 1\n",
    "            else: res_layer = res_layers.append(None)\n",
    "        self.inc_mods = nn.ModuleList(inc_mods)\n",
    "        self.res_layers = nn.ModuleList(res_layers)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for d, l in enumerate(range(self.depth)):\n",
    "            x = self.inc_mods[d](x)\n",
    "            if self.residual and d % 3 == 2:\n",
    "                res = self.res_layers[d](res)\n",
    "                x += res\n",
    "                res = x\n",
    "                x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "class InceptionTime(nn.Module):\n",
    "    def __init__(self,c_in,c_out,bottleneck=32,ks=40,nb_filters=32,residual=True,depth=6):\n",
    "        super().__init__()\n",
    "        self.block = InceptionBlock(c_in,bottleneck=bottleneck,ks=ks,nb_filters=nb_filters,\n",
    "                                    residual=residual,depth=depth)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(nb_filters * 4, c_out)\n",
    "\n",
    "    def forward(self, *x):\n",
    "        x = torch.cat(x, dim=-2)\n",
    "        x = self.block(x)\n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Sigmoid(nn.Module):\n",
    "    '''\n",
    "    sigmoid layer\n",
    "    '''\n",
    "    def __init__(self, low, high):\n",
    "        super().__init__()\n",
    "        self.high, self.low = high, low\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x)*(self.high-self.low)+self.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeSgmOld(nn.Module):\n",
    "    '''\n",
    "    add a sigmoid layer to InceptionTime to get the ouput in a certain range\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        nn.Sequential()\n",
    "        self.inc = InceptionTime(n_in, n_out)\n",
    "        self.low, self.high = -1., 1.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.inc(x)) * (self.high - self.low) + self.low\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeSgm(nn.Module):\n",
    "    '''\n",
    "    add a sigmoid layer to InceptionTime to get the ouput in a certain range\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out, range=(-1,1)):\n",
    "        super().__init__()\n",
    "        self.mod = nn.Sequential(InceptionTime(n_in, n_out), SigmoidRange(*range))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        return self.mod(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeD(nn.Module):\n",
    "    '''\n",
    "    add a sigmoid layer to InceptionTime to get the ouput in a certain range\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.mod = nn.Sequential(InceptionTime(n_in, n_out), Sigmoid(-1., 1.))\n",
    "        \n",
    "    def forward(self, xc, xd):\n",
    "        x = torch.cat([xc.float(), xd.float()], dim=-2)\n",
    "        x = x.float()\n",
    "#         print(f'InceptionTimeSgm dtype {x.dtype}')\n",
    "        return self.mod(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "class InceptionTime_NH(nn.Module):\n",
    "    '''inceptiontime, no final layer'''\n",
    "    def __init__(self,c_in,c_out,bottleneck=32,ks=40,nb_filters=32,residual=True,depth=6):\n",
    "        super().__init__()\n",
    "        self.block = InceptionBlock(c_in,bottleneck=bottleneck,ks=ks,nb_filters=nb_filters,\n",
    "                                    residual=residual,depth=depth)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.fc = nn.Linear(nb_filters * 4, c_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "#         print(x.shape)\n",
    "        x = self.gap(x).squeeze(-1)\n",
    "#         x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_sz_rule(n_cat):\n",
    "    \"Rule of thumb to pick embedding size corresponding to `n_cat`\"\n",
    "    return min(600, round(1.6 * n_cat**0.56))\n",
    "\n",
    "# Cell\n",
    "def _one_emb_sz(classes, n, sz_dict=None):\n",
    "    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n",
    "    sz_dict = ifnone(sz_dict, {})\n",
    "    n_cat = len(classes[n])\n",
    "    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n",
    "    return n_cat,sz\n",
    "\n",
    "# Cell\n",
    "def get_emb_sz(to, sz_dict=None):\n",
    "    \"Get default embedding size from `TabularPreprocessor` `proc` or the ones in `sz_dict`\"\n",
    "    return [_one_emb_sz(to.classes,v n, sz_dict) for n in to.cat_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _map_xs(xs, xs_mask):\n",
    "    '''\n",
    "    xs: i-tuple of tensors\n",
    "    xs_mask: length j>=i mask\n",
    "    xs_id: lenght j>=i string list of x identifiers \n",
    "    '''\n",
    "    assert np.array(xs_mask).sum()==len(xs)\n",
    "    res = np.array([None]*len(xs_mask))\n",
    "    res[np.where(xs_mask)[0]]=xs\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeD_Mixed(nn.Module):\n",
    "    '''\n",
    "    mixed model for timeseries and tabular data\n",
    "    ts_mod: InceptionTime model without final fully connected lay\n",
    "    tab_mod: MLP or TabNet, currently both cont and cat is required\n",
    "    outputs are concatenated, then put through a fully connected layer, then sigmoid range\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_c, n_d, n_out, n_cont, emb_szs=None):\n",
    "        super().__init__()\n",
    "        self.n_c, self.n_d, self.n_cont, self.emb_szs = n_c, n_d, n_out, emb_szs\n",
    "        assert n_c>0, 'at least one continuous channel required'\n",
    "        self.ts_mod = InceptionTime_NH(n_c+n_d, n_out) #128\n",
    "        self.sgm = Sigmoid(-1,1)\n",
    "#         self.mod = nn.Sequential(InceptionTime(n_in, n_out), Sigmoid(-1., 1.))\n",
    "#         self.tab_mod = nn.Sequential(nn.Linear(2,100), nn.ReLU(), nn.Linear(100,64))\n",
    "        self.tab_mod = TabNetModel(emb_szs=emb_szs, n_cont=n_cont, out_sz=64)\n",
    "        self.fc = nn.Linear(192,n_out)\n",
    "        \n",
    "#     def forward(self, xc, xd, xt, xcat=None):\n",
    "    def forward(self, *xs):\n",
    "        \n",
    "        \n",
    "        xs_mask = [self.n_c>0, self.n_d>0, self.n_cont>0, len(self.emb_szs)>0]\n",
    "#         x_type_idxs = [i for i in range(4) if has_x[i]]\n",
    "        xc,xd,xt,xcat = map_xs(xs, xs_mask)\n",
    "        \n",
    "        x_ts=xc.float()\n",
    "        if xd is not None: x_ts = torch.cat([x_ts, xd.float()], dim=-2)\n",
    "        \n",
    "#         x_ts=torch.cat([xs[0].float(), xd.float()], dim=-2) if self.n_d>0 else x_ts\n",
    "        \n",
    "        \n",
    "#         x = t\n",
    "#         x = x.float()\n",
    "#         print(f'InceptionTimeSgm dtype {x.dtype}')\n",
    "#         print(self.ts_mod(x).shape, self.tab_mod(xt.float().squeeze(-2)).shape )\n",
    "        xcat=xcat.long() if xcat is not None else None\n",
    "        xt=xt.float() if xt is not None else None\n",
    "        x_all = torch.cat([self.ts_mod(x_ts), self.tab_mod(xcat, xt)], dim=-1)\n",
    "        return self.sgm(self.fc(x_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTime_Mixed(nn.Module):\n",
    "    '''\n",
    "    mixed model for timeseries and tabular data\n",
    "    ts_mod: InceptionTime model without final fully connected lay\n",
    "    tab_mod: MLP or TabNet, currently both cont and cat is required\n",
    "    outputs are concatenated, then put through a fully connected layer, no sigmoid for classification\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_c, n_d, n_out, n_cont, emb_szs=None):\n",
    "        super().__init__()\n",
    "        self.n_c, self.n_d, self.n_cont, self.emb_szs = n_c, n_d, n_out, emb_szs\n",
    "        assert n_c>0, 'at least one continuous channel required'\n",
    "        self.ts_mod = InceptionTime_NH(n_c+n_d, n_out) #128\n",
    "#         self.mod = nn.Sequential(InceptionTime(n_in, n_out), Sigmoid(-1., 1.))\n",
    "#         self.tab_mod = nn.Sequential(nn.Linear(2,100), nn.ReLU(), nn.Linear(100,64))\n",
    "        self.tab_mod = TabNetModel(emb_szs=emb_szs, n_cont=n_cont, out_sz=64)\n",
    "        self.fc = nn.Linear(192,n_out)\n",
    "        \n",
    "#     def forward(self, xc, xd, xt, xcat=None):\n",
    "    def forward(self, *xs):\n",
    "        \n",
    "        \n",
    "        xs_mask = [self.n_c>0, self.n_d>0, self.n_cont>0, len(self.emb_szs)>0]\n",
    "#         x_type_idxs = [i for i in range(4) if has_x[i]]\n",
    "        xc,xd,xt,xcat = map_xs(xs, xs_mask)\n",
    "        \n",
    "        x_ts=xc.float()\n",
    "        if xd is not None: x_ts = torch.cat([x_ts, xd.float()], dim=-2)\n",
    "        \n",
    "#         x_ts=torch.cat([xs[0].float(), xd.float()], dim=-2) if self.n_d>0 else x_ts\n",
    "        \n",
    "        \n",
    "#         x = t\n",
    "#         x = x.float()\n",
    "#         print(f'InceptionTimeSgm dtype {x.dtype}')\n",
    "#         print(self.ts_mod(x).shape, self.tab_mod(xt.float().squeeze(-2)).shape )\n",
    "        xcat=xcat.long() if xcat is not None else None\n",
    "        xt=xt.float() if xt is not None else None\n",
    "        x_all = torch.cat([self.ts_mod(x_ts), self.tab_mod(xcat, xt)], dim=-1)\n",
    "        return self.fc(x_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TabNetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TabNetTT(nn.Module):\n",
    "    '''\n",
    "    convenience wrapper for pure TabNetModel models\n",
    "    '''\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, **kwargs):\n",
    "        super().__init__()\n",
    "        self.tab = TabNetModel(emb_szs, n_cont, out_sz, **kwargs)\n",
    "        \n",
    "    def forward(self, xt, xcat):\n",
    "        xcat=xcat.long() if xcat is not None else None\n",
    "        xt=xt.float() if xt is not None else None\n",
    "        return self.tab(xcat, xt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = (torch.rand(5), torch.rand(5))\n",
    "xs_mask = [True, False, False, True]\n",
    "# xs_id = ['xa', 'xb', 'xc', 'xd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = map_xs(xs, xs_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7881, 0.9778, 0.3140, 0.2567, 0.1967]),\n",
       " None,\n",
       " None,\n",
       " tensor([0.6820, 0.7085, 0.3898, 0.3335, 0.7590]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-b883e28c3c1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "l[np.where(np.array(xs_mask))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [None, None, None, None, None]\n",
    "l=np.array(l)\n",
    "l[np.where(np.array(xs_mask))[0]]=xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([tensor([0.6260, 0.2170, 0.5897, 0.6390, 0.7939]), None, None,\n",
       "       tensor([0.3255, 0.9235, 0.8020, 0.9128, 0.3032]), None],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-47bad7d53b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "l[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.6263, 0.9378, 0.4598, 0.1943, 0.2437]), None, None, tensor([0.8345, 0.0159, 0.7012, 0.8561, 0.9810]))\n"
     ]
    }
   ],
   "source": [
    "xa,xb,xc,xd = map_xs(xs, xs_mask, xs_id)\n",
    "print((xa, xb, xc, xd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, tensor([0.1152, 0.8739, 0.6097, 0.4476, 0.8327]), None, tensor([0.7637, 0.0014, 0.7492, 0.7462, 0.4787]))\n"
     ]
    }
   ],
   "source": [
    "xs = (torch.rand(5), torch.rand(5))\n",
    "xs_mask = [False, True, False, True]\n",
    "xs_id = ['xa', 'xb', 'xc', 'xd']\n",
    "xa,xb,xc,xd = map_xs(xs, xs_mask, xs_id)\n",
    "print((xa, xb, xc, xd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array([True, False, True,True, False, False]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = InceptionTimeD_Mixed(4,2,7,2,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionTimeD_Mixed(\n",
       "  (ts_mod): InceptionTime_NH(\n",
       "    (block): InceptionBlock(\n",
       "      (inc_mods): ModuleList(\n",
       "        (0): Inception(\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(6, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(6, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(6, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(6, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (1): Inception(\n",
       "          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (2): Inception(\n",
       "          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (3): Inception(\n",
       "          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (4): Inception(\n",
       "          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (5): Inception(\n",
       "          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (res_layers): ModuleList(\n",
       "        (0): None\n",
       "        (1): None\n",
       "        (2): Sequential(\n",
       "          (0): Conv1d(6, 128, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): None\n",
       "        (4): None\n",
       "        (5): Sequential(\n",
       "          (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (gap): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (sgm): Sigmoid()\n",
       "  (tab_mod): TabNetModel(\n",
       "    (embeds): ModuleList()\n",
       "    (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "    (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (tab_net): TabNetNoEmbeddings(\n",
       "      (initial_bn): BatchNorm1d(2, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (initial_splitter): FeatTransformer(\n",
       "        (shared): GLU_Block(\n",
       "          (shared_layers): ModuleList(\n",
       "            (0): Linear(in_features=2, out_features=32, bias=False)\n",
       "            (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "          )\n",
       "          (glu_layers): ModuleList(\n",
       "            (0): GLU_Layer(\n",
       "              (fc): Linear(in_features=2, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (specifics): GLU_Block(\n",
       "          (glu_layers): ModuleList(\n",
       "            (0): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feat_transformers): ModuleList(\n",
       "        (0): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=2, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=2, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=2, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=2, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=2, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=2, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (att_transformers): ModuleList(\n",
       "        (0): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=2, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(2, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (sp_max): Sparsemax()\n",
       "        )\n",
       "        (1): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=2, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(2, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (sp_max): Sparsemax()\n",
       "        )\n",
       "        (2): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=2, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(2, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (sp_max): Sparsemax()\n",
       "        )\n",
       "      )\n",
       "      (final_mapping): Linear(in_features=8, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=192, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TabNetModel??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_x = [True, False, True, True, False, True]\n",
    "xs = (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(has_x[:3]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(*xs):\n",
    "        print(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4212, 0.7626, 0.0298, 0.4731, 0.0086])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.rand(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "n_emb = sum(e.embedding_dim for e in embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionTimeD_Mixed(\n",
       "  (ts_mod): InceptionTime_NH(\n",
       "    (block): InceptionBlock(\n",
       "      (inc_mods): ModuleList(\n",
       "        (0): Inception(\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(6, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(6, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(6, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(6, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (1): Inception(\n",
       "          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (2): Inception(\n",
       "          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (3): Inception(\n",
       "          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (4): Inception(\n",
       "          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "        (5): Inception(\n",
       "          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (conv_layers): ModuleList(\n",
       "            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "          )\n",
       "          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (res_layers): ModuleList(\n",
       "        (0): None\n",
       "        (1): None\n",
       "        (2): Sequential(\n",
       "          (0): Conv1d(6, 128, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): None\n",
       "        (4): None\n",
       "        (5): Sequential(\n",
       "          (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (gap): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (sgm): Sigmoid()\n",
       "  (tab_mod): TabNetModel(\n",
       "    (embeds): ModuleList(\n",
       "      (0): Embedding(11, 4)\n",
       "    )\n",
       "    (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "    (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (tab_net): TabNetNoEmbeddings(\n",
       "      (initial_bn): BatchNorm1d(6, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (initial_splitter): FeatTransformer(\n",
       "        (shared): GLU_Block(\n",
       "          (shared_layers): ModuleList(\n",
       "            (0): Linear(in_features=6, out_features=32, bias=False)\n",
       "            (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "          )\n",
       "          (glu_layers): ModuleList(\n",
       "            (0): GLU_Layer(\n",
       "              (fc): Linear(in_features=6, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (specifics): GLU_Block(\n",
       "          (glu_layers): ModuleList(\n",
       "            (0): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): GLU_Layer(\n",
       "              (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "              (bn): GBN(\n",
       "                (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (feat_transformers): ModuleList(\n",
       "        (0): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=6, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=6, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=6, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=6, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): FeatTransformer(\n",
       "          (shared): GLU_Block(\n",
       "            (shared_layers): ModuleList(\n",
       "              (0): Linear(in_features=6, out_features=32, bias=False)\n",
       "              (1): Linear(in_features=16, out_features=32, bias=False)\n",
       "            )\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=6, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (specifics): GLU_Block(\n",
       "            (glu_layers): ModuleList(\n",
       "              (0): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): GLU_Layer(\n",
       "                (fc): Linear(in_features=16, out_features=32, bias=False)\n",
       "                (bn): GBN(\n",
       "                  (bn): BatchNorm1d(32, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (att_transformers): ModuleList(\n",
       "        (0): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=6, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(6, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (sp_max): Sparsemax()\n",
       "        )\n",
       "        (1): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=6, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(6, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (sp_max): Sparsemax()\n",
       "        )\n",
       "        (2): AttentiveTransformer(\n",
       "          (fc): Linear(in_features=8, out_features=6, bias=False)\n",
       "          (bn): GBN(\n",
       "            (bn): BatchNorm1d(6, eps=1e-05, momentum=0.02, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (sp_max): Sparsemax()\n",
       "        )\n",
       "      )\n",
       "      (final_mapping): Linear(in_features=8, out_features=64, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=192, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InceptionTimeD_Mixed(6,4,2,emb_szs=[(11,4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeVar(nn.Module):\n",
    "    '''\n",
    "    output mean and variance\n",
    "    regression model, sigmoid for the mean output optional\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out, meanrange=None):\n",
    "        super().__init__()\n",
    "        models  = [InceptionTime(n_in, n_out+1)]\n",
    "        if meanrange:\n",
    "            self.sigmoid = Sigmoid(*meanrange)\n",
    "        self.mod = nn.Sequential(*models)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        output = self.mod(x)\n",
    "        ## enforce positivity of sigma^2\n",
    "        ##output_sig_pos = tf.log(1 + tf.exp(output_sig)) + 1e-06\n",
    "#         output[:,-1] = (output[:,-1].exp()+1).log_() + 1e-06\n",
    "        output[:,-1] = F.softplus(output[:,-1].clone())\n",
    "        \n",
    "        if getattr(self, 'sigmoid', None): output[:,:-1] = self.sigmoid(output[:,:-1])\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model sanity checks\n",
    "xb = torch.randn((128,10,100))\n",
    "yb = torch.rand(128,1)\n",
    "model = InceptionTimeVar(10,1)\n",
    "\n",
    "preds = model(xb)\n",
    "assert preds.shape == (128,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def nll_regression(preds, y_true, c=5):\n",
    "    '''\n",
    "    negative log likelihood loss for regression, both mu and sigma are predicted\n",
    "    \n",
    "    Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles\n",
    "    Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell, DeepMind\n",
    "\n",
    "    '''\n",
    "    \n",
    "    s1 = 0.5*preds[:,1].log() \n",
    "    s2 = 0.5*(yb.squeeze()-preds[:,0]).pow(2).div(preds[:,1])\n",
    "    loss = (s1+s2).mean() + c\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def nll_leaky_loss(preds, y_true, c=5, alpha=0.5):\n",
    "    '''\n",
    "    leaky_loss with variance\n",
    "    \n",
    "    Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles\n",
    "    Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell, DeepMind\n",
    "\n",
    "    '''\n",
    "    \n",
    "    s1 = 0.5*preds[:,1].log() \n",
    "    l1 = -F.leaky_relu(preds[:,0], alpha)*y_true.float().squeeze()\n",
    "    s2 = 0.5*(l1.div(preds[:,1]+1)) ## +1 to prevent optimizing for variance, maybe just an artifical problem\n",
    "    loss = (s1+s2).mean() + c\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0285, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nll_leaky_loss(preds, yb)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = InceptionTimeVar(10,1,meanrange=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9921, grad_fn=<AddBackward0>)\n",
      "tensor(4.9292, grad_fn=<AddBackward0>)\n",
      "tensor(4.8599, grad_fn=<AddBackward0>)\n",
      "tensor(4.7835, grad_fn=<AddBackward0>)\n",
      "tensor(4.6996, grad_fn=<AddBackward0>)\n",
      "tensor(4.6080, grad_fn=<AddBackward0>)\n",
      "tensor(4.5083, grad_fn=<AddBackward0>)\n",
      "tensor(4.4009, grad_fn=<AddBackward0>)\n",
      "tensor(4.2859, grad_fn=<AddBackward0>)\n",
      "tensor(4.1640, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "#simple training loop\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "loss_fn = nll_regression\n",
    "m = model_var\n",
    "loss_fn = nll_leaky_loss\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for p in m.parameters():\n",
    "            p.sub_(lr*p.grad)\n",
    "    m.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unweighted_profit(m(xb)[:,0], yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = InceptionTimeVar(10,1)\n",
    "opt = torch.optim.Adam(model_var.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3211, grad_fn=<AddBackward0>)\n",
      "tensor(5.0782, grad_fn=<AddBackward0>)\n",
      "tensor(4.9166, grad_fn=<AddBackward0>)\n",
      "tensor(4.8056, grad_fn=<AddBackward0>)\n",
      "tensor(4.7254, grad_fn=<AddBackward0>)\n",
      "tensor(4.6706, grad_fn=<AddBackward0>)\n",
      "tensor(4.6308, grad_fn=<AddBackward0>)\n",
      "tensor(4.5937, grad_fn=<AddBackward0>)\n",
      "tensor(4.5542, grad_fn=<AddBackward0>)\n",
      "tensor(4.4943, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "m = model_var\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = nll_regression(preds, yb)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7368, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(4.7209, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(4.7053, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(4.6899, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(4.6746, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(4.6594, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(4.6443, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(4.6291, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(4.6138, grad_fn=<AddBackward0>)\n",
      "tensor(0.)\n",
      "tensor(4.5980, grad_fn=<AddBackward0>)\n",
      "tensor(0.0075)\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "model_var = InceptionTimeVar(10,1,meanrange=(-10,5))\n",
    "m = model_var\n",
    "loss_fn = partial(nll_leaky_loss, alpha=0.5)\n",
    "epochs=10\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    print(loss)\n",
    "    print(unweighted_profit(preds[:,0], yb))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0187)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unweighted_profit(m(xb)[:,0],yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24,  0.41],\n",
       "       [-0.77,  0.37],\n",
       "       [-1.81,  0.34],\n",
       "       [-3.6 ,  0.34],\n",
       "       [ 0.49,  0.42],\n",
       "       [-0.82,  0.41],\n",
       "       [-0.84,  0.4 ],\n",
       "       [-1.05,  0.35],\n",
       "       [-1.3 ,  0.34],\n",
       "       [-3.04,  0.29],\n",
       "       [-1.94,  0.36],\n",
       "       [-1.5 ,  0.37],\n",
       "       [-0.93,  0.4 ],\n",
       "       [-1.24,  0.39],\n",
       "       [-2.31,  0.34],\n",
       "       [-2.13,  0.35],\n",
       "       [-2.28,  0.34],\n",
       "       [-1.16,  0.4 ],\n",
       "       [-2.3 ,  0.37],\n",
       "       [-0.31,  0.4 ],\n",
       "       [-0.99,  0.39],\n",
       "       [-2.82,  0.32],\n",
       "       [-1.4 ,  0.36],\n",
       "       [-1.21,  0.38],\n",
       "       [-1.85,  0.35],\n",
       "       [-2.52,  0.31],\n",
       "       [-0.78,  0.39],\n",
       "       [-0.78,  0.35],\n",
       "       [-1.72,  0.36],\n",
       "       [-0.28,  0.42],\n",
       "       [-1.53,  0.39],\n",
       "       [-1.91,  0.38],\n",
       "       [-0.84,  0.38],\n",
       "       [ 0.08,  0.4 ],\n",
       "       [-1.78,  0.36],\n",
       "       [-2.64,  0.34],\n",
       "       [-1.59,  0.38],\n",
       "       [-0.46,  0.37],\n",
       "       [-2.69,  0.41],\n",
       "       [-1.29,  0.36],\n",
       "       [-1.29,  0.37],\n",
       "       [-0.14,  0.43],\n",
       "       [-0.64,  0.42],\n",
       "       [-1.41,  0.39],\n",
       "       [-1.29,  0.37],\n",
       "       [-1.66,  0.36],\n",
       "       [-0.98,  0.4 ],\n",
       "       [ 0.04,  0.43],\n",
       "       [-1.94,  0.39],\n",
       "       [-0.69,  0.41],\n",
       "       [-1.81,  0.37],\n",
       "       [-0.37,  0.39],\n",
       "       [-1.61,  0.36],\n",
       "       [-2.83,  0.29],\n",
       "       [-1.5 ,  0.31],\n",
       "       [-0.3 ,  0.4 ],\n",
       "       [-0.88,  0.38],\n",
       "       [-0.58,  0.42],\n",
       "       [-0.88,  0.38],\n",
       "       [-3.04,  0.31],\n",
       "       [-0.95,  0.35],\n",
       "       [ 0.15,  0.39],\n",
       "       [-1.81,  0.36],\n",
       "       [-0.62,  0.37],\n",
       "       [-2.63,  0.37],\n",
       "       [-0.46,  0.34],\n",
       "       [-0.07,  0.37],\n",
       "       [-0.61,  0.39],\n",
       "       [-2.88,  0.29],\n",
       "       [-3.02,  0.3 ],\n",
       "       [-1.92,  0.37],\n",
       "       [-1.98,  0.38],\n",
       "       [-0.3 ,  0.39],\n",
       "       [-2.23,  0.33],\n",
       "       [-1.89,  0.38],\n",
       "       [-2.58,  0.36],\n",
       "       [-2.81,  0.33],\n",
       "       [-2.72,  0.35],\n",
       "       [-0.56,  0.38],\n",
       "       [-0.85,  0.37],\n",
       "       [-1.04,  0.39],\n",
       "       [-0.22,  0.42],\n",
       "       [-0.72,  0.39],\n",
       "       [-1.29,  0.38],\n",
       "       [-1.56,  0.35],\n",
       "       [-0.99,  0.4 ],\n",
       "       [-1.27,  0.39],\n",
       "       [-1.19,  0.36],\n",
       "       [-1.76,  0.39],\n",
       "       [-1.79,  0.38],\n",
       "       [-1.83,  0.33],\n",
       "       [-1.94,  0.39],\n",
       "       [-1.05,  0.35],\n",
       "       [-2.46,  0.35],\n",
       "       [-2.79,  0.3 ],\n",
       "       [-2.23,  0.37],\n",
       "       [-0.42,  0.41],\n",
       "       [-1.69,  0.34],\n",
       "       [-2.06,  0.33],\n",
       "       [ 0.37,  0.45],\n",
       "       [-1.51,  0.38],\n",
       "       [-1.29,  0.39],\n",
       "       [-2.05,  0.34],\n",
       "       [-2.7 ,  0.32],\n",
       "       [-1.79,  0.36],\n",
       "       [-1.95,  0.37],\n",
       "       [-1.07,  0.37],\n",
       "       [-0.39,  0.4 ],\n",
       "       [-0.65,  0.39],\n",
       "       [-1.01,  0.34],\n",
       "       [-1.73,  0.41],\n",
       "       [-2.51,  0.36],\n",
       "       [-1.76,  0.36],\n",
       "       [-0.4 ,  0.39],\n",
       "       [-0.64,  0.42],\n",
       "       [-0.64,  0.41],\n",
       "       [-2.  ,  0.37],\n",
       "       [-2.64,  0.31],\n",
       "       [-0.63,  0.38],\n",
       "       [-0.8 ,  0.38],\n",
       "       [-0.19,  0.42],\n",
       "       [-1.85,  0.33],\n",
       "       [-0.77,  0.41],\n",
       "       [-0.  ,  0.38],\n",
       "       [-0.98,  0.39],\n",
       "       [-0.51,  0.37],\n",
       "       [-2.52,  0.34],\n",
       "       [-0.16,  0.41]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(m(xb).detach().numpy(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.zeros(128,1)\n",
    "x2 = torch.ones(128,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_p = torch.cat([x1,x2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1578)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_regression(x_p, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2412, 1.1038, 1.0968, 1.1289, 1.1712, 1.2208, 1.1951, 1.1454, 1.1252,\n",
       "        1.1933, 1.0675, 1.1149, 1.1066, 1.0956, 1.1696, 1.0552, 1.0579, 0.9636,\n",
       "        1.0129, 1.0893, 0.9862, 1.0107, 1.0038, 1.0636, 1.1439, 0.9745, 1.1000,\n",
       "        1.2131, 1.1016, 1.2258, 1.1087, 1.2406, 1.0853, 1.1349, 1.0362, 1.1309,\n",
       "        1.0748, 1.2192, 0.9677, 1.0714, 1.2125, 0.9704, 1.0656, 1.1497, 1.1989,\n",
       "        1.0258, 1.0785, 1.2363, 1.1467, 1.2652, 1.1868, 1.0978, 1.2464, 1.0098,\n",
       "        1.2476, 1.1435, 1.1949, 1.2127, 1.1184, 1.0172, 1.1773, 1.0714, 1.2171,\n",
       "        1.2256, 1.0014, 1.1111, 1.0696, 0.9619, 1.0394, 1.0350, 1.1551, 1.2075,\n",
       "        1.1798, 1.0780, 1.0359, 1.1595, 1.1368, 1.2355, 1.0904, 1.1077, 1.2141,\n",
       "        1.1579, 1.2191, 0.9764, 1.1609, 1.1440, 1.1462, 1.0915, 1.2595, 1.1571,\n",
       "        1.0948, 1.0177, 1.0333, 1.1375, 1.0431, 1.1731, 1.1842, 1.1336, 1.0594,\n",
       "        0.9231])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=torch.randn(100)\n",
    "torch.sigmoid(t)*(1.3-0.9)+0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8529, 0.5096, 0.4920, 0.5722, 0.6781, 0.8021, 0.7378, 0.6134, 0.5630,\n",
       "        0.7333, 0.4188, 0.5373, 0.5166, 0.4889, 0.6739, 0.3879, 0.3948, 0.1589,\n",
       "        0.2823, 0.4732, 0.2154, 0.2768, 0.2596, 0.4089, 0.6098, 0.1863, 0.4999,\n",
       "        0.7829, 0.5040, 0.8145, 0.5218, 0.8515, 0.4632, 0.5873, 0.3405, 0.5772,\n",
       "        0.4370, 0.7979, 0.1692, 0.4284, 0.7813, 0.1761, 0.4139, 0.6243, 0.7473,\n",
       "        0.3146, 0.4463, 0.8406, 0.6166, 0.9130, 0.7170, 0.4946, 0.8661, 0.2745,\n",
       "        0.8691, 0.6086, 0.7372, 0.7816, 0.5459, 0.2930, 0.6932, 0.4285, 0.7927,\n",
       "        0.8141, 0.2535, 0.5278, 0.4240, 0.1548, 0.3485, 0.3376, 0.6376, 0.7688,\n",
       "        0.6995, 0.4450, 0.3398, 0.6487, 0.5921, 0.8388, 0.4760, 0.5193, 0.7853,\n",
       "        0.6446, 0.7977, 0.1909, 0.6523, 0.6101, 0.6155, 0.4787, 0.8987, 0.6428,\n",
       "        0.4870, 0.2943, 0.3332, 0.5939, 0.3577, 0.6827, 0.7106, 0.5839, 0.3985,\n",
       "        0.0578])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_random_results(size):\n",
    "    high, low = 1.2, 0.9\n",
    "    res=torch.randn(size)\n",
    "    res = torch.sigmoid(res)*(high-low)+low\n",
    "    res *= -100\n",
    "    res[torch.rand(size)>0.5] = 100.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb = _create_random_results((128,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0676, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_leaky_loss(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.7970)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = InceptionTimeVar(10,1,meanrange=(-1,1))\n",
    "m = model_var\n",
    "loss_fn = partial(nll_leaky_loss, alpha=0.5)\n",
    "epochs=20\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5096, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7970)\n",
      "tensor(5.1599, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7970)\n",
      "tensor(4.8130, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7970)\n",
      "tensor(4.4551, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7970)\n",
      "tensor(4.0745, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7970)\n",
      "tensor(3.6592, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7970)\n",
      "tensor(3.1964, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7970)\n",
      "tensor(2.6728, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7970)\n",
      "tensor(2.0750, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7970)\n",
      "tensor(1.3914, grad_fn=<AddBackward0>)\n",
      "tensor(-3.7970)\n",
      "tensor(0.6170, grad_fn=<AddBackward0>)\n",
      "tensor(-3.6784)\n",
      "tensor(-0.2108, grad_fn=<AddBackward0>)\n",
      "tensor(-3.4114)\n",
      "tensor(-1.0847, grad_fn=<AddBackward0>)\n",
      "tensor(-3.2334)\n",
      "tensor(-1.9718, grad_fn=<AddBackward0>)\n",
      "tensor(-2.7291)\n",
      "tensor(-2.8243, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3731)\n",
      "tensor(-3.6813, grad_fn=<AddBackward0>)\n",
      "tensor(-2.3138)\n",
      "tensor(-4.5413, grad_fn=<AddBackward0>)\n",
      "tensor(-2.1655)\n",
      "tensor(-5.3586, grad_fn=<AddBackward0>)\n",
      "tensor(-2.0172)\n",
      "tensor(-6.1124, grad_fn=<AddBackward0>)\n",
      "tensor(-1.9875)\n",
      "tensor(-6.8013, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8985)\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    print(loss)\n",
    "    print(unweighted_profit(preds[:,0], yb))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.4223, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8985)\n",
      "tensor(-7.9829, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-8.4926, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-8.9576, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-9.3802, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-9.7627, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-10.1091, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-10.4264, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-10.7213, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-11.0006, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-11.2681, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-11.5253, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-11.7730, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-12.0138, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-12.2503, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-12.4847, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-12.7178, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-12.9498, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-13.1822, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n",
      "tensor(-13.4142, grad_fn=<AddBackward0>)\n",
      "tensor(-1.8688)\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    print(loss)\n",
    "    print(unweighted_profit(preds[:,0], yb))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6247,  1.3449],\n",
       "        [-0.6335,  1.3496],\n",
       "        [ 0.9159,  0.4525],\n",
       "        [ 0.8944,  0.5261],\n",
       "        [ 0.9143,  0.4376],\n",
       "        [ 0.9153,  0.4637],\n",
       "        [-0.6126,  1.3296],\n",
       "        [ 0.9140,  0.4445],\n",
       "        [-0.5532,  1.3306],\n",
       "        [-0.6109,  1.3950],\n",
       "        [-0.6027,  1.3360],\n",
       "        [-0.6743,  1.3920],\n",
       "        [ 0.9251,  0.4406],\n",
       "        [ 0.8916,  0.4790],\n",
       "        [ 0.9104,  0.4841],\n",
       "        [-0.6220,  1.4334],\n",
       "        [-0.6139,  1.3183],\n",
       "        [ 0.8935,  0.4824],\n",
       "        [-0.6616,  1.3746],\n",
       "        [ 0.8775,  0.5117],\n",
       "        [-0.6336,  1.3861],\n",
       "        [ 0.8849,  0.4871],\n",
       "        [-0.4994,  1.2708],\n",
       "        [ 0.8904,  0.5076],\n",
       "        [-0.5878,  1.3920],\n",
       "        [ 0.8754,  0.5185],\n",
       "        [-0.6419,  1.3675],\n",
       "        [ 0.8800,  0.5075],\n",
       "        [-0.5670,  1.2860],\n",
       "        [ 0.9170,  0.4520],\n",
       "        [ 0.9310,  0.4050],\n",
       "        [ 0.9005,  0.4669],\n",
       "        [-0.6053,  1.3027],\n",
       "        [-0.6309,  1.3387],\n",
       "        [ 0.9197,  0.4594],\n",
       "        [ 0.9192,  0.4601],\n",
       "        [ 0.8661,  0.5524],\n",
       "        [-0.6225,  1.3228],\n",
       "        [-0.6303,  1.4104],\n",
       "        [-0.6104,  1.2661],\n",
       "        [-0.6933,  1.4350],\n",
       "        [-0.5877,  1.3820],\n",
       "        [ 0.9110,  0.4778],\n",
       "        [-0.6197,  1.2872],\n",
       "        [ 0.8965,  0.5019],\n",
       "        [ 0.8792,  0.5109],\n",
       "        [ 0.8990,  0.4648],\n",
       "        [ 0.8972,  0.4815],\n",
       "        [-0.6613,  1.3753],\n",
       "        [ 0.8970,  0.5049],\n",
       "        [-0.6133,  1.4095],\n",
       "        [ 0.9155,  0.4484],\n",
       "        [-0.6005,  1.3517],\n",
       "        [ 0.8867,  0.5058],\n",
       "        [-0.5530,  1.2682],\n",
       "        [ 0.8920,  0.5115],\n",
       "        [ 0.8989,  0.5164],\n",
       "        [-0.5227,  1.2632],\n",
       "        [-0.4952,  1.2499],\n",
       "        [ 0.9102,  0.4895],\n",
       "        [ 0.8962,  0.4724],\n",
       "        [-0.6255,  1.3138],\n",
       "        [-0.6196,  1.4047],\n",
       "        [-0.5859,  1.3288],\n",
       "        [-0.5864,  1.3144],\n",
       "        [ 0.9097,  0.4525],\n",
       "        [ 0.8896,  0.4811],\n",
       "        [ 0.8899,  0.5332],\n",
       "        [ 0.9156,  0.4441],\n",
       "        [-0.6353,  1.3613],\n",
       "        [ 0.8925,  0.4815],\n",
       "        [ 0.9195,  0.4566],\n",
       "        [-0.6244,  1.3391],\n",
       "        [ 0.9141,  0.4341],\n",
       "        [-0.6148,  1.3583],\n",
       "        [ 0.9013,  0.4819],\n",
       "        [-0.6498,  1.3702],\n",
       "        [-0.6022,  1.4135],\n",
       "        [ 0.9283,  0.4278],\n",
       "        [-0.7026,  1.3763],\n",
       "        [-0.6602,  1.3407],\n",
       "        [ 0.9397,  0.4238],\n",
       "        [ 0.8835,  0.4979],\n",
       "        [ 0.8890,  0.5239],\n",
       "        [ 0.8867,  0.4886],\n",
       "        [-0.6548,  1.3408],\n",
       "        [ 0.8856,  0.4944],\n",
       "        [ 0.9037,  0.4726],\n",
       "        [ 0.9140,  0.4526],\n",
       "        [-0.6353,  1.3967],\n",
       "        [ 0.9110,  0.4631],\n",
       "        [ 0.9248,  0.4223],\n",
       "        [ 0.9146,  0.4530],\n",
       "        [-0.6635,  1.4301],\n",
       "        [-0.6996,  1.3852],\n",
       "        [-0.6079,  1.3183],\n",
       "        [-0.6285,  1.3827],\n",
       "        [-0.5611,  1.2883],\n",
       "        [-0.5338,  1.2964],\n",
       "        [-0.5741,  1.2744],\n",
       "        [-0.6500,  1.3638],\n",
       "        [-0.5993,  1.3542],\n",
       "        [-0.6248,  1.3410],\n",
       "        [-0.6420,  1.3455],\n",
       "        [-0.6700,  1.3785],\n",
       "        [ 0.8914,  0.4919],\n",
       "        [-0.5831,  1.2996],\n",
       "        [-0.6475,  1.4053],\n",
       "        [-0.5793,  1.3525],\n",
       "        [ 0.9162,  0.4538],\n",
       "        [ 0.8774,  0.5216],\n",
       "        [-0.6163,  1.3266],\n",
       "        [ 0.8972,  0.4571],\n",
       "        [-0.5208,  1.2671],\n",
       "        [-0.6606,  1.4231],\n",
       "        [ 0.9039,  0.5137],\n",
       "        [-0.5814,  1.2875],\n",
       "        [ 0.9050,  0.4772],\n",
       "        [ 0.9169,  0.4297],\n",
       "        [-0.5544,  1.3397],\n",
       "        [-0.6652,  1.4605],\n",
       "        [ 0.9059,  0.4463],\n",
       "        [-0.6098,  1.2855],\n",
       "        [ 0.9044,  0.4586],\n",
       "        [-0.7010,  1.4750],\n",
       "        [ 0.9084,  0.4847],\n",
       "        [ 0.9330,  0.3956],\n",
       "        [ 0.8842,  0.5259]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0298, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n",
      "tensor(-0.0697, grad_fn=<MulBackward0>)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "#cuda\n",
    "model_mean = InceptionTimeSgm(10,1, range=(-1,1))\n",
    "m = model_mean\n",
    "loss_fn = partial(leaky_loss, alpha=0.1)\n",
    "# loss_fn = F.mse_loss\n",
    "\n",
    "epochs=2\n",
    "\n",
    "opt = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = loss_fn(preds, yb)\n",
    "    print(loss)\n",
    "    print(unweighted_profit(preds, yb))\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22],\n",
       "       [-0.17],\n",
       "       [-0.16],\n",
       "       [-0.18],\n",
       "       [-0.23],\n",
       "       [-0.18],\n",
       "       [-0.19],\n",
       "       [-0.17],\n",
       "       [-0.18],\n",
       "       [-0.18],\n",
       "       [-0.15],\n",
       "       [-0.16],\n",
       "       [-0.23],\n",
       "       [-0.21],\n",
       "       [-0.16],\n",
       "       [-0.18],\n",
       "       [-0.21],\n",
       "       [-0.2 ],\n",
       "       [-0.2 ],\n",
       "       [-0.16],\n",
       "       [-0.18],\n",
       "       [-0.2 ],\n",
       "       [-0.16],\n",
       "       [-0.14],\n",
       "       [-0.19],\n",
       "       [-0.2 ],\n",
       "       [-0.18],\n",
       "       [-0.2 ],\n",
       "       [-0.21],\n",
       "       [-0.16],\n",
       "       [-0.19],\n",
       "       [-0.19],\n",
       "       [-0.18],\n",
       "       [-0.15],\n",
       "       [-0.18],\n",
       "       [-0.2 ],\n",
       "       [-0.18],\n",
       "       [-0.17],\n",
       "       [-0.19],\n",
       "       [-0.15],\n",
       "       [-0.17],\n",
       "       [-0.17],\n",
       "       [-0.17],\n",
       "       [-0.15],\n",
       "       [-0.2 ],\n",
       "       [-0.18],\n",
       "       [-0.22],\n",
       "       [-0.22],\n",
       "       [-0.21],\n",
       "       [-0.16],\n",
       "       [-0.22],\n",
       "       [-0.14],\n",
       "       [-0.19],\n",
       "       [-0.16],\n",
       "       [-0.17],\n",
       "       [-0.18],\n",
       "       [-0.17],\n",
       "       [-0.17],\n",
       "       [-0.22],\n",
       "       [-0.17],\n",
       "       [-0.18],\n",
       "       [-0.18],\n",
       "       [-0.13],\n",
       "       [-0.17],\n",
       "       [-0.17],\n",
       "       [-0.2 ],\n",
       "       [-0.17],\n",
       "       [-0.2 ],\n",
       "       [-0.13],\n",
       "       [-0.18],\n",
       "       [-0.19],\n",
       "       [-0.18],\n",
       "       [-0.21],\n",
       "       [-0.21],\n",
       "       [-0.15],\n",
       "       [-0.19],\n",
       "       [-0.21],\n",
       "       [-0.14],\n",
       "       [-0.2 ],\n",
       "       [-0.21],\n",
       "       [-0.21],\n",
       "       [-0.21],\n",
       "       [-0.17],\n",
       "       [-0.19],\n",
       "       [-0.17],\n",
       "       [-0.23],\n",
       "       [-0.18],\n",
       "       [-0.2 ],\n",
       "       [-0.16],\n",
       "       [-0.19],\n",
       "       [-0.21],\n",
       "       [-0.19],\n",
       "       [-0.18],\n",
       "       [-0.18],\n",
       "       [-0.22],\n",
       "       [-0.17],\n",
       "       [-0.2 ],\n",
       "       [-0.18],\n",
       "       [-0.21],\n",
       "       [-0.19],\n",
       "       [-0.18],\n",
       "       [-0.17],\n",
       "       [-0.2 ],\n",
       "       [-0.17],\n",
       "       [-0.21],\n",
       "       [-0.21],\n",
       "       [-0.2 ],\n",
       "       [-0.16],\n",
       "       [-0.18],\n",
       "       [-0.18],\n",
       "       [-0.19],\n",
       "       [-0.15],\n",
       "       [-0.17],\n",
       "       [-0.2 ],\n",
       "       [-0.15],\n",
       "       [-0.19],\n",
       "       [-0.19],\n",
       "       [-0.18],\n",
       "       [-0.18],\n",
       "       [-0.17],\n",
       "       [-0.17],\n",
       "       [-0.2 ],\n",
       "       [-0.21],\n",
       "       [-0.13],\n",
       "       [-0.19],\n",
       "       [-0.17],\n",
       "       [-0.21],\n",
       "       [-0.17]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(preds.detach().numpy(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_profit(preds, yb, threshold=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = F.leaky_relu(preds[:,0], 0.5)*yb.float().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3143, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unweighted_profit(preds[:,0], yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0022, -0.0017, -0.0016, -0.0018, -0.0023, -0.0018, -0.0019, -0.0017,\n",
       "        -0.0018, -0.0018, -0.0015, -0.0016, -0.0023, -0.0021, -0.0016, -0.0018,\n",
       "        -0.0021, -0.0020, -0.0020, -0.0016, -0.0018, -0.0020, -0.0016, -0.0014,\n",
       "        -0.0019, -0.0020, -0.0018, -0.0020, -0.0021, -0.0016, -0.0019, -0.0019,\n",
       "        -0.0018, -0.0015, -0.0018, -0.0020, -0.0018, -0.0017, -0.0019, -0.0015,\n",
       "        -0.0017, -0.0017, -0.0017, -0.0015, -0.0020, -0.0018, -0.0022, -0.0022,\n",
       "        -0.0021, -0.0016, -0.0022, -0.0014, -0.0019, -0.0016, -0.0017, -0.0018,\n",
       "        -0.0017, -0.0017, -0.0022, -0.0017, -0.0018, -0.0018, -0.0013, -0.0017,\n",
       "        -0.0017, -0.0020, -0.0017, -0.0020, -0.0013, -0.0018, -0.0019, -0.0018,\n",
       "        -0.0021, -0.0021, -0.0015, -0.0019, -0.0021, -0.0014, -0.0020, -0.0021,\n",
       "        -0.0021, -0.0021, -0.0017, -0.0019, -0.0017, -0.0023, -0.0018, -0.0020,\n",
       "        -0.0016, -0.0019, -0.0021, -0.0019, -0.0018, -0.0018, -0.0022, -0.0017,\n",
       "        -0.0020, -0.0018, -0.0021, -0.0019, -0.0018, -0.0017, -0.0020, -0.0017,\n",
       "        -0.0021, -0.0021, -0.0020, -0.0016, -0.0018, -0.0018, -0.0019, -0.0015,\n",
       "        -0.0017, -0.0020, -0.0015, -0.0019, -0.0019, -0.0018, -0.0018, -0.0017,\n",
       "        -0.0017, -0.0020, -0.0021, -0.0013, -0.0019, -0.0017, -0.0021, -0.0017],\n",
       "       grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.leaky_relu(preds[:,0], 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 11.9359,   8.0182,  -7.7866,  -9.0861, -11.4707,  -8.9106,  10.3597,\n",
       "         -8.5555,   9.3440,   8.4255,   7.4203,   8.2735, -11.3047, -10.5139,\n",
       "         -7.8700,   9.4019,  10.7593, -10.0267,  11.5149,  -7.8406,  10.6564,\n",
       "        -10.2022,   7.4194,  -7.1230,   9.0873,  -9.9591,   8.8141, -10.2012,\n",
       "         10.9219,  -8.2058,  -9.4535,  -9.6962,   9.5209,   8.0113,  -9.0895,\n",
       "        -10.1661,  -8.7832,   9.3378,  10.4465,   8.1611,   9.0926,   7.9269,\n",
       "         -8.4618,   7.2584,  -9.9300,  -9.0956, -10.9277, -11.0166,  11.3946,\n",
       "         -8.0560,  11.3752,  -6.8826,   9.9466,  -8.2181,   9.5487,  -9.1482,\n",
       "         -8.7303,   8.3823,  11.5327,  -8.7412,  -9.1766,   9.8428,   6.7408,\n",
       "          7.8306,   9.1732, -10.1838,  -8.7309,  -9.9286,  -6.6186,   9.1171,\n",
       "         -9.2919,  -8.9576,  10.8023, -10.2791,   8.7763,  -9.3696,  10.8481,\n",
       "          7.8006, -10.1285,  12.4134,  10.9164, -10.6060,  -8.6401,  -9.4403,\n",
       "         -8.5532,  12.2043,  -9.0122, -10.1679,  -8.2355,  10.3118, -10.2801,\n",
       "         -9.5576,  -8.9077,  10.0318,  11.9753,   8.5680,  10.8382,   9.1783,\n",
       "         10.4968,  10.2423,   9.8806,   8.3278,  10.1277,   8.3497,  11.6046,\n",
       "        -10.2534,   9.5348,   7.6696,   9.7276,  -8.7972,  -9.4388,   8.1709,\n",
       "         -8.5460,  10.2070,   7.6625,  -9.6280,   9.3722,  -8.7837,  -9.2491,\n",
       "          8.4864,   9.5940, -10.0189,  10.2395,  -6.7206,   9.1686,  -8.3190,\n",
       "        -10.3654,  -8.6477], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QD loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_results = _create_random_results(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -94.4114, -115.2035,  100.0000,  100.0000, -105.0282, -111.8546,\n",
       "         100.0000,  100.0000, -102.5273, -107.5752])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lower=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_upper = preds + torch.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hard\n",
    "y_toy = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khu = (torch.sign(y_upper-y_toy) > 0).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "khl = (torch.sign(y_toy-y_lower) > 0).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##soft\n",
    "s = 10\n",
    "ksu = torch.sigmoid((y_upper-y_toy)*s)\n",
    "ksl = torch.sigmoid((y_toy-y_lower)*s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.    , 1.    , 1.    , 1.    , 1.    , 0.0043, 0.9947, 0.0039,\n",
       "       1.    , 0.0485], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(ksu.numpy(),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.   , 0.061, 0.   , 0.   , 0.001, 0.999, 0.028, 1.   , 0.   ,\n",
       "       1.   ], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(ksl.numpy(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##soft\n",
    "s = 10\n",
    "ksu = torch.sigmoid((y_upper-y_results)*s)\n",
    "ksl = torch.sigmoid((y_results-y_lower)*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0., 0., 1., 1., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ksu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 1., 0., 0., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ksl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -94.4114, -115.2035,  100.0000,  100.0000, -105.0282, -111.8546,\n",
       "         100.0000,  100.0000, -102.5273, -107.5752])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3337,  0.9843,  1.1095, -0.3597,  1.0252, -1.1859, -0.4918,  0.0426,\n",
       "         0.7183, -0.3774])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7789, 0.2434, 0.7520, 0.4110, 0.2541, 0.7903, 0.3795, 0.5107, 0.3238,\n",
       "        0.6001])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(y_upper*y_results*0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betting adaption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given results targets `y_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -94.4114, -115.2035,  100.0000,  100.0000, -105.0282, -111.8546,\n",
       "         100.0000,  100.0000, -102.5273, -107.5752])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the NN predicts upper and lower bounds `y_upper`, `y_lower`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.3337,  0.9843,  1.1095, -0.3597,  1.0252, -1.1859, -0.4918,  0.0426,\n",
       "          0.7183, -0.3774]),\n",
       " tensor([-1.4573,  0.1762,  1.0387, -0.9068,  0.2352, -1.3277, -0.6610, -0.3312,\n",
       "         -0.1677, -0.9362]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_upper, y_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to leaky loss, we want both bounds to have the same sign as the target `y_results` and higher absolute values for more confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def qd_loss(preds, y_true, alpha=0.4, l=0.01, s=0.01, add=False, slope=1.):\n",
    "    '''\n",
    "    qd loss implementation adapted for \"leaky loss problems\"\n",
    "    preds: predictions for both lower and upper bounds\n",
    "    alpha: confidence intervall parameter, different from alpha in leaky_loss\n",
    "    s: smoothing factor for sigmoid\n",
    "    l: agrangian controlling width vs coverage (default in the paper impl. is 0.01 which seems lowI)\n",
    "    '''\n",
    "    ll = lambda x: F.leaky_relu(x, negative_slope=slope)\n",
    "    \n",
    "    y_lower = preds[:,0].clone()\n",
    "    y_upper = preds[:,1].clone() if not add else y_lower+preds[:,1]\n",
    "    \n",
    "#     if not add:\n",
    "#         y_lower, y_upper = preds[:, 0].clone(), preds[:, 1].clone()\n",
    "#     else:\n",
    "#         y_lower, y_upper = preds[:, 0].clone(), preds[:,0].clone()+preds[:, 1].clone()\n",
    "# #     hard counts, how many of the predictions have the right sign?\n",
    "    khu = (torch.sign(y_upper*y_true) > 0).int()\n",
    "    khl = (torch.sign(y_lower*y_true) > 0).int()\n",
    "    \n",
    "#     return preds.mean()\n",
    "    # soft counts, sign step function replaced by a smoother sigmoid\n",
    "    \n",
    "    ksu = torch.sigmoid((ll(y_upper)*y_true)*s)\n",
    "    ksl = torch.sigmoid((y_true*ll(y_lower))*s)\n",
    "    kh,ks = khu*khl, ksu*ksl\n",
    "#     print(kh)\n",
    "#     print(kh.sum(), ks.sum())\n",
    "    \n",
    "    #mpiw: mean predicted interval width\n",
    "    f = 1/(kh.sum()+1e-6)\n",
    "#     print((y_upper-y_lower))\n",
    "    mpiw = ((y_upper-y_lower)*kh).sum()*f\n",
    "    \n",
    "    #picp: predicted interval coverage probability\n",
    "    picp_s = ks.mean()\n",
    "    \n",
    "    print(f'mpiw {mpiw}, pcip_soft: {picp_s}')\n",
    "    s2 = l*preds.shape[0]/(alpha*(1-alpha))\n",
    "    s3 = torch.max(torch.zeros(1, device=preds.device), picp_s).pow(2)\n",
    "    loss_s = mpiw + l*preds.shape[0]/(alpha*(1-alpha)) * torch.max(torch.zeros(1, device=preds.device), \n",
    "                                                                   picp_s).pow(2)\n",
    "    return loss_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb1af9cb8d0>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(F.softplus(torch.arange(-5,5).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johannes/anaconda3/envs/nbdev/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb19bf31a10>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(F.softmax(torch.arange(-5,5).float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeBounds(nn.Module):\n",
    "    '''\n",
    "    use InceptionTimeVar implementation for bounds\n",
    "    output[:, -1] is positive and y_upper corresponds to output[:,0]+output[:,1] --> loss\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out, meanrange=None):\n",
    "        super().__init__()\n",
    "        models  = [InceptionTime(n_in, n_out+1)]\n",
    "        if meanrange:\n",
    "            self.sigmoid = Sigmoid(*meanrange)\n",
    "        self.mod = nn.Sequential(*models)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        output = self.mod(x)\n",
    "        ## enforce positivity of sigma^2\n",
    "        ##output_sig_pos = tf.log(1 + tf.exp(output_sig)) + 1e-06\n",
    "#         output[:,-1] = (output[:,-1].exp()+1).log_() + 1e-06\n",
    "        output[:,-1] = F.softplus(output[:,-1].clone())  ## autograd problems when not using clone, why???\n",
    "        \n",
    "        if getattr(self, 'sigmoid', None): output[:,:-1] = self.sigmoid(output[:,:-1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionTimeBounds(10,1, meanrange=(-3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = _create_random_results(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-106.7638,  100.0000, -100.0458,  100.0000,  100.0000, -109.3803,\n",
       "        -107.8266,  100.0000,  100.0000, -113.7330, -110.4757,  100.0000,\n",
       "         100.0000,  100.0000,  -99.7512,  100.0000, -108.8533,  -98.5843,\n",
       "        -101.6713, -102.8774, -110.6917,  100.0000,  -96.2356,  100.0000,\n",
       "         100.0000, -104.1054,  100.0000,  100.0000,  100.0000,  100.0000,\n",
       "         -99.7910, -110.3254, -109.7371,  100.0000,  100.0000, -101.7797,\n",
       "         -94.8627,  100.0000,  100.0000,  100.0000, -110.1486,  100.0000,\n",
       "        -116.1869, -102.9370,  -95.7134, -116.2622, -107.7277,  -92.6611,\n",
       "         100.0000,  100.0000,  100.0000, -110.8954, -100.5063,  100.0000,\n",
       "        -106.7785,  100.0000,  100.0000, -117.0415,  -98.9544, -101.8524,\n",
       "         100.0000,  100.0000,  100.0000,  100.0000, -104.1076, -115.2596,\n",
       "         100.0000,  -98.1928, -104.9685, -113.3826,  100.0000,  100.0000,\n",
       "        -111.9894, -105.9567,  100.0000,  -95.4433,  100.0000, -112.9684,\n",
       "        -106.2161, -104.0756,  100.0000,  100.0000, -103.9313,  100.0000,\n",
       "         -99.1358,  -97.0312,  100.0000, -105.8008,  100.0000,  100.0000,\n",
       "         -98.7198,  -96.6286,  100.0000,  -94.7076,  -99.1611,  100.0000,\n",
       "         100.0000,  100.0000,  100.0000,  100.0000, -113.9651,  100.0000,\n",
       "         100.0000,  -97.8544,  100.0000, -109.2917,  100.0000,  100.0000,\n",
       "         -96.1180,  100.0000,  100.0000, -104.8634, -114.1572,  100.0000,\n",
       "         100.0000, -118.1289,  100.0000, -113.3931,  100.0000, -112.7533,\n",
       "         100.0000,  100.0000,  100.0000, -114.2994,  100.0000, -105.1760,\n",
       "         -93.4807, -108.4077])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpiw 0.5742294788360596, pcip_soft: 0.2512679100036621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9110], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_loss(model(xb), y_true, add=True, slope=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbdev]",
   "language": "python",
   "name": "conda-env-nbdev-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
