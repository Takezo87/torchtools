{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@gmail.com based on:\n",
    "\n",
    "# Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... & Petitjean, F. (2019). InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.\n",
    "# Official InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime\n",
    "\n",
    "\n",
    "def noop(x):\n",
    "    return x\n",
    "\n",
    "def shortcut(c_in, c_out):\n",
    "    return nn.Sequential(*[nn.Conv1d(c_in, c_out, kernel_size=1), \n",
    "                           nn.BatchNorm1d(c_out)])\n",
    "    \n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, c_in, bottleneck=32, ks=40, nb_filters=32):\n",
    "\n",
    "        super().__init__()\n",
    "        self.bottleneck = nn.Conv1d(c_in, bottleneck, 1) if bottleneck and c_in > 1 else noop\n",
    "        mts_feat = bottleneck or c_in\n",
    "        conv_layers = []\n",
    "        kss = [ks // (2**i) for i in range(3)]\n",
    "        # ensure odd kss until nn.Conv1d with padding='same' is available in pytorch 1.3\n",
    "        kss = [ksi if ksi % 2 != 0 else ksi - 1 for ksi in kss]  \n",
    "        for i in range(len(kss)):\n",
    "            conv_layers.append(\n",
    "                nn.Conv1d(mts_feat, nb_filters, kernel_size=kss[i], padding=kss[i] // 2))\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.maxpool = nn.MaxPool1d(3, stride=1, padding=1)\n",
    "        self.conv = nn.Conv1d(c_in, nb_filters, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm1d(nb_filters * 4)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = x\n",
    "        x = self.bottleneck(input_tensor)\n",
    "        for i in range(3):\n",
    "            out_ = self.conv_layers[i](x)\n",
    "            if i == 0: out = out_\n",
    "            else: out = torch.cat((out, out_), 1)\n",
    "        mp = self.conv(self.maxpool(input_tensor))\n",
    "        inc_out = torch.cat((out, mp), 1)\n",
    "        return self.act(self.bn(inc_out))\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self,c_in,bottleneck=32,ks=40,nb_filters=32,residual=True,depth=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = residual\n",
    "        self.depth = depth\n",
    "\n",
    "        #inception & residual layers\n",
    "        inc_mods = []\n",
    "        res_layers = []\n",
    "        res = 0\n",
    "        for d in range(depth):\n",
    "            inc_mods.append(\n",
    "                Inception(c_in if d == 0 else nb_filters * 4, bottleneck=bottleneck if d > 0 else 0,ks=ks,\n",
    "                          nb_filters=nb_filters))\n",
    "            if self.residual and d % 3 == 2:\n",
    "                res_layers.append(shortcut(c_in if res == 0 else nb_filters * 4, nb_filters * 4))\n",
    "                res += 1\n",
    "            else: res_layer = res_layers.append(None)\n",
    "        self.inc_mods = nn.ModuleList(inc_mods)\n",
    "        self.res_layers = nn.ModuleList(res_layers)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for d, l in enumerate(range(self.depth)):\n",
    "            x = self.inc_mods[d](x)\n",
    "            if self.residual and d % 3 == 2:\n",
    "                res = self.res_layers[d](res)\n",
    "                x += res\n",
    "                res = x\n",
    "                x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "class InceptionTime(nn.Module):\n",
    "    def __init__(self,c_in,c_out,bottleneck=32,ks=40,nb_filters=32,residual=True,depth=6):\n",
    "        super().__init__()\n",
    "        self.block = InceptionBlock(c_in,bottleneck=bottleneck,ks=ks,nb_filters=nb_filters,\n",
    "                                    residual=residual,depth=depth)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(nb_filters * 4, c_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeSgm(nn.Module):\n",
    "    '''\n",
    "    add a sigmoid layer to InceptionTime to get the ouput in a certain range\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.inc = InceptionTime(n_in, n_out)\n",
    "        self.sig = SigmoidRange(-1,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sig(self.inc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbdev]",
   "language": "python",
   "name": "conda-env-nbdev-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
