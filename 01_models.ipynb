{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from torchtools.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch.nn as nn\n",
    "import torch as torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7616])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.tensor([-2.])) * (1 - -1) + -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@gmail.com based on:\n",
    "\n",
    "# Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... & Petitjean, F. (2019). InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.\n",
    "# Official InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime\n",
    "\n",
    "\n",
    "def noop(x):\n",
    "    return x\n",
    "\n",
    "def shortcut(c_in, c_out):\n",
    "    return nn.Sequential(*[nn.Conv1d(c_in, c_out, kernel_size=1), \n",
    "                           nn.BatchNorm1d(c_out)])\n",
    "    \n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, c_in, bottleneck=32, ks=40, nb_filters=32):\n",
    "\n",
    "        super().__init__()\n",
    "        self.bottleneck = nn.Conv1d(c_in, bottleneck, 1) if bottleneck and c_in > 1 else noop\n",
    "        mts_feat = bottleneck or c_in\n",
    "        conv_layers = []\n",
    "        kss = [ks // (2**i) for i in range(3)]\n",
    "        # ensure odd kss until nn.Conv1d with padding='same' is available in pytorch 1.3\n",
    "        kss = [ksi if ksi % 2 != 0 else ksi - 1 for ksi in kss]  \n",
    "        for i in range(len(kss)):\n",
    "            conv_layers.append(\n",
    "                nn.Conv1d(mts_feat, nb_filters, kernel_size=kss[i], padding=kss[i] // 2))\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.maxpool = nn.MaxPool1d(3, stride=1, padding=1)\n",
    "        self.conv = nn.Conv1d(c_in, nb_filters, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm1d(nb_filters * 4)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = x.to(torch.float)\n",
    "        x = self.bottleneck(input_tensor)\n",
    "        for i in range(3):\n",
    "            out_ = self.conv_layers[i](x)\n",
    "            if i == 0: out = out_\n",
    "            else: out = torch.cat((out, out_), 1)\n",
    "        mp = self.conv(self.maxpool(input_tensor))\n",
    "        inc_out = torch.cat((out, mp), 1)\n",
    "        return self.act(self.bn(inc_out))\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self,c_in,bottleneck=32,ks=40,nb_filters=32,residual=True,depth=6):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual = residual\n",
    "        self.depth = depth\n",
    "\n",
    "        #inception & residual layers\n",
    "        inc_mods = []\n",
    "        res_layers = []\n",
    "        res = 0\n",
    "        for d in range(depth):\n",
    "            inc_mods.append(\n",
    "                Inception(c_in if d == 0 else nb_filters * 4, bottleneck=bottleneck if d > 0 else 0,ks=ks,\n",
    "                          nb_filters=nb_filters))\n",
    "            if self.residual and d % 3 == 2:\n",
    "                res_layers.append(shortcut(c_in if res == 0 else nb_filters * 4, nb_filters * 4))\n",
    "                res += 1\n",
    "            else: res_layer = res_layers.append(None)\n",
    "        self.inc_mods = nn.ModuleList(inc_mods)\n",
    "        self.res_layers = nn.ModuleList(res_layers)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for d, l in enumerate(range(self.depth)):\n",
    "            x = self.inc_mods[d](x)\n",
    "            if self.residual and d % 3 == 2:\n",
    "                res = self.res_layers[d](res)\n",
    "                x += res\n",
    "                res = x\n",
    "                x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "class InceptionTime(nn.Module):\n",
    "    def __init__(self,c_in,c_out,bottleneck=32,ks=40,nb_filters=32,residual=True,depth=6):\n",
    "        super().__init__()\n",
    "        self.block = InceptionBlock(c_in,bottleneck=bottleneck,ks=ks,nb_filters=nb_filters,\n",
    "                                    residual=residual,depth=depth)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(nb_filters * 4, c_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Sigmoid(nn.Module):\n",
    "    '''\n",
    "    sigmoid layer\n",
    "    '''\n",
    "    def __init__(self, low, high):\n",
    "        super().__init__()\n",
    "        self.high, self.low = high, low\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x)*(self.high-self.low)+self.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeSgmOld(nn.Module):\n",
    "    '''\n",
    "    add a sigmoid layer to InceptionTime to get the ouput in a certain range\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        nn.Sequential()\n",
    "        self.inc = InceptionTime(n_in, n_out)\n",
    "        self.low, self.high = -1., 1.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.inc(x)) * (self.high - self.low) + self.low\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeSgm(nn.Module):\n",
    "    '''\n",
    "    add a sigmoid layer to InceptionTime to get the ouput in a certain range\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out, range=(-1,1)):\n",
    "        super().__init__()\n",
    "        self.mod = nn.Sequential(InceptionTime(n_in, n_out), Sigmoid(*range))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        return self.mod(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeD(nn.Module):\n",
    "    '''\n",
    "    add a sigmoid layer to InceptionTime to get the ouput in a certain range\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.mod = nn.Sequential(InceptionTime(n_in, n_out), Sigmoid(-1., 1.))\n",
    "        \n",
    "    def forward(self, xc, xd):\n",
    "        x = torch.cat([xc.float(), xd.float()], dim=-2)\n",
    "        x = x.float()\n",
    "#         print(f'InceptionTimeSgm dtype {x.dtype}')\n",
    "        return self.mod(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class InceptionTimeVar(nn.Module):\n",
    "    '''\n",
    "    output mean and variance\n",
    "    regression model, sigmoid for the mean output optional\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_in, n_out, meanrange=None):\n",
    "        super().__init__()\n",
    "        models  = [InceptionTime(n_in, n_out+1)]\n",
    "        if meanrange:\n",
    "            self.sigmoid = Sigmoid(*meanrange)\n",
    "        self.mod = nn.Sequential(*models)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        output = self.mod(x)\n",
    "        ## enforce positivity of sigma^2\n",
    "        ##output_sig_pos = tf.log(1 + tf.exp(output_sig)) + 1e-06\n",
    "        output[:,-1] = (output[:,-1].exp()+1).log_() + 1e-06\n",
    "        if getattr(self, 'sigmoid', None): output[:,:-1] = self.sigmoid(output[:,:-1])\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2036, 0.9338, 0.8957, 0.8432, 0.8481, 1.2246, 0.9543, 1.1465, 0.7756,\n",
       "         0.9615]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.rand((1,10)).exp()+1).log() + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = InceptionTimeVar(10,1, meanrange=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionTimeVar(\n",
       "  (sigmoid): Sigmoid()\n",
       "  (mod): Sequential(\n",
       "    (0): InceptionTime(\n",
       "      (block): InceptionBlock(\n",
       "        (inc_mods): ModuleList(\n",
       "          (0): Inception(\n",
       "            (conv_layers): ModuleList(\n",
       "              (0): Conv1d(10, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "              (1): Conv1d(10, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "              (2): Conv1d(10, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "            )\n",
       "            (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (conv): Conv1d(10, 32, kernel_size=(1,), stride=(1,))\n",
       "            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (1): Inception(\n",
       "            (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "            (conv_layers): ModuleList(\n",
       "              (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "              (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "              (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "            )\n",
       "            (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (2): Inception(\n",
       "            (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "            (conv_layers): ModuleList(\n",
       "              (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "              (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "              (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "            )\n",
       "            (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (3): Inception(\n",
       "            (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "            (conv_layers): ModuleList(\n",
       "              (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "              (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "              (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "            )\n",
       "            (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (4): Inception(\n",
       "            (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "            (conv_layers): ModuleList(\n",
       "              (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "              (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "              (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "            )\n",
       "            (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (5): Inception(\n",
       "            (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "            (conv_layers): ModuleList(\n",
       "              (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))\n",
       "              (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))\n",
       "              (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "            )\n",
       "            (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "            (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "            (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (res_layers): ModuleList(\n",
       "          (0): None\n",
       "          (1): None\n",
       "          (2): Sequential(\n",
       "            (0): Conv1d(10, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (3): None\n",
       "          (4): None\n",
       "          (5): Sequential(\n",
       "            (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (gap): AdaptiveAvgPool1d(output_size=1)\n",
       "      (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4105, 0.2505],\n",
       "        [0.4318, 0.2657],\n",
       "        [0.3777, 0.2642],\n",
       "        [0.3953, 0.2852],\n",
       "        [0.4394, 0.2912],\n",
       "        [0.3648, 0.2685],\n",
       "        [0.3878, 0.2818],\n",
       "        [0.4113, 0.2373],\n",
       "        [0.4401, 0.2637],\n",
       "        [0.4423, 0.2611],\n",
       "        [0.3725, 0.2485],\n",
       "        [0.4087, 0.2721],\n",
       "        [0.3843, 0.2865],\n",
       "        [0.4171, 0.2702],\n",
       "        [0.4280, 0.2767],\n",
       "        [0.4198, 0.2661],\n",
       "        [0.4392, 0.2845],\n",
       "        [0.4135, 0.2801],\n",
       "        [0.4134, 0.2512],\n",
       "        [0.4217, 0.2726],\n",
       "        [0.3907, 0.2770],\n",
       "        [0.4313, 0.2884],\n",
       "        [0.4159, 0.2700],\n",
       "        [0.4348, 0.2645],\n",
       "        [0.4055, 0.2762],\n",
       "        [0.4049, 0.2636],\n",
       "        [0.3728, 0.2552],\n",
       "        [0.4231, 0.2745],\n",
       "        [0.4095, 0.2549],\n",
       "        [0.4459, 0.2475],\n",
       "        [0.4107, 0.2761],\n",
       "        [0.4225, 0.2802],\n",
       "        [0.4316, 0.2669],\n",
       "        [0.4285, 0.2749],\n",
       "        [0.3991, 0.2729],\n",
       "        [0.4219, 0.2815],\n",
       "        [0.4021, 0.2642],\n",
       "        [0.4031, 0.2788],\n",
       "        [0.3768, 0.2732],\n",
       "        [0.4061, 0.2451],\n",
       "        [0.4047, 0.2533],\n",
       "        [0.4143, 0.2844],\n",
       "        [0.4319, 0.2667],\n",
       "        [0.3971, 0.2616],\n",
       "        [0.4170, 0.2652],\n",
       "        [0.4403, 0.2533],\n",
       "        [0.4039, 0.2449],\n",
       "        [0.4327, 0.2803],\n",
       "        [0.4131, 0.2865],\n",
       "        [0.3971, 0.2582],\n",
       "        [0.4222, 0.2910],\n",
       "        [0.4458, 0.2657],\n",
       "        [0.3839, 0.2773],\n",
       "        [0.4063, 0.2718],\n",
       "        [0.3839, 0.2727],\n",
       "        [0.4160, 0.2596],\n",
       "        [0.3999, 0.2818],\n",
       "        [0.3846, 0.2580],\n",
       "        [0.4385, 0.2751],\n",
       "        [0.4017, 0.2680],\n",
       "        [0.4248, 0.2906],\n",
       "        [0.4458, 0.2566],\n",
       "        [0.3855, 0.2710],\n",
       "        [0.4512, 0.2869],\n",
       "        [0.3974, 0.2452],\n",
       "        [0.3887, 0.2538],\n",
       "        [0.4096, 0.2710],\n",
       "        [0.3843, 0.2559],\n",
       "        [0.3823, 0.2688],\n",
       "        [0.4318, 0.2685],\n",
       "        [0.3986, 0.2635],\n",
       "        [0.4045, 0.2748],\n",
       "        [0.3980, 0.2712],\n",
       "        [0.3808, 0.2877],\n",
       "        [0.3937, 0.2685],\n",
       "        [0.3882, 0.2567],\n",
       "        [0.4016, 0.2823],\n",
       "        [0.3918, 0.2766],\n",
       "        [0.4066, 0.2745],\n",
       "        [0.3997, 0.2819],\n",
       "        [0.3773, 0.2689],\n",
       "        [0.4426, 0.2665],\n",
       "        [0.3668, 0.2867],\n",
       "        [0.4090, 0.2569],\n",
       "        [0.4561, 0.2771],\n",
       "        [0.3826, 0.2757],\n",
       "        [0.4184, 0.2592],\n",
       "        [0.3906, 0.2636],\n",
       "        [0.4033, 0.2607],\n",
       "        [0.4055, 0.2539],\n",
       "        [0.4253, 0.2693],\n",
       "        [0.3957, 0.2670],\n",
       "        [0.4502, 0.2623],\n",
       "        [0.4081, 0.2808],\n",
       "        [0.4138, 0.2712],\n",
       "        [0.4019, 0.2847],\n",
       "        [0.4210, 0.2568],\n",
       "        [0.4244, 0.2420],\n",
       "        [0.4248, 0.2734],\n",
       "        [0.4164, 0.2758],\n",
       "        [0.4322, 0.2710],\n",
       "        [0.4202, 0.2746],\n",
       "        [0.4338, 0.2836],\n",
       "        [0.4061, 0.2632],\n",
       "        [0.4224, 0.2806],\n",
       "        [0.4003, 0.2803],\n",
       "        [0.3970, 0.2737],\n",
       "        [0.4295, 0.2669],\n",
       "        [0.4039, 0.2863],\n",
       "        [0.4314, 0.2705],\n",
       "        [0.4371, 0.2709],\n",
       "        [0.4225, 0.2607],\n",
       "        [0.4511, 0.2751],\n",
       "        [0.4270, 0.2651],\n",
       "        [0.4392, 0.2685],\n",
       "        [0.4504, 0.2589],\n",
       "        [0.3749, 0.2631],\n",
       "        [0.3802, 0.2653],\n",
       "        [0.4068, 0.2664],\n",
       "        [0.4122, 0.2666],\n",
       "        [0.4098, 0.2654],\n",
       "        [0.4506, 0.2847],\n",
       "        [0.3963, 0.2739],\n",
       "        [0.4189, 0.2822],\n",
       "        [0.3934, 0.2606],\n",
       "        [0.4144, 0.2600],\n",
       "        [0.4208, 0.2859],\n",
       "        [0.3660, 0.2723]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_var(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model sanity checks\n",
    "xb = torch.randn((128,10,100))\n",
    "yb = torch.rand(128,1)\n",
    "model = InceptionTimeSgm(10,1)\n",
    "model = InceptionTime(10,1)\n",
    "\n",
    "y_m = model(xb)\n",
    "assert y_m.shape == (128,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_regression(preds, y_true, c=5):\n",
    "    '''\n",
    "    negative log likelihood loss for regression, both mu and sigma are predicted\n",
    "    \n",
    "    Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles\n",
    "    Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell, DeepMind\n",
    "\n",
    "    '''\n",
    "    \n",
    "    s1 = 0.5*preds[:,1].log() \n",
    "    s2 = 0.5*(yb.squeeze()-preds[:,0]).pow(2).div(preds[:,1])\n",
    "    loss = (s1+s2).mean() + c\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nll_regression(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.0136, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = InceptionTimeVar(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3641, grad_fn=<AddBackward0>)\n",
      "tensor(4.3060, grad_fn=<AddBackward0>)\n",
      "tensor(4.2522, grad_fn=<AddBackward0>)\n",
      "tensor(4.2023, grad_fn=<AddBackward0>)\n",
      "tensor(4.1553, grad_fn=<AddBackward0>)\n",
      "tensor(4.1099, grad_fn=<AddBackward0>)\n",
      "tensor(4.0648, grad_fn=<AddBackward0>)\n",
      "tensor(4.0278, grad_fn=<AddBackward0>)\n",
      "tensor(4.5346, grad_fn=<AddBackward0>)\n",
      "tensor(33.2470, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "epochs = 10\n",
    "loss_fn = nll_regression\n",
    "m = model_var\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = nll_regression(preds, yb)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for p in m.parameters():\n",
    "            p.sub_(lr*p.grad)\n",
    "    m.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = InceptionTimeVar(10,1)\n",
    "opt = Adam(model_var.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0208, grad_fn=<AddBackward0>)\n",
      "tensor(4.8659, grad_fn=<AddBackward0>)\n",
      "tensor(4.7756, grad_fn=<AddBackward0>)\n",
      "tensor(4.7244, grad_fn=<AddBackward0>)\n",
      "tensor(4.6740, grad_fn=<AddBackward0>)\n",
      "tensor(4.6212, grad_fn=<AddBackward0>)\n",
      "tensor(4.5612, grad_fn=<AddBackward0>)\n",
      "tensor(4.4978, grad_fn=<AddBackward0>)\n",
      "tensor(4.4351, grad_fn=<AddBackward0>)\n",
      "tensor(4.3768, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = model_var\n",
    "for i in range(epochs):\n",
    "    preds = m(xb)\n",
    "    loss = nll_regression(preds, yb)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6883,  0.3206],\n",
       "        [ 0.1993,  0.2503],\n",
       "        [ 0.7823,  0.3220],\n",
       "        [ 0.8343,  0.2718],\n",
       "        [ 0.7920,  0.2804],\n",
       "        [ 0.4914,  0.2229],\n",
       "        [ 0.4477,  0.2060],\n",
       "        [ 0.7541,  0.2245],\n",
       "        [ 0.2930,  0.2679],\n",
       "        [ 0.3321,  0.1942],\n",
       "        [ 0.7895,  0.2710],\n",
       "        [ 0.7996,  0.2916],\n",
       "        [ 0.6036,  0.2444],\n",
       "        [ 0.2867,  0.1834],\n",
       "        [ 0.8796,  0.3102],\n",
       "        [ 0.0324,  0.1723],\n",
       "        [ 0.0474,  0.1506],\n",
       "        [ 0.3758,  0.2717],\n",
       "        [ 0.0773,  0.1942],\n",
       "        [ 0.2873,  0.2005],\n",
       "        [ 0.8112,  0.2479],\n",
       "        [ 0.3492,  0.2456],\n",
       "        [ 0.1308,  0.2137],\n",
       "        [-0.0081,  0.1893],\n",
       "        [ 0.3030,  0.2092],\n",
       "        [ 0.8836,  0.3118],\n",
       "        [ 0.2829,  0.1725],\n",
       "        [ 0.8589,  0.3252],\n",
       "        [ 0.7722,  0.2483],\n",
       "        [ 0.3383,  0.2190],\n",
       "        [ 0.8427,  0.3556],\n",
       "        [ 0.2270,  0.2487],\n",
       "        [ 0.5968,  0.3048],\n",
       "        [ 0.6996,  0.2803],\n",
       "        [ 0.2520,  0.2477],\n",
       "        [ 0.3296,  0.2098],\n",
       "        [ 0.9147,  0.2937],\n",
       "        [ 0.6379,  0.3024],\n",
       "        [ 0.4818,  0.2793],\n",
       "        [ 0.4964,  0.2532],\n",
       "        [ 0.4840,  0.2342],\n",
       "        [ 0.2983,  0.2084],\n",
       "        [ 0.7834,  0.3209],\n",
       "        [ 0.9680,  0.2465],\n",
       "        [ 0.6692,  0.2513],\n",
       "        [ 0.4076,  0.2524],\n",
       "        [ 0.7360,  0.3258],\n",
       "        [ 0.1536,  0.2423],\n",
       "        [ 0.6793,  0.2452],\n",
       "        [ 0.4453,  0.2378],\n",
       "        [ 0.6386,  0.2931],\n",
       "        [ 0.1529,  0.2680],\n",
       "        [ 0.1404,  0.2261],\n",
       "        [ 0.1874,  0.2233],\n",
       "        [ 0.1212,  0.2165],\n",
       "        [ 0.0498,  0.2311],\n",
       "        [ 0.8900,  0.2706],\n",
       "        [ 0.8873,  0.2875],\n",
       "        [ 0.4650,  0.1637],\n",
       "        [ 0.5433,  0.2383],\n",
       "        [ 0.7037,  0.2627],\n",
       "        [ 0.3802,  0.2255],\n",
       "        [ 0.5300,  0.2633],\n",
       "        [ 0.7687,  0.3298],\n",
       "        [ 0.2303,  0.2384],\n",
       "        [ 0.8948,  0.3204],\n",
       "        [ 0.5234,  0.2203],\n",
       "        [ 0.6063,  0.2479],\n",
       "        [ 0.6722,  0.2587],\n",
       "        [ 0.7918,  0.3160],\n",
       "        [ 0.8448,  0.2739],\n",
       "        [ 0.0120,  0.2336],\n",
       "        [ 0.5401,  0.2651],\n",
       "        [ 0.4685,  0.2408],\n",
       "        [ 0.2908,  0.1618],\n",
       "        [ 0.9031,  0.2997],\n",
       "        [ 0.7385,  0.2894],\n",
       "        [ 0.7426,  0.2939],\n",
       "        [ 0.5538,  0.3340],\n",
       "        [ 0.4346,  0.2984],\n",
       "        [ 0.8048,  0.3476],\n",
       "        [ 0.2499,  0.2116],\n",
       "        [ 0.9373,  0.2722],\n",
       "        [ 0.8508,  0.3062],\n",
       "        [ 0.8829,  0.2825],\n",
       "        [ 0.1461,  0.2659],\n",
       "        [ 0.1799,  0.1742],\n",
       "        [ 0.0126,  0.2281],\n",
       "        [ 0.3533,  0.2082],\n",
       "        [ 0.6495,  0.2508],\n",
       "        [ 0.3350,  0.2662],\n",
       "        [ 0.3684,  0.2066],\n",
       "        [ 0.3814,  0.2588],\n",
       "        [ 0.7020,  0.2469],\n",
       "        [ 0.8554,  0.2410],\n",
       "        [ 0.8887,  0.2889],\n",
       "        [ 0.1985,  0.1724],\n",
       "        [ 0.4810,  0.3193],\n",
       "        [ 0.0532,  0.1942],\n",
       "        [ 0.7217,  0.3273],\n",
       "        [ 0.8507,  0.3141],\n",
       "        [ 0.2843,  0.2872],\n",
       "        [ 0.0973,  0.1973],\n",
       "        [ 0.2166,  0.2294],\n",
       "        [ 0.8037,  0.3152],\n",
       "        [ 0.5923,  0.2683],\n",
       "        [ 0.6585,  0.2602],\n",
       "        [ 0.6880,  0.2996],\n",
       "        [ 0.5206,  0.2927],\n",
       "        [ 0.2850,  0.2513],\n",
       "        [ 0.7472,  0.2777],\n",
       "        [ 0.3990,  0.2409],\n",
       "        [ 0.8065,  0.2373],\n",
       "        [ 0.1772,  0.2349],\n",
       "        [ 0.6181,  0.2950],\n",
       "        [ 0.0244,  0.2404],\n",
       "        [ 0.8377,  0.2948],\n",
       "        [ 0.3724,  0.2316],\n",
       "        [ 0.1405,  0.1848],\n",
       "        [ 0.8034,  0.3291],\n",
       "        [ 0.7125,  0.3461],\n",
       "        [ 0.5185,  0.2926],\n",
       "        [ 0.6163,  0.2476],\n",
       "        [ 0.2254,  0.2622],\n",
       "        [ 0.7263,  0.2412],\n",
       "        [ 0.2242,  0.2128],\n",
       "        [ 0.5838,  0.2907],\n",
       "        [ 0.7683,  0.3177]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-13.67,  14.95],\n",
       "       [-13.4 ,  14.7 ],\n",
       "       [-13.59,  14.68],\n",
       "       [-13.08,  14.41],\n",
       "       [-13.2 ,  14.35],\n",
       "       [-12.29,  13.37],\n",
       "       [-12.93,  14.05],\n",
       "       [-13.16,  14.2 ],\n",
       "       [-12.83,  13.93],\n",
       "       [-12.51,  13.71],\n",
       "       [-12.9 ,  14.04],\n",
       "       [-12.89,  13.99],\n",
       "       [-12.57,  13.74],\n",
       "       [-13.13,  14.35],\n",
       "       [-13.13,  14.29],\n",
       "       [-12.74,  13.99],\n",
       "       [-13.03,  14.2 ],\n",
       "       [-13.1 ,  14.23],\n",
       "       [-13.45,  14.53],\n",
       "       [-12.36,  13.47],\n",
       "       [-13.03,  14.09],\n",
       "       [-13.41,  14.55],\n",
       "       [-13.65,  14.73],\n",
       "       [-13.02,  14.3 ],\n",
       "       [-13.26,  14.28],\n",
       "       [-14.06,  15.14],\n",
       "       [-12.45,  13.59],\n",
       "       [-13.2 ,  14.22],\n",
       "       [-13.46,  14.66],\n",
       "       [-12.46,  13.56],\n",
       "       [-13.14,  14.35],\n",
       "       [-13.24,  14.36],\n",
       "       [-12.95,  14.16],\n",
       "       [-12.57,  13.64],\n",
       "       [-13.48,  14.6 ],\n",
       "       [-12.49,  13.52],\n",
       "       [-13.26,  14.29],\n",
       "       [-13.59,  14.66],\n",
       "       [-13.16,  14.27],\n",
       "       [-12.68,  13.94],\n",
       "       [-13.06,  14.35],\n",
       "       [-13.7 ,  14.87],\n",
       "       [-13.6 ,  14.9 ],\n",
       "       [-13.08,  14.29],\n",
       "       [-13.24,  14.25],\n",
       "       [-13.19,  14.44],\n",
       "       [-12.8 ,  13.89],\n",
       "       [-12.81,  13.99],\n",
       "       [-12.63,  13.7 ],\n",
       "       [-12.57,  13.6 ],\n",
       "       [-13.33,  14.67],\n",
       "       [-13.38,  14.55],\n",
       "       [-12.98,  14.11],\n",
       "       [-13.  ,  14.17],\n",
       "       [-13.52,  14.71],\n",
       "       [-12.95,  14.18],\n",
       "       [-13.14,  14.42],\n",
       "       [-13.01,  14.21],\n",
       "       [-13.81,  14.93],\n",
       "       [-13.41,  14.59],\n",
       "       [-12.96,  14.2 ],\n",
       "       [-12.72,  13.85],\n",
       "       [-12.79,  14.02],\n",
       "       [-13.33,  14.51],\n",
       "       [-13.09,  14.33],\n",
       "       [-12.95,  14.21],\n",
       "       [-13.18,  14.31],\n",
       "       [-13.39,  14.48],\n",
       "       [-12.34,  13.6 ],\n",
       "       [-13.08,  14.12],\n",
       "       [-12.6 ,  13.7 ],\n",
       "       [-13.23,  14.35],\n",
       "       [-12.92,  14.  ],\n",
       "       [-12.89,  14.2 ],\n",
       "       [-11.89,  12.97],\n",
       "       [-13.06,  14.26],\n",
       "       [-13.47,  14.56],\n",
       "       [-12.58,  13.6 ],\n",
       "       [-13.33,  14.45],\n",
       "       [-12.61,  13.72],\n",
       "       [-13.06,  14.33],\n",
       "       [-12.44,  13.65],\n",
       "       [-12.55,  13.69],\n",
       "       [-12.75,  13.91],\n",
       "       [-12.37,  13.38],\n",
       "       [-13.42,  14.44],\n",
       "       [-12.91,  14.13],\n",
       "       [-12.91,  14.03],\n",
       "       [-12.71,  13.81],\n",
       "       [-13.99,  15.17],\n",
       "       [-13.37,  14.29],\n",
       "       [-13.06,  14.42],\n",
       "       [-13.22,  14.22],\n",
       "       [-13.16,  14.25],\n",
       "       [-13.59,  14.59],\n",
       "       [-12.66,  13.64],\n",
       "       [-13.12,  14.13],\n",
       "       [-13.43,  14.45],\n",
       "       [-13.46,  14.51],\n",
       "       [-12.85,  13.97],\n",
       "       [-13.05,  14.23],\n",
       "       [-12.48,  13.41],\n",
       "       [-13.21,  14.21],\n",
       "       [-13.19,  14.46],\n",
       "       [-12.67,  13.93],\n",
       "       [-13.15,  14.24],\n",
       "       [-12.27,  13.4 ],\n",
       "       [-12.69,  13.68],\n",
       "       [-13.01,  14.25],\n",
       "       [-13.36,  14.53],\n",
       "       [-12.62,  13.77],\n",
       "       [-13.03,  13.97],\n",
       "       [-13.5 ,  14.52],\n",
       "       [-13.66,  14.82],\n",
       "       [-13.2 ,  14.22],\n",
       "       [-13.15,  14.35],\n",
       "       [-13.08,  14.17],\n",
       "       [-13.49,  14.52],\n",
       "       [-13.05,  14.15],\n",
       "       [-13.5 ,  14.61],\n",
       "       [-13.25,  14.37],\n",
       "       [-13.16,  14.29],\n",
       "       [-13.15,  14.3 ],\n",
       "       [-12.84,  13.88],\n",
       "       [-12.5 ,  13.72],\n",
       "       [-12.75,  13.85],\n",
       "       [-13.21,  14.2 ],\n",
       "       [-12.79,  13.83]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(m(xb).detach().numpy(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7950],\n",
       "        [0.7399],\n",
       "        [0.0699],\n",
       "        [0.9656],\n",
       "        [0.4764],\n",
       "        [0.4740],\n",
       "        [0.6303],\n",
       "        [0.2540],\n",
       "        [0.4151],\n",
       "        [0.6722],\n",
       "        [0.4814],\n",
       "        [0.4797],\n",
       "        [0.9125],\n",
       "        [0.5585],\n",
       "        [0.6816],\n",
       "        [0.9544],\n",
       "        [0.7428],\n",
       "        [0.2958],\n",
       "        [0.5067],\n",
       "        [0.2693],\n",
       "        [0.3197],\n",
       "        [0.5284],\n",
       "        [0.0573],\n",
       "        [0.6996],\n",
       "        [0.2437],\n",
       "        [0.3375],\n",
       "        [0.4211],\n",
       "        [0.0990],\n",
       "        [0.5340],\n",
       "        [0.9474],\n",
       "        [0.4812],\n",
       "        [0.2803],\n",
       "        [0.6376],\n",
       "        [0.6415],\n",
       "        [0.3024],\n",
       "        [0.0556],\n",
       "        [0.2722],\n",
       "        [0.2621],\n",
       "        [0.4258],\n",
       "        [0.6229],\n",
       "        [0.8274],\n",
       "        [0.5367],\n",
       "        [0.7236],\n",
       "        [0.9387],\n",
       "        [0.0994],\n",
       "        [0.5417],\n",
       "        [0.0348],\n",
       "        [0.9079],\n",
       "        [0.4295],\n",
       "        [0.1127],\n",
       "        [0.9783],\n",
       "        [0.7960],\n",
       "        [0.4782],\n",
       "        [0.7561],\n",
       "        [0.4944],\n",
       "        [0.6424],\n",
       "        [0.5027],\n",
       "        [0.8129],\n",
       "        [0.3108],\n",
       "        [0.0314],\n",
       "        [0.8256],\n",
       "        [0.4758],\n",
       "        [0.9275],\n",
       "        [0.6571],\n",
       "        [0.8468],\n",
       "        [0.9441],\n",
       "        [0.5645],\n",
       "        [0.3249],\n",
       "        [0.8562],\n",
       "        [0.3836],\n",
       "        [0.3362],\n",
       "        [0.2119],\n",
       "        [0.4902],\n",
       "        [0.9710],\n",
       "        [0.1358],\n",
       "        [0.8682],\n",
       "        [0.6258],\n",
       "        [0.4839],\n",
       "        [0.1821],\n",
       "        [0.9512],\n",
       "        [0.9183],\n",
       "        [0.8027],\n",
       "        [0.8479],\n",
       "        [0.3166],\n",
       "        [0.1567],\n",
       "        [0.5582],\n",
       "        [0.5204],\n",
       "        [0.3818],\n",
       "        [0.2762],\n",
       "        [0.2639],\n",
       "        [0.0034],\n",
       "        [0.9097],\n",
       "        [0.0748],\n",
       "        [0.0341],\n",
       "        [0.0623],\n",
       "        [0.6634],\n",
       "        [0.2931],\n",
       "        [0.4275],\n",
       "        [0.1595],\n",
       "        [0.4284],\n",
       "        [0.2275],\n",
       "        [0.2003],\n",
       "        [0.2777],\n",
       "        [0.8996],\n",
       "        [0.9790],\n",
       "        [0.3393],\n",
       "        [0.5192],\n",
       "        [0.2296],\n",
       "        [0.9707],\n",
       "        [0.7867],\n",
       "        [0.5301],\n",
       "        [0.1533],\n",
       "        [0.0033],\n",
       "        [0.4565],\n",
       "        [0.0814],\n",
       "        [0.9102],\n",
       "        [0.3537],\n",
       "        [0.5766],\n",
       "        [0.5154],\n",
       "        [0.3513],\n",
       "        [0.2213],\n",
       "        [0.5360],\n",
       "        [0.2296],\n",
       "        [0.5150],\n",
       "        [0.2385],\n",
       "        [0.8536],\n",
       "        [0.0919],\n",
       "        [0.5104]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.zeros(128,1)\n",
    "x2 = torch.ones(128,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_p = torch.cat([x1,x2], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4584)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_regression(x_p, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4870],\n",
       "        [ 1.2089],\n",
       "        [ 2.9409],\n",
       "        [-0.8186],\n",
       "        [ 0.2653],\n",
       "        [-0.7720],\n",
       "        [ 0.0791],\n",
       "        [ 0.3814],\n",
       "        [ 0.9626],\n",
       "        [ 0.3247],\n",
       "        [-1.1993],\n",
       "        [ 0.3751],\n",
       "        [-0.6192],\n",
       "        [-0.3383],\n",
       "        [ 0.5900],\n",
       "        [ 0.0607],\n",
       "        [-0.4568],\n",
       "        [ 0.3758],\n",
       "        [-0.7539],\n",
       "        [ 0.4403],\n",
       "        [ 0.6936],\n",
       "        [ 1.2951],\n",
       "        [-0.2941],\n",
       "        [-0.7952],\n",
       "        [-0.3005],\n",
       "        [-1.0045],\n",
       "        [-2.1125],\n",
       "        [-2.4088],\n",
       "        [ 0.8342],\n",
       "        [-1.8054],\n",
       "        [-1.3376],\n",
       "        [ 0.6540],\n",
       "        [-0.1123],\n",
       "        [ 1.0436],\n",
       "        [-0.2022],\n",
       "        [-1.0255],\n",
       "        [-0.1646],\n",
       "        [ 0.5818],\n",
       "        [-0.5151],\n",
       "        [-0.1190],\n",
       "        [-1.3705],\n",
       "        [ 0.7422],\n",
       "        [ 1.8771],\n",
       "        [ 0.0707],\n",
       "        [-1.5921],\n",
       "        [ 0.7774],\n",
       "        [-1.4089],\n",
       "        [ 0.2089],\n",
       "        [-0.4930],\n",
       "        [ 0.4699],\n",
       "        [-0.0482],\n",
       "        [-0.2985],\n",
       "        [ 1.2841],\n",
       "        [-1.4571],\n",
       "        [-1.6483],\n",
       "        [-0.0075],\n",
       "        [ 0.1031],\n",
       "        [-0.9086],\n",
       "        [ 1.3114],\n",
       "        [ 0.2798],\n",
       "        [ 0.4942],\n",
       "        [ 0.1085],\n",
       "        [-0.1875],\n",
       "        [ 0.3414],\n",
       "        [ 0.1216],\n",
       "        [ 1.7783],\n",
       "        [ 1.6296],\n",
       "        [-0.3994],\n",
       "        [ 0.0753],\n",
       "        [-0.4307],\n",
       "        [-0.6740],\n",
       "        [ 0.2480],\n",
       "        [-0.2274],\n",
       "        [-0.0470],\n",
       "        [ 1.2562],\n",
       "        [ 0.3656],\n",
       "        [-2.1018],\n",
       "        [ 1.5465],\n",
       "        [-1.0159],\n",
       "        [-0.9425],\n",
       "        [-0.5119],\n",
       "        [-1.3735],\n",
       "        [ 0.0032],\n",
       "        [ 1.1827],\n",
       "        [ 0.2460],\n",
       "        [ 0.9233],\n",
       "        [ 0.0897],\n",
       "        [ 0.2159],\n",
       "        [ 0.3031],\n",
       "        [ 0.4637],\n",
       "        [ 0.5943],\n",
       "        [-1.0196],\n",
       "        [ 0.6387],\n",
       "        [-1.5419],\n",
       "        [-0.8972],\n",
       "        [-0.1979],\n",
       "        [-1.2235],\n",
       "        [ 0.1527],\n",
       "        [-1.0874],\n",
       "        [ 2.7019],\n",
       "        [ 0.9183],\n",
       "        [-1.3231],\n",
       "        [-0.7963],\n",
       "        [-0.0194],\n",
       "        [ 0.8233],\n",
       "        [-1.5799],\n",
       "        [-0.4217],\n",
       "        [ 1.1918],\n",
       "        [-1.0704],\n",
       "        [ 0.5154],\n",
       "        [-0.8226],\n",
       "        [-0.5882],\n",
       "        [-0.5135],\n",
       "        [-0.9451],\n",
       "        [-1.2081],\n",
       "        [-0.4360],\n",
       "        [ 0.2406],\n",
       "        [-0.0526],\n",
       "        [-1.0071],\n",
       "        [-0.0188],\n",
       "        [ 0.0216],\n",
       "        [ 1.6696],\n",
       "        [ 0.3746],\n",
       "        [-0.2899],\n",
       "        [-0.8267],\n",
       "        [-0.0328],\n",
       "        [-1.1045],\n",
       "        [-0.4355]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0012)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(10):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "with torch.no_grad():\n",
    "    for p in model_v.parameters():\n",
    "        p -= p.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_2 = model_v(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_2 = nll_regression(preds_2, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.6749, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1327, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.5*preds[:,1].log() + 0.5*(yb.squeeze()-preds[:,0]).pow(2).div(preds[:,1].pow(2))).mean() + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = list(model.parameters())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbdev]",
   "language": "python",
   "name": "conda-env-nbdev-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
