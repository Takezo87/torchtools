{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "> helper functions for loading timeseries datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UCR data: \n",
    "- from dataframe: main export `items_from_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torchtools.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "#import torch\n",
    "from fastai.torch_basics import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.all import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the UCR functions are ported from https://github.com/timeseriesAI/timeseriesAI to work with fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import tempfile\n",
    "try: from urllib import urlretrieve\n",
    "except ImportError: from urllib.request import urlretrieve\n",
    "import shutil\n",
    "from pyunpack import Archive\n",
    "from scipy.io import arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TSUtilities\n",
    "def To3dArray(arr):\n",
    "    arr = ToArray(arr)\n",
    "    if arr.ndim == 1: arr = arr[None, None]\n",
    "    elif arr.ndim == 2: arr = arr[:, None]\n",
    "    elif arr.ndim == 4: arr = arr[0]\n",
    "    assert arr.ndim == 3, 'Please, review input dimensions'\n",
    "    return np.array(arr)\n",
    "\n",
    "def ToArray(arr):\n",
    "    if isinstance(arr, torch.Tensor):\n",
    "        arr = np.array(arr)\n",
    "    elif not isinstance(arr, np.ndarray):\n",
    "        print(f\"Can't convert {type(arr)} to np.array\")\n",
    "    if arr.dtype == 'O': arr = np.array(arr, dtype=np.float32)\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def decompress_from_url(url, target_dir=None, verbose=False):\n",
    "    \"\"\"Downloads a compressed file from its URL and uncompresses it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        URL from which to download.\n",
    "    target_dir : str or None (default: None)\n",
    "        Directory to be used to extract downloaded files.\n",
    "    verbose : bool (default: False)\n",
    "        Whether to print information about the process (cached files used, ...)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or None\n",
    "        Directory in which the compressed file has been extracted if the process was\n",
    "        successful, None otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fname = os.path.basename(url)\n",
    "        tmpdir = tempfile.mkdtemp()\n",
    "        local_comp_fname = os.path.join(tmpdir, fname)\n",
    "        urlretrieve(url, local_comp_fname)\n",
    "    except:\n",
    "        shutil.rmtree(tmpdir)\n",
    "        if verbose:\n",
    "            sys.stderr.write(\"Could not download url. Please, check url.\\n\")\n",
    "    try:\n",
    "        if not os.path.exists(target_dir): os.makedirs(target_dir)\n",
    "        Archive(local_comp_fname).extractall(target_dir)\n",
    "        shutil.rmtree(tmpdir)\n",
    "        if verbose:\n",
    "            print(\"Successfully extracted file %s to path %s\" %\n",
    "                  (local_comp_fname, target_dir))\n",
    "        return target_dir\n",
    "    except:\n",
    "        shutil.rmtree(tmpdir)\n",
    "        if verbose:\n",
    "            sys.stderr.write(\"Could not uncompress file, aborting.\\n\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_UCR_univariate_list():\n",
    "    return sorted([\n",
    "        'ACSF1', 'Adiac', 'AllGestureWiimoteX', 'AllGestureWiimoteY',\n",
    "        'AllGestureWiimoteZ', 'ArrowHead', 'AsphaltObstacles', 'BME', 'Beef',\n",
    "        'BeetleFly', 'BirdChicken', 'CBF', 'Car', 'Chinatown',\n",
    "        'ChlorineConcentration', 'CinCECGTorso', 'Coffee', 'Computers',\n",
    "        'CricketX', 'CricketY', 'CricketZ', 'Crop', 'DiatomSizeReduction',\n",
    "        'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect',\n",
    "        'DistalPhalanxTW', 'DodgerLoopDay', 'DodgerLoopGame',\n",
    "        'DodgerLoopWeekend', 'ECG200', 'ECG5000', 'ECGFiveDays',\n",
    "        'EOGHorizontalSignal', 'EOGVerticalSignal', 'Earthquakes',\n",
    "        'ElectricDevices', 'EthanolLevel', 'FaceAll', 'FaceFour', 'FacesUCR',\n",
    "        'FiftyWords', 'Fish', 'FordA', 'FordB', 'FreezerRegularTrain',\n",
    "        'FreezerSmallTrain', 'Fungi', 'GestureMidAirD1', 'GestureMidAirD2',\n",
    "        'GestureMidAirD3', 'GesturePebbleZ1', 'GesturePebbleZ2', 'GunPoint',\n",
    "        'GunPointAgeSpan', 'GunPointMaleVersusFemale',\n",
    "        'GunPointOldVersusYoung', 'Ham', 'HandOutlines', 'Haptics', 'Herring',\n",
    "        'HouseTwenty', 'InlineSkate', 'InsectEPGRegularTrain',\n",
    "        'InsectEPGSmallTrain', 'InsectWingbeatSound', 'ItalyPowerDemand',\n",
    "        'LargeKitchenAppliances', 'Lightning2', 'Lightning7', 'Mallat', 'Meat',\n",
    "        'MedicalImages', 'MelbournePedestrian', 'MiddlePhalanxOutlineAgeGroup',\n",
    "        'MiddlePhalanxOutlineCorrect', 'MiddlePhalanxTW',\n",
    "        'MixedShapesRegularTrain', 'MixedShapesSmallTrain', 'MoteStrain',\n",
    "        'NonInvasiveFetalECGThorax1', 'NonInvasiveFetalECGThorax2', 'OSULeaf',\n",
    "        'OliveOil', 'PLAID', 'PhalangesOutlinesCorrect', 'Phoneme',\n",
    "        'PickupGestureWiimoteZ', 'PigAirwayPressure', 'PigArtPressure',\n",
    "        'PigCVP', 'Plane', 'PowerCons', 'ProximalPhalanxOutlineAgeGroup',\n",
    "        'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxTW',\n",
    "        'RefrigerationDevices', 'Rock', 'ScreenType', 'SemgHandGenderCh2',\n",
    "        'SemgHandMovementCh2', 'SemgHandSubjectCh2', 'ShakeGestureWiimoteZ',\n",
    "        'ShapeletSim', 'ShapesAll', 'SmallKitchenAppliances', 'SmoothSubspace',\n",
    "        'SonyAIBORobotSurface1', 'SonyAIBORobotSurface2', 'StarLightCurves',\n",
    "        'Strawberry', 'SwedishLeaf', 'Symbols', 'SyntheticControl',\n",
    "        'ToeSegmentation1', 'ToeSegmentation2', 'Trace', 'TwoLeadECG',\n",
    "        'TwoPatterns', 'UMD', 'UWaveGestureLibraryAll', 'UWaveGestureLibraryX',\n",
    "        'UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'Wafer', 'Wine',\n",
    "        'WordSynonyms', 'Worms', 'WormsTwoClass', 'Yoga'\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_UCR_multivariate_list():\n",
    "    return sorted([\n",
    "        'ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions',\n",
    "        'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'ERing',\n",
    "        'EigenWorms', 'Epilepsy', 'EthanolConcentration', 'FaceDetection',\n",
    "        'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat',\n",
    "        'InsectWingbeat', 'JapaneseVowels', 'LSST', 'Libras', 'MotorImagery',\n",
    "        'NATOPS', 'PEMS-SF', 'PenDigits', 'PhonemeSpectra', 'RacketSports',\n",
    "        'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits',\n",
    "        'StandWalkJump', 'UWaveGestureLibrary'\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_UCR_univariate(sel_dataset, parent_dir='data/UCR', verbose=False, drop_na=False, check=True):\n",
    "    if check and sel_dataset not in get_UCR_univariate_list():\n",
    "        print('This dataset does not exist. Please select one from this list:')\n",
    "        print(get_UCR_univariate_list())\n",
    "        return None, None, None, None\n",
    "    if verbose: print('Dataset:', sel_dataset)\n",
    "    src_website = 'http://www.timeseriesclassification.com/Downloads/'\n",
    "    tgt_dir = Path(parent_dir) / sel_dataset\n",
    "    if verbose: print('Downloading and decompressing data...')\n",
    "    if not os.path.isdir(tgt_dir):\n",
    "        decompress_from_url(\n",
    "            src_website + sel_dataset + '.zip', target_dir=tgt_dir, verbose=verbose)\n",
    "    if verbose: print('...data downloaded and decompressed')\n",
    "    fname_train = sel_dataset + \"_TRAIN.arff\"\n",
    "    fname_test = sel_dataset + \"_TEST.arff\"\n",
    "\n",
    "    train_df = pd.DataFrame(arff.loadarff(os.path.join(tgt_dir, fname_train))[0])\n",
    "    test_df = pd.DataFrame(arff.loadarff(os.path.join(tgt_dir, fname_test))[0])\n",
    "    unique_cats = train_df.iloc[:, -1].unique()\n",
    "    mapping = dict(zip(unique_cats, np.arange(len(unique_cats))))\n",
    "    train_df = train_df.replace({train_df.columns.values[-1]: mapping})\n",
    "    test_df = test_df.replace({test_df.columns.values[-1]: mapping})\n",
    "    if drop_na:\n",
    "        train_df.dropna(axis=1, inplace=True)\n",
    "        test_df.dropna(axis=1, inplace=True)\n",
    "\n",
    "    X_train = train_df.iloc[:, :-1].values.astype(np.float32)\n",
    "    X_test = test_df.iloc[:, :-1].values.astype(np.float32)\n",
    "    y_train = train_df.iloc[:, -1].values.astype(int)\n",
    "    y_test = test_df.iloc[:, -1].values.astype(int)\n",
    "\n",
    "    X_train = To3dArray(X_train)\n",
    "    X_test = To3dArray(X_test)\n",
    "\n",
    "    if verbose:\n",
    "        print('Successfully extracted dataset\\n')\n",
    "        print('X_train:', X_train.shape)\n",
    "        print('y_train:', y_train.shape)\n",
    "        print('X_valid:', X_test.shape)\n",
    "        print('y_valid:', y_test.shape, '\\n')\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def get_UCR_multivariate(sel_dataset, parent_dir='data/UCR', verbose=False, check=True):\n",
    "    if sel_dataset.lower() == 'mphoneme': sel_dataset = 'Phoneme'\n",
    "    if check and sel_dataset not in get_UCR_multivariate_list():\n",
    "        print('This dataset does not exist. Please select one from this list:')\n",
    "        print(get_UCR_multivariate_list())\n",
    "        return None, None, None, None\n",
    "    if verbose: print('Dataset:', sel_dataset)\n",
    "    src_website = 'http://www.timeseriesclassification.com/Downloads/'\n",
    "    tgt_dir = Path(parent_dir) / sel_dataset\n",
    "\n",
    "    if verbose: print('Downloading and decompressing data...')\n",
    "    if not os.path.isdir(tgt_dir):\n",
    "        decompress_from_url(\n",
    "            src_website + sel_dataset + '.zip', target_dir=tgt_dir, verbose=verbose)\n",
    "    if verbose: print('...data downloaded and decompressed')\n",
    "    if verbose: print('Extracting data...')\n",
    "    X_train_ = []\n",
    "    X_test_ = []\n",
    "    for i in range(10000):\n",
    "        if not os.path.isfile(\n",
    "                f'{parent_dir}/{sel_dataset}/{sel_dataset}Dimension'\n",
    "                + str(i + 1) + '_TRAIN.arff'):\n",
    "            break\n",
    "        train_df = pd.DataFrame(\n",
    "            arff.loadarff(\n",
    "                f'{parent_dir}/{sel_dataset}/{sel_dataset}Dimension'\n",
    "                + str(i + 1) + '_TRAIN.arff')[0])\n",
    "        unique_cats = train_df.iloc[:, -1].unique()\n",
    "        mapping = dict(zip(unique_cats, np.arange(len(unique_cats))))\n",
    "        train_df = train_df.replace({train_df.columns.values[-1]: mapping})\n",
    "        test_df = pd.DataFrame(\n",
    "            arff.loadarff(\n",
    "                f'{parent_dir}/{sel_dataset}/{sel_dataset}Dimension'\n",
    "                + str(i + 1) + '_TEST.arff')[0])\n",
    "        test_df = test_df.replace({test_df.columns.values[-1]: mapping})\n",
    "        X_train_.append(train_df.iloc[:, :-1].values)\n",
    "        X_test_.append(test_df.iloc[:, :-1].values)\n",
    "\n",
    "    if verbose: print('...extraction complete')\n",
    "    X_train = np.stack(X_train_, axis=-1)\n",
    "    X_test = np.stack(X_test_, axis=-1)\n",
    "\n",
    "    # In this case we need to rearrange the arrays ()\n",
    "    X_train = np.transpose(X_train, (0, 2, 1))\n",
    "    X_test = np.transpose(X_test, (0, 2, 1))\n",
    "\n",
    "    y_train = np.array([int(float(x)) for x in train_df.iloc[:, -1]])\n",
    "    y_test = np.array([int(float(x)) for x in test_df.iloc[:, -1]])\n",
    "\n",
    "    if verbose:\n",
    "        print('Successfully extracted dataset\\n')\n",
    "        print('X_train:', X_train.shape)\n",
    "        print('y_train:', y_train.shape)\n",
    "        print('X_valid:', X_test.shape)\n",
    "        print('y_valid:', y_test.shape, '\\n')\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def get_UCR_data(dsid, parent_dir='data/UCR', verbose=False, check=True):\n",
    "    if dsid in get_UCR_univariate_list():\n",
    "        return get_UCR_univariate(dsid, verbose=verbose, check=check)\n",
    "    elif dsid in get_UCR_multivariate_list():\n",
    "        return get_UCR_multivariate(dsid, verbose=verbose, check=check)\n",
    "    else:\n",
    "        print(f'This {dsid} dataset does not exist. Please select one from these lists:')\n",
    "        print('\\nunivariate datasets')\n",
    "        print(get_UCR_univariate_list())\n",
    "        print('\\nmultivariate datasets')\n",
    "        print(get_UCR_multivariate_list(), '\\n')\n",
    "        return None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Wine', 'WordSynonyms', 'Worms', 'WormsTwoClass', 'Yoga'],\n",
       " ['SelfRegulationSCP1',\n",
       "  'SelfRegulationSCP2',\n",
       "  'SpokenArabicDigits',\n",
       "  'StandWalkJump',\n",
       "  'UWaveGestureLibrary'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_uni = get_UCR_univariate_list()\n",
    "datasets_multi = get_UCR_multivariate_list()\n",
    "\n",
    "datasets_uni[-5:], datasets_multi[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = get_UCR_multivariate('UWaveGestureLibrary')\n",
    "dset[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ucr_to_items(dset):\n",
    "    '''\n",
    "    create items for DataBlock from a UCR dset\n",
    "    '''\n",
    "    x_train, y_train, x_test, y_test = dset\n",
    "    n_train = x_train.shape[0]\n",
    "    return list(zip(np.concatenate([x_train, x_test]), np.concatenate([y_train, y_test]))), n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 315), 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items, n_train = ucr_to_items(dset)\n",
    "\n",
    "items[0][0].shape, items[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCR workflow\n",
    "```\n",
    "dset = get_UCR_multivariate(univariate)(name)\n",
    "items, n_train = ucr_to_items(dset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 66)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main = pd.read_csv('./data/custom/bi_sample_anon.csv', nrows=100000)\n",
    "\n",
    "df_main.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a manual preprocessing first\n",
    "\n",
    "- fill missing\n",
    "- calculate normalization stats (also do normalization)\n",
    "- transform columns values to tensors\n",
    "\n",
    "Doing these transformations lazily is very slow, e.g. getting the columns values from the dataframe\n",
    "Can this be integrated into DataBlock? E.g. in the get_items method? get_items is not a Transform, so this is not reversible for e.g. data inspection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple config\n",
    "x_cols = [[f'x{i}_{j}' for j in range(10)] for i in range(6)]\n",
    "dep = 'y0'\n",
    "n_train = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_simple_config(discrete=False):\n",
    "    '''get a simple column configuration for development'''\n",
    "    if not discrete:\n",
    "        x_cols = [[f'x{i}_{j}' for j in range(10)] for i in range(6)]\n",
    "    else:\n",
    "        x_cols_cont = [[f'x{i}_{j}' for j in range(10)] for i in [0,1,3,4]] \n",
    "        x_cols_discrete = [[f'x{i}_{j}' for j in range(10)] for i in [2,5]]\n",
    "        x_cols = x_cols_cont, x_cols_discrete\n",
    "    dep = 'y0'\n",
    "    n_train = 8000\n",
    "    \n",
    "    return x_cols, dep, n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int\n",
    "x_cols_2, dep, n_train = get_simple_config(discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_x(df, x_cols, dtype=np.float32):\n",
    "    return np.stack([df[x_cols[i]].values for i in range(len(x_cols))], axis=1).astype(dtype)\n",
    "def _get_y(df, dep):\n",
    "    return df[dep].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not isinstance(x_cols, tuple):\n",
    "x,y = _get_x(df_main, x_cols), _get_y(df_main, dep)\n",
    "x_cont, x_dis, y = _get_x(df_main, x_cols_2[0]), _get_x(df_main, x_cols_2[1], dtype=int), _get_y(df_main, dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 6, 10), (100000, 4, 10), (100000, 2, 10), dtype('int64'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x_cont.shape, x_dis.shape, x_dis.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cont.astype(np.float32).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "## not for discrete columns\n",
    "def _calc_stats(x, n_train, axis=(0,2)):\n",
    "    return np.nanmean(x[:n_train], axis=axis), np.nanstd(x[:n_train], axis=axis),np.nanmedian(x[:n_train], axis=axis)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means,stds,medians =  _calc_stats(x, n_train)\n",
    "# means,stds,medians =  _calc_stats(x_cont, n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2868776 , -0.7260259 ,  0.01021978, -1.7377727 , -1.0197207 ,\n",
       "       -0.01062794], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _fillna(x, values):\n",
    "    if np.ndim(values)==0: np.nan_to_num(x, copy=False, nan=0)\n",
    "        \n",
    "    else:\n",
    "        #print(x.shape, values.shape)\n",
    "#         assert x.shape[0]==values.shape[0]\n",
    "        for i,v in enumerate(values): np.nan_to_num(x[:,i,...], copy=False, nan=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _normalize(x, means, stds):\n",
    "    assert x.shape[-2]==means.shape[0]\n",
    "    for i,v in enumerate(means): x[:,i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_fillna(x, means)\n",
    "assert not np.isnan(x).any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def items_from_df(df, cols_c, cols_y, n_train, cols_d=None, tab_cols_c=None , stats=None):\n",
    "    '''\n",
    "    creates timeseries items from a dataframe\n",
    "    \n",
    "    parameters:\n",
    "    df: input dataframe\n",
    "    cols_c: list of lists of columns for continuous valued channels (one list for each channel)\n",
    "    cols_d: (optional) list of lists of columns for discrete valued channels\n",
    "    cols_y: (list or single value) target column(s)\n",
    "    n_train: int, neeeds to be provided for calculating the stats that are necessary to fill missing values\n",
    "    tab_cols_c: (list or single value) tabular continunous columns\n",
    "    stats: tuple (means, stds, medians)\n",
    "    \n",
    "    return a list of (xc,(xd),y) tuples (one list element for each dataframe row)\n",
    "    '''\n",
    "    \n",
    "    cols_x = [cols_c]+[cols for cols in [cols_d, tab_cols_c]]\n",
    "    _types = [np.float32, np.int16, np.float32]\n",
    "#     print(cols_x)\n",
    "    xs=[]\n",
    "    for cols,t in zip(cols_x, _types):\n",
    "        if cols is not None:\n",
    "            x=_get_x(df, cols, dtype=t)\n",
    "            axis=(0,2) if is_listy(cols[0]) else (0)\n",
    "            if stats is None:\n",
    "                means, stds, medians =  _calc_stats(x, n_train, axis=axis)\n",
    "            else:\n",
    "                means, stds, medians = stats #medians will be None right now\n",
    "                if isinstance(means, torch.Tensor):\n",
    "                    means=means.to('cpu').numpy()\n",
    "                if isinstance(stds, torch.Tensor):\n",
    "                    stds=stds.to('cpu').numpy()\n",
    "                if isinstance(medians, torch.Tensor):\n",
    "                    medians=medians.to('cpu').numpy()\n",
    "                means, stds = means.squeeze(), stds.squeeze()\n",
    "            \n",
    "            #print(means.squeeze())\n",
    "            #print(_calc_stats(x, n_train, axis=axis)[0])\n",
    "            _fillna(x, means)\n",
    "            assert not np.isnan(x).any()\n",
    "            xs.append(x)\n",
    "\n",
    "    y =  _get_y(df, cols_y)\n",
    "#     return xs,y\n",
    "    return list(zip(*xs, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _apply_cats(voc, add, c):\n",
    "    if not is_categorical_dtype(c):\n",
    "        return pd.Categorical(c, categories=voc[c.name][add:]).codes+add\n",
    "    return c.cat.codes+add #if is_categorical_dtype(c) else c.map(voc[c.name].o2i)\n",
    "\n",
    "def cats_from_df(df, tab_cols_cat, n_train, add_na=True):\n",
    "    '''\n",
    "    extract category codes for categorical columns from df, create 'na'\n",
    "    \n",
    "    parameters:\n",
    "    df: input dataframe\n",
    "    tab_cols_cat: list of categorical column names\n",
    "    n_train: categories taken from df.iloc[:n_train] applied to df.iloc[n_train:]\n",
    "    '''\n",
    "    cat_maps = {c:CategoryMap(df[c].iloc[:n_train], add_na=add_na) for c in tab_cols_cat}  ## setup\n",
    "    return np.stack([partial(_apply_cats,cat_maps,1)(df[c]) for c in tab_cols_cat], axis=1), cat_maps\n",
    "#     return np.stack([pd.Cate])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtools.configs import get_discrete_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_c, cols_d, cols_y, n_train = get_discrete_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 4, 10)\n",
      "(100000, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "items = items_from_df(df_main, cols_c, cols_y, n_train, cols_d=cols_d)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  -7.8431373,  100.       ,   -6.497726 ,  -24.509804 ,\n",
       "          -33.22259  ,  -42.735043 ,  -55.24862  ,  -56.497173 ,\n",
       "          -30.674847 ,  100.       ],\n",
       "        [ 100.       ,    0.       ,    0.       ,    0.       ,\n",
       "            0.       ,    0.       ,    0.       ,    0.       ,\n",
       "            0.       ,    0.       ],\n",
       "        [ -17.301039 ,  100.       ,  -30.674847 ,  -54.945053 ,\n",
       "         -227.27272  ,  -27.855154 , -400.       , -147.05882  ,\n",
       "         -100.       ,  -94.33962  ],\n",
       "        [   0.       ,    0.       ,    0.       ,    0.       ,\n",
       "            0.       ,    0.       ,    0.       ,    0.       ,\n",
       "            0.       ,    0.       ]], dtype=float32),\n",
       " array([[ 0,  1, -1, -2, -1,  0,  0,  0, -2,  1],\n",
       "        [ 0,  1,  0, -1, -1, -1, -1,  0, -1, -2]], dtype=int16),\n",
       " -56.49717514124313)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# def df_to_items(df, x_cols, dep, n_train):\n",
    "#     x,y = _get_x(df, x_cols), _get_y(df, dep)    \n",
    "#     print(x.shape)\n",
    "#     means,stds,medians =  _calc_stats(x, n_train)\n",
    "#     _fillna(x, means)\n",
    "#     assert not np.isnan(x).any() \n",
    "#     return list(zip(x,y)), n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# def df_to_items_discrete(df, x_cols, dep, n_train):\n",
    "#     assert len(x_cols)==2, 'conts and discretes needed'\n",
    "#     x_cont, x_dis, y = _get_x(df, x_cols[0]), _get_x(df, x_cols[1]), _get_y(df, dep)    \n",
    "#     print(x_cont.shape, x_dis.shape)\n",
    "#     means,stds,medians =  _calc_stats(x_cont, n_train)\n",
    "#     _fillna(x_cont, means)\n",
    "#     _fillna(x_dis, 0)\n",
    "    \n",
    "#     assert not np.isnan(x_cont).any() \n",
    "#     assert not np.isnan(x_dis).any() \n",
    "#     #convert discrete variable to int only after filling nans, integer types cannot store nan\n",
    "#     return list(zip(x_cont, x_dis.astype(np.int16), y)), n_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe workflow:\n",
    "\n",
    "```\n",
    "items = items_from_df(df, x_cols, dep, n_train)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ttools]",
   "language": "python",
   "name": "conda-env-ttools-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
