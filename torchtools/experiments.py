# AUTOGENERATED! DO NOT EDIT! File to edit: 200_experiments.ipynb (unless otherwise specified).

__all__ = ['df_fn', 'df_dir', 'df_path', 'trn_end', 'val_end', 'test_end', 'splits', 'df_config', 'col_config',
           'df_source', 'dataset_name', 'data_params', 'get_dls', 'run_training', 'arch', 'n_epochs', 'max_lr', 'wd',
           'loss_fn_name', 'alpha', 'metrics', 'N', 'magnitude', 'seed', 'pct_start', 'div_factor', 'aug',
           'train_params', 'get_recorder_dict', 'TSExperiments', 'build_data_params']

# Cell
from .core import *
from .data import *
from .models import *
from .datasets import *
from .augmentations import *
from .datablock import *
from .dataloader import *

# Cell
import pandas as pd
import numpy as np
from fastai2.basics import *

# Cell
## data config
df_fn = 'bi_sample_anon.csv'
df_dir = Path('./data/custom')
df_path = Path(Path(df_dir)/df_fn)

trn_end = 120000
val_end = 160000
test_end = 200000
splits = (L(range(trn_end)), L(range(trn_end, val_end)))
df_config = f'{int(trn_end/1000)}_{int(val_end/1000)}_{int(test_end/1000)}'

col_config = '6chan_anon_discrete'
cols_c, cols_d, cols_y, n_train = get_discrete_config()

df_source = Path(df_fn).stem


dataset_name = f'{df_source}_{col_config}_{cols_y}_{df_config}'
data_params = defaultdict(lambda:None, {'df_fn':df_fn, 'df_dir':df_dir, 'df_path':df_path, 'trn_end':trn_end,
                                        'val_end':val_end, 'splits':splits, 'col_config_id':col_config,
                                        'cols_c':cols_c, 'cols_d':cols_d, 'cols_y':cols_y, 'ds_id':dataset_name})

# Cell
def get_dls(df, cols_c, cols_y, splits, cols_d=None, bs=64, ds_type=TSDatasets3, shuffle_train=True,
           verbose=False):
    '''
    create dataloaders
    '''
    if cols_d is not None:
        items, _ = df_to_items_discrete(df, [cols_c, cols_d], cols_y, n_train)
    else:
        items, _ = df_to_items(df, cols_c, cols_y, n_train)

    if cols_d: Xc,Xd,y = items_to_arrays(items)
    else: (Xc,y), Xd = items_to_arrays(items), None
    print(ds_type)
    dsets = ds_type(X=Xc, X_dis=Xd, y=y, splits=splits)
    print(dsets.n_subsets)
    ss = TSStandardize(by_var=True, verbose=verbose)
#     augmix = AugmixSS()
    ds = [dsets.subset(i) for i in range(dsets.n_subsets)]
    dls = TSDataLoaders.from_dsets(*ds, bs=[bs]+[bs]*len(splits), batch_tfms=[ss], shuffle_train=shuffle_train)
#     dls = TSDataLoaders.from_dsets(dsets.train, bs=[128,128])

    return dls

# Cell
def _remove_augs(dls):
    '''
    remove augmentation transforms from dls.after_batch
    '''
    fs = [f for f in dls.after_batch.fs if not issubclass(type(f), AugTransform)]
    print(fs)
    dls.after_batch.fs.clear()
    for f in fs: dls.after_batch.add(f)
    print(dls.after_batch, dls.train.after_batch)

# Cell
def run_training(dls, arch=None, seed=1234, n_epochs=None, max_lr=None, wd=None,
                 loss_fn_name=None, alpha=None, metrics=unweighted_profit,
                 N=2, magnitude=0.1, pct_start=0.3, div_factor=25.0, aug='randaugment', **kwargs):
    # model = ResNetSig(db.features, db.c).to(device)
    '''
    run a training cycle
    parameterization important for keeping track

    '''
    assert loss_fn_name and n_epochs, 'must pass loss_fn_name, and n_epochs'

    print(f'pct_start: {pct_start} div_factor: {div_factor}')
    set_seed(seed)
#     model = arch(db.features, db.c)
    model = arch(6,1)

    _remove_augs(dls)
    augs = RandAugment(N=N, magnitude=magnitude, verbose=True) if aug=='randaugment' else Augmix(
        N=N, magnitude=magnitude, verbose=True)
#     augs  = Augmix(verbose=True)
    dls.after_batch.add(augs)
    loss_fn = get_loss_fn(loss_fn_name, alpha=alpha)
    print(loss_fn)

    learn = Learner(dls, model, loss_func=loss_fn, metrics=metrics)

    learn.fit_one_cycle(n_epochs, max_lr, wd=wd, pct_start=pct_start, div_factor=div_factor)
#     learn.recorder.plot_losses()
#     learn.recorder.plot_metrics()
    return learn

# Cell
#train params
arch = InceptionTimeD
n_epochs = 5
max_lr = 1e-5
wd = 0.03
loss_fn_name = 'leaky_loss'
alpha = 0.5
metrics = [unweighted_profit]#, partial(unweighted_profit, threshold=0.2),
           #partial(unweighted_profit, threshold=0.5)] #[weighted_profit, unweighted_profit_0, unweighted_profit_05]
N = 3
magnitude = 0.4
# bs = [64*4, 64*4*2]  treated as a data_param
# y_range = (-1, 1) # not sure yet about this one
seed = 1234
# ds_name = dataset_name #data_param inferred
# ds_path = str(ds_full_path) #data_param
pct_start=0.3                   #fastai default: 0.3
div_factor = 25.0               #fastai default 25.0
aug='augmix'

#default dict?
train_params = {'arch':arch, 'n_epochs':n_epochs, 'max_lr':max_lr, 'wd':wd, 'loss_fn_name':loss_fn_name, 'alpha':alpha,
               'metrics':metrics, 'N':N, 'magnitude':magnitude,
                #'bs':bs,
                'seed':seed,
                #'ds_name':ds_name,
               'pct_start':pct_start, 'div_factor':div_factor, 'aug':aug}

# Cell
def _losses_from_recorder(r, metrics=False):
    idx = slice(0,2) if not metrics else slice(2,None)
    return r.values[-1][idx]
def _minmax_values_from_recorder(r, metrics=False):
    idx = [0,1] if not metrics else list(range(2, len(r.values[0])))
    f = np.min if not metrics else np.max
    return L([f(L(r.values).itemgot(i)) for i in idx])

# Cell
def get_recorder_dict(recorder):
    '''
    return a dictionary containing train and validation loss and metrics values
    '''
    metrics = recorder.metrics
#     loss_values = [recorder.losses[-1].item(), recorder.val_losses[-1].item()]
    loss_values = _losses_from_recorder(recorder)
#     loss_min_values = [np.min(recorder.losses), np.min(recorder.val_losses)]
    loss_min_values = _minmax_values_from_recorder(recorder)
    metrics_values = _losses_from_recorder(recorder, metrics=True)
#     metrics_max_values = [np.max([m[i] for m in recorder.metrics]) for i in range(len(recorder.metrics[0]))]
    metrics_max_values = _minmax_values_from_recorder(recorder, metrics=True)
    recorder_keys = ['trn_loss', 'val_loss', 'trn_loss_min', 'val_loss_min',
                     *[f'{m.name}_{i}_value' for i,m in enumerate(metrics)], *[f'{m.name}_{i}_max' for i,m in enumerate(metrics)]]
    return dict(zip(recorder_keys, loss_values+loss_min_values+metrics_values+metrics_max_values))

# Cell
def _to_flat_dict(train_params):
    flat_dict={}
    for key,value in train_params.items():
        if key=='metrics':
            for i,_ in enumerate(listify(value)):
                flat_dict[f'metric_{i}'] = value[i].__name__
        elif key=='arch': flat_dict[key] = value.__name__
        else: flat_dict[key]=value
    return flat_dict

# Cell
def _write_results(df, fn):
    if not os.path.isfile(fn):
        df.to_csv(fn, index=False)
    else:
        print('not new')
        df_old = pd.read_csv(fn)
        df_new = pd.concat([df_old, df], ignore_index=True, sort=False)
#         df.to_csv(fn, index=False, mode='a', header=False)
        df_new.to_csv(fn, index=False)

# Cell
def _dict_product(params):
            values = list(itertools.product(*params.values()))
            return [dict(zip(params.keys(), values[i])) for i in range(len(values))]

# Cell
def _get_preds_fn(prefix='val'):
    return f'{prefix}_preds_{abs(hash(datetime.utcnow()))}.pt'

def _get_model_fn(prefix='model'):
    return f'{prefix}_{abs(hash(datetime.utcnow()))}'

# Cell
def _id_from_splits(splits):
    return '_'.join([str(l[-1]//1000) for l in splits])

def _get_ds_id(data_params, splits):
    return f"{data_params['df_path'].stem}_{data_params['col_config_id']}_\
{cols_y}_{_id_from_splits(splits)}"


# Cell
class TSExperiments:
    '''
    Wrapper class for Timeseries modelling experiment
    needed: data_params, train_params for setup
    provides:
        - `run_experiment(df_results)`: run one modelling run using `train_params`
        - `grid_search(hypers, df_results)`: update `train_params` with each possible configuration of `hypers`
        and run the respective experiment
    experimental results and all necessary parameters for reproducibility is stored in `df_results`

    '''
    def __init__(self, save_model=False, preds_path=None, model_path=None, results_path=None):

        #self.train_params = train_params #training params can change, e.g. when running grid search
        self.save_model = save_model## models are big
        self.preds_path = ifnone(preds_path, './experiments/preds')
        self.model_path = ifnone(model_path, './experiments/models')
        self.results_path = ifnone(results_path, './experiments/results')

    def setup_data(self, data_params):
        #read in dataframe
        self.data_params=data_params
        self.df_base = pd.read_csv(data_params['df_path'], nrows=data_params['nrows'])
        #get continuous, discrete, and dependent columns
        cols_c, cols_d, cols_y, splits = map(data_params.get, ['cols_c', 'cols_d', 'cols_y', 'splits'])
        #get splits
#         print(splits, callable(splits))
        self.splits = splits(self.df_base) if callable(splits) else splits
#         print(splits)

        self.bs = data_params['bs']
        self.ds_id = _get_ds_id(data_params, self.splits)
        self.dls = get_dls(self.df_base, cols_c, cols_y, self.splits, cols_d=cols_d, bs=self.bs)

    def setup_training(self, train_params):
        assert hasattr(self, 'data_params'), 'setup_data first'
        self.train_params = train_params
        self.train_params['bs']=self.bs
        self.train_params['ds_id'] = self.ds_id


    def _save_preds(self, test=False):
    #         val_preds_fn = _get_preds_fn()
        preds_fn = _get_preds_fn()
        preds, y_true = self.learn.get_preds(1)
        torch.save(preds, Path(self.preds_path)/preds_fn)
        self.df_dict.update({'val_preds':preds_fn})
        if len(list(self.dls))==3:
            preds_fn = _get_preds_fn('test')
            preds, y_true = self.learn.get_preds(2)
            torch.save(preds, Path(self.preds_path)/preds_fn)
            self.df_dict.update({'test_preds':preds_fn})

    def _save_model(self):
    #         val_preds_fn = _get_preds_fn()
        model_fn = _get_model_fn()
        self.learn.save(model_fn)
        self.df_dict.update({'model_fn':f'{self.learn.model_dir}/{model_fn}.pth'})



    def run_training(self, arch=None, seed=1234, n_epochs=None, max_lr=None, wd=None,
                     loss_fn_name=None, alpha=None, metrics=unweighted_profit,
                     N=2, magnitude=0.1, pct_start=0.3, div_factor=25.0, aug='randaugment',
                     verbose=False, **kwargs):
        # model = ResNetSig(db.features, db.c).to(device)
        '''
        run a training cycle
        parameterization important for keeping track
        '''
        assert loss_fn_name and n_epochs, 'must pass loss_fn_name, and n_epochs'

        print(f'pct_start: {pct_start} div_factor: {div_factor}')
        set_seed(seed)
        ## reset dls.rng --> consistent shuffling
        self.dls.train.rng = random.Random(random.randint(0,2**32-1))
#         huffle_fn(self, idxs): return self.rng.sample(idxs, len(idxs))
#         print(self.)
    #     model = arch(db.features, db.c)
        model = arch(6,1)

        _remove_augs(self.dls)
        print(aug)
        if aug=='randaugment':  augs=RandAugment(N=N, magnitude=magnitude, verbose=verbose)
#         elif aug=='augmix': augs=Augmix(N=N, magnitude=magnitude, verbose=verbose)
        elif aug=='augmix':
            augs=AugmixSS(N=N, magnitude=magnitude, verbose=verbose)
            print(f'augmix order {augs.order}')
        else: augs=None
        print(augs is None)
        if augs:
            self.dls.after_batch.add(augs)
            augs.setup(self.dls[0])
            ## Pipeline.add does not reorder the transforms, but we want the augmentation before the standardisation
            self.dls.after_batch.fs = self.dls.after_batch.fs.sorted(key='order')

        loss_fn = get_loss_fn(loss_fn_name, alpha=alpha)
        print(loss_fn)
        learn = Learner(self.dls, model, loss_func=loss_fn, metrics=metrics, model_dir=self.model_path)
        print(learn.dls.after_batch)

        learn.fit_one_cycle(n_epochs, max_lr, wd=wd, pct_start=pct_start, div_factor=div_factor)
    #     learn.recorder.plot_losses()
    #     learn.recorder.plot_metrics()
        return learn


    def run_experiment(self, df_fn=None):
        '''
        could wrap the dataset parameters
        '''
        assert df_fn is not None, 'please specify results csv filename'

        self.learn = self.run_training(**self.train_params)
#         rec_dict = get_recorder_dict(self.learn.recorder)
        self.df_dict = dict()
        self.df_dict.update(_to_flat_dict(train_params))
        self.df_dict.update(get_recorder_dict(self.learn.recorder))
        self.df_dict['Timestamp'] = str(datetime.now())
        ## store prediction in a separate file as tensors, but add filename
        self._save_preds(test=False)
        if self.save_model: self._save_model()
        _write_results(pd.DataFrame([self.df_dict], index=[0]), Path(self.results_path)/df_fn)
#         return df_dict



    def run_grid_search(self, hypers:dict, df_results_fn=None):
        '''
        run hyper parameter grid search, note that this changes self.train_params
        '''
        hyper_configs = _dict_product(hypers) #list of dictionaries
        if hasattr(self, 'hyper_configs'): self.hyper_configs+=hyper_configs
        else: self.hyper_configs = hyper_configs
        for config in hyper_configs:
            self.train_params.update(config)
            print(self.train_params)
            self.run_experiment(df_fn=df_results_fn)

# Cell
def build_data_params(df_path, trn_end=None, val_end=None, test_end=None, splitter_fn=TSSplitter(),
                      col_config=None, col_fn=None, bs=64, nrows=None):
#     assert col_config or col_fn, 'need to pass either cont. cols and y cols, or a col_fn'

    assert col_config, 'need to pass columns configuration'

    if trn_end and val_end:
        splits=L(L(range(trn_end)), L(range(trn_end, val_end)))
        if test_end: splits.append(L(range(val_end, test_end)))
    else:
        splits = splitter_fn

    cols_c, cols_d, cols_y, cols_config_id = map(col_config.__getitem__, ['cols_c', 'cols_d', 'cols_y', 'id'])

    data_params = defaultdict(lambda:None, {'df_path':df_path, 'splits':splits, 'col_config_id':cols_config_id,
                                            'cols_c':cols_c, 'cols_d':cols_d, 'cols_y':cols_y,
                                             'bs':bs,  'nrows':nrows})
#                'ds_full_path':ds_full_path,
                 #'dataset_name':dataset_id,

    return data_params