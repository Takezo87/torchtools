# AUTOGENERATED! DO NOT EDIT! File to edit: 200_experiments.ipynb (unless otherwise specified).

__all__ = ['ds_fn', 'ds_path', 'trn_end', 'val_end', 'test_end', 'splits', 'ds_config', 'col_config', 'ds_source',
           'ds_full_path', 'dataset_name', 'data_params', 'TSDatasets3', 'get_dls', 'run_training', 'arch', 'n_epochs',
           'max_lr', 'wd', 'loss_fn_name', 'alpha', 'metrics', 'N', 'magnitude', 'bs', 'seed', 'ds_name', 'ds_path',
           'pct_start', 'div_factor', 'aug', 'train_params', 'get_recorder_dict', 'TSExperiments']

# Cell
from .core import *
from .data import *
from .models import *
from .datasets import *
from .augmentations import *
from .datablock import *
from .dataloader import *

# Cell
import pandas as pd
import numpy as np
from fastai2.basics import *

# Cell
## data config
ds_fn = 'bi_sample_anon.csv'
ds_path = Path('./data/custom')

trn_end = 120000
val_end = 160000
test_end = 200000
splits = (L(range(trn_end)), L(range(trn_end, val_end)))
ds_config = f'{int(trn_end/1000)}_{int(val_end/1000)}_{int(test_end/1000)}'

col_config = '6chan_anon_discrete'
cols_c, cols_d, cols_y, n_train = get_discrete_config()

ds_source = Path(ds_fn).stem
ds_full_path = Path(Path(ds_path)/ds_fn)

dataset_name = f'{ds_source}_{col_config}_{cols_y}_{ds_config}'



data_params = defaultdict(lambda:None, {'ds_fn':ds_fn, 'ds_path':ds_path, 'trn_end':trn_end, 'val_end':val_end, 'splits':splits,
              'col_config':col_config, 'cols_c':cols_c, 'cols_d':cols_d, 'cols_y':cols_y,
               'ds_full_path':ds_full_path, 'dataset_name':dataset_name})

# Cell
#tsai.data.core
## slightly adapted version
##NOTE TODO: Why does _ytype=TensorFloat not work (autograd fails)
class TSDatasets3(NumpyDatasets):
    "A dataset that creates tuples from X (and y) and applies `item_tfms`"
    _xtype, _xdistype, _ytype = TSTensor, TSIntTensor, None # Expected X and y output types (torch.Tensor - default - or subclass)
    def __init__(self, X=None, X_dis=None, y=None, items=None, sel_vars=None, sel_steps=None, tfms=None, tls=None, n_inp=None, dl_type=None,
                 inplace=True, **kwargs):
        self.inplace = inplace
        if tls is None:
            X = itemify(to3darray(X), tup_id=0)
            X_dis = itemify(to3darray(X_dis), tup_id=0) if X_dis is not None else X_dis
            y = itemify(y, tup_id=0) if y is not None else y
            items = tuple((X,)) if y is None else tuple((X,y))
            if X_dis is not None: items = tuple((X, X_dis, y)) if y is not None else tuple(X, X_dis,)
            self.tfms = L(ifnone(tfms,[None]*len(ifnone(tls,items))))

#         if X_dis is not None: self.X_dis = X_dis

        self.sel_vars = ifnone(sel_vars, slice(None))
        self.sel_steps = ifnone(sel_steps,slice(None))
#         self.splits_help = splits
        self.tls = L(tls if tls else [TfmdLists(item, t, **kwargs) for item,t in zip(items,self.tfms)])
        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp
        if len(self.tls[0]) > 0:
            _tls_types = [self._xtype, self._ytype] if len(self.tls)==2 else [self._xtype, self._xdistype, self._ytype]
#             print(_tls_types)
#             print(len(self.tls))
#             for tl,_typ in zip(self.tls, _tls_types):
#                 print (len(tl), _typ, type(tl[0]), isinstance(tl[0], torch.Tensor))
            self.types = L([ifnone(_typ, type(tl[0]) if isinstance(tl[0], torch.Tensor) else tensor) for
                            tl,_typ in zip(self.tls, _tls_types)])

            self.types = L([ifnone(_typ, type(tl[0]) if isinstance(tl[0], torch.nn.Sequential) else tensor) for
                            tl,_typ in zip(self.tls, _tls_types)])
            if self.inplace and X and y: self.ptls=L(
                [tensor(X), tensor(y)]) if not X_dis else L([tensor(X), tensor(X_dis), tensor(y)])
            else:
                self.ptls = L([tl if not self.inplace else tl[:] if type(tl[0]).__name__ == 'memmap' else
                               tensor(stack(tl[:])) for tl in self.tls])

    def __getitem__(self, it):

#         for i,(ptl,typ) in enumerate(zip(self.ptls,self.types)):
#             print (i, typ)

#         return tuple([typ(ptl[it])[...,self.sel_vars, self.sel_steps] if i==0 else
#                       typ(ptl[it]) for i,(ptl,typ) in enumerate(zip(self.ptls,self.types))])
        ## do not enable slicing for now
        return tuple([typ(ptl[it]) for i,(ptl,typ) in enumerate(zip(self.ptls,self.types))])


    def subset(self, i):
        if self.inplace:
            X = self.ptls[0][self.splits[i]]
            y = self.ptls[-1][self.splits[i]]
            X_dis = None if len(self.ptls)==2 else self.ptls[1][self.splits[i]]
            print(X.shape, y.shape, X_dis.shape)
            res = type(self)(X=X, X_dis=X_dis, y=y, n_inp=self.n_inp,
                                           inplace=self.inplace, tfms=self.tfms,
                                           sel_vars=self.sel_vars, sel_steps=self.sel_steps)
            res.set_split_idx_fixed(i)
            return res


        else:
            return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp,
                                           inplace=self.inplace, tfms=self.tfms,
                                           sel_vars=self.sel_vars, sel_steps=self.sel_steps)
    @property
    def vars(self): return self[0][0].shape[-2]
    @property
    def len(self): return self[0][0].shape[-1]

    ## do not confuse with set_split_idx contextmanager in fastai2 Datasets
    def set_split_idx_fixed(self, i):
        for tl in self.tls: tl.tfms.split_idx = i


# Cell
def get_dls(df, cols_c, cols_y, splits, cols_d=None, bs=[256, 512], ds_type=TSDatasets3, shuffle_train=True):
    '''
    create dataloaders
    '''
    if cols_d is not None:
        items, _ = df_to_items_discrete(df, [cols_c, cols_d], cols_y, n_train)
    else:
        items, _ = df_to_items(df, cols_c, cols_y, n_train)

    if cols_d: Xc,Xd,y = items_to_arrays(items)
    else: (Xc,y), Xd = items_to_arrays(items), None
    print(ds_type)
    dsets = ds_type(X=Xc, X_dis=Xd, y=y, splits=splits)
    print(dsets.n_subsets)
    ss = TSStandardize()
    ds = [dsets.subset(i) for i in range(dsets.n_subsets)]
    dls = TSDataLoaders.from_dsets(*ds, bs=[128]+[256]*len(splits), batch_tfms=[ss], shuffle_train=shuffle_train)
#     dls = TSDataLoaders.from_dsets(dsets.train, bs=[128,128])

    return dls

# Cell
def _remove_augs(dls):
    '''
    remove augmentation transforms from dls.after_batch
    '''
    fs = [f for f in dls.after_batch.fs if not issubclass(type(f), AugTransform)]
    print(fs)
    dls.after_batch.fs.clear()
    for f in fs: dls.after_batch.add(f)
    print(dls.after_batch, dls.train.after_batch)

# Cell
def run_training(dls, arch=None, seed=1234, n_epochs=None, max_lr=None, wd=None,
                 loss_fn_name=None, alpha=None, metrics=unweighted_profit,
                 N=2, magnitude=0.1, pct_start=0.3, div_factor=25.0, aug='randaugment', **kwargs):
    # model = ResNetSig(db.features, db.c).to(device)
    '''
    run a training cycle
    parameterization important for keeping track

    '''
    assert loss_fn_name and n_epochs, 'must pass loss_fn_name, and n_epochs'

    print(f'pct_start: {pct_start} div_factor: {div_factor}')
    set_seed(seed)
#     model = arch(db.features, db.c)
    model = arch(6,1)

    _remove_augs(dls)
    augs = RandAugment(N=N, magnitude=magnitude, verbose=True) if aug=='randaugment' else Augmix(
        N=N, magnitude=magnitude, verbose=True)
#     augs  = Augmix(verbose=True)
    dls.after_batch.add(augs)
    loss_fn = get_loss_fn(loss_fn_name, alpha=alpha)
    print(loss_fn)

    learn = Learner(dls, model, loss_func=loss_fn, metrics=metrics)

    learn.fit_one_cycle(n_epochs, max_lr, wd=wd, pct_start=pct_start, div_factor=div_factor)
#     learn.recorder.plot_losses()
#     learn.recorder.plot_metrics()
    return learn

# Cell
#train params
arch = InceptionTimeD
n_epochs = 5
max_lr = 1e-5
wd = 0.03
loss_fn_name = 'leaky_loss'
alpha = 0.5
metrics = [unweighted_profit]#, partial(unweighted_profit, threshold=0.2),
           #partial(unweighted_profit, threshold=0.5)] #[weighted_profit, unweighted_profit_0, unweighted_profit_05]
N = 3
magnitude = 0.4
bs = [64*4, 64*4*2]  ## the validation batch size is not a training parameter...
# y_range = (-1, 1) # not sure yet about this one
seed = 1234
ds_name = dataset_name          # base dataset identifier, source csv, column config, split info
ds_path = str(ds_full_path)
pct_start=0.3                   #fastai default: 0.3
div_factor = 25.0               #fastai default 25.0
aug='augmix'

#default dict?
train_params = {'arch':arch, 'n_epochs':n_epochs, 'max_lr':max_lr, 'wd':wd, 'loss_fn_name':loss_fn_name, 'alpha':alpha,
               'metrics':metrics, 'N':N, 'magnitude':magnitude, 'bs':bs, 'seed':seed, 'ds_name':ds_name,
               'pct_start':pct_start, 'div_factor':div_factor, 'aug':aug}

# Cell
def _losses_from_recorder(r, metrics=False):
    idx = slice(0,2) if not metrics else slice(2,None)
    return r.values[-1][idx]
def _minmax_values_from_recorder(r, metrics=False):
    idx = [0,1] if not metrics else list(range(2, len(r.values[0])))
    f = np.min if not metrics else np.max
    return L([f(L(r.values).itemgot(i)) for i in idx])

# Cell
def get_recorder_dict(recorder):
    '''
    return a dictionary containing train and validation loss and metrics values
    '''
    metrics = recorder.metrics
#     loss_values = [recorder.losses[-1].item(), recorder.val_losses[-1].item()]
    loss_values = _losses_from_recorder(recorder)
#     loss_min_values = [np.min(recorder.losses), np.min(recorder.val_losses)]
    loss_min_values = _minmax_values_from_recorder(recorder)
    metrics_values = _losses_from_recorder(recorder, metrics=True)
#     metrics_max_values = [np.max([m[i] for m in recorder.metrics]) for i in range(len(recorder.metrics[0]))]
    metrics_max_values = _minmax_values_from_recorder(recorder, metrics=True)
    recorder_keys = ['trn_loss', 'val_loss', 'trn_loss_min', 'val_loss_min',
                     *[f'{m.name}_{i}_value' for i,m in enumerate(metrics)], *[f'{m.name}_{i}_max' for i,m in enumerate(metrics)]]
    return dict(zip(recorder_keys, loss_values+loss_min_values+metrics_values+metrics_max_values))

# Cell
def _to_flat_dict(train_params):
    flat_dict={}
    for key,value in train_params.items():
        if key=='metrics':
            for i,_ in enumerate(listify(value)):
                flat_dict[f'metric_{i}'] = value[i].__name__
        elif key=='arch': flat_dict[key] = value.__name__
        else: flat_dict[key]=value
    return flat_dict

# Cell
def _write_results(df, fn):
    if not os.path.isfile(fn):
        df.to_csv(fn, index=False)
    else:
        print('not new')
        df_old = pd.read_csv(fn)
        df_new = pd.concat([df_old, df], ignore_index=True, sort=False)
#         df.to_csv(fn, index=False, mode='a', header=False)
        df_new.to_csv(fn, index=False)

# Cell
def _dict_product(params):
            values = list(itertools.product(*params.values()))
            return [dict(zip(params.keys(), values[i])) for i in range(len(values))]

# Cell
def _get_preds_fn(prefix='val'):
    return f'{prefix}_preds_{abs(hash(datetime.utcnow()))}.pt'

def _get_model_fn(prefix='model'):
    return f'{prefix}_{abs(hash(datetime.utcnow()))}'

# Cell
class TSExperiments:
    '''
    Wrapper class for TS modelling experiments
    '''
    def __init__(self, train_params, save_model=False, preds_path=None, model_path=None, results_path=None):

        self.train_params = train_params #training params can change, e.g. when running grid search
        self.save_model = save_model## models are big
        self.preds_path = ifnone(preds_path, './experiments/preds')
        self.model_path = ifnone(model_path, './experiments/models')
        self.results_path = ifnone(results_path, './experiments/results')

    def setup_data(self, df_base, data_params):
        cols_c, cols_d, cols_y, splits = map(data_params.get, ['cols_c', 'cols_d', 'cols_y', 'splits'])

        self.bs = self.train_params['bs']
        self.splits = splits
        self.dls = get_dls(df_base, cols_c, cols_y, splits, cols_d=cols_d, bs=self.bs)


    def _save_preds(self, test=False):
    #         val_preds_fn = _get_preds_fn()
        preds_fn = _get_preds_fn()
        preds, y_true = self.learn.get_preds(1)
        torch.save(preds, Path(self.preds_path)/preds_fn)
        self.df_dict.update({'val_preds':preds_fn})
        if len(list(self.dls))==3:
            preds_fn = _get_preds_fn('test')
            preds, y_true = self.learn.get_preds(2)
            torch.save(preds, Path(self.preds_path)/preds_fn)
            self.df_dict.update({'test_preds':preds_fn})

    def _save_model(self):
    #         val_preds_fn = _get_preds_fn()
        model_fn = _get_model_fn()
        self.learn.save(model_fn)
        self.df_dict.update({'model_fn':f'{self.learn.model_dir}/{model_fn}.pth'})



    def run_training(self, arch=None, seed=1234, n_epochs=None, max_lr=None, wd=None,
                     loss_fn_name=None, alpha=None, metrics=unweighted_profit,
                     N=2, magnitude=0.1, pct_start=0.3, div_factor=25.0, aug='randaugment',
                     verbose=False, **kwargs):
        # model = ResNetSig(db.features, db.c).to(device)
        '''
        run a training cycle
        parameterization important for keeping track
        '''
        assert loss_fn_name and n_epochs, 'must pass loss_fn_name, and n_epochs'

        print(f'pct_start: {pct_start} div_factor: {div_factor}')
        set_seed(seed)
        ## reset dls.rng --> consistent shuffling
        self.dls.train.rng = random.Random(random.randint(0,2**32-1))
#         huffle_fn(self, idxs): return self.rng.sample(idxs, len(idxs))
#         print(self.)
    #     model = arch(db.features, db.c)
        model = arch(6,1)

        _remove_augs(self.dls)
        print(aug)
        if aug=='randaugment':  augs=RandAugment(N=N, magnitude=magnitude, verbose=verbose)
        elif aug=='augmix': augs=Augmix(N=N, magnitude=magnitude, verbose=verbose)
        else: augs=None
        print(augs is None)
        if augs: self.dls.after_batch.add(augs)

        loss_fn = get_loss_fn(loss_fn_name, alpha=alpha)
        print(loss_fn)
        learn = Learner(self.dls, model, loss_func=loss_fn, metrics=metrics, model_dir=self.model_path)
        print(learn.dls.after_batch)

        learn.fit_one_cycle(n_epochs, max_lr, wd=wd, pct_start=pct_start, div_factor=div_factor)
    #     learn.recorder.plot_losses()
    #     learn.recorder.plot_metrics()
        return learn


    def run_experiment(self, df_fn=None):
        '''
        could wrap the dataset parameters
        '''
        assert df_fn is not None, 'please specify results csv filename'

        self.learn = self.run_training(**self.train_params)
#         rec_dict = get_recorder_dict(self.learn.recorder)
        self.df_dict = dict()
        self.df_dict.update(_to_flat_dict(train_params))
        self.df_dict.update(get_recorder_dict(self.learn.recorder))
        self.df_dict['Timestamp'] = str(datetime.now())
        ## store prediction in a separate file as tensors, but add filename
        self._save_preds(test=False)
        if self.save_model: self._save_model()
        _write_results(pd.DataFrame([self.df_dict], index=[0]), Path(self.results_path)/df_fn)
#         return df_dict



    def run_grid_search(self, hypers:dict, df_results_fn=None):
        '''
        run hyper parameter grid search, note that this changes self.train_params
        '''
        hyper_configs = _dict_product(hypers) #list of dictionaries
        if hasattr(self, 'hyper_configs'): self.hyper_configs+=hyper_configs
        else: self.hyper_configs = hyper_configs
        for config in hyper_configs:
            self.train_params.update(config)
            print(self.train_params)
            self.run_experiment(df_fn=df_results_fn)