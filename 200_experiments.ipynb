{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "> refactor modelling, experiment functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torchtools.core import *\n",
    "from torchtools.data import *\n",
    "from torchtools.models import *\n",
    "from torchtools.datasets import *\n",
    "from torchtools.augmentations import *\n",
    "from torchtools.datablock import *\n",
    "from torchtools.dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai2.basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worklflow with Discrete Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "## data config\n",
    "ds_fn = 'bi_sample_anon.csv'\n",
    "ds_path = Path('./data/custom')\n",
    "\n",
    "trn_end = 120000\n",
    "val_end = 160000\n",
    "test_end = 200000\n",
    "splits = (L(range(trn_end)), L(range(trn_end, val_end)))\n",
    "ds_config = f'{int(trn_end/1000)}_{int(val_end/1000)}_{int(test_end/1000)}'\n",
    "\n",
    "col_config = '6chan_anon_discrete'\n",
    "cols_c, cols_d, cols_y, n_train = get_discrete_config()\n",
    "\n",
    "ds_source = Path(ds_fn).stem\n",
    "ds_full_path = Path(Path(ds_path)/ds_fn)\n",
    "\n",
    "dataset_name = f'{ds_source}_{col_config}_{cols_y}_{ds_config}'\n",
    "\n",
    "\n",
    "\n",
    "data_params = defaultdict(lambda:None, {'ds_fn':ds_fn, 'ds_path':ds_path, 'trn_end':trn_end, 'val_end':val_end, 'splits':splits,\n",
    "              'col_config':col_config, 'cols_c':cols_c, 'cols_d':cols_d, 'cols_y':cols_y, \n",
    "               'ds_full_path':ds_full_path, 'dataset_name':dataset_name})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows=200000\n",
    "df_main = pd.read_csv(data_params['ds_full_path'], nrows=nrows)\n",
    "\n",
    "# cols_c, cols_d, cols_y, n_train = get_discrete_config()\n",
    "# splits = TSSplitter()(df_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df_main.iloc[:10000]\n",
    "splits_small = TSSplitter()(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#8000) [0,1,2,3,4,5,6,7,8,9...],\n",
       " (#2000) [8000,8001,8002,8003,8004,8005,8006,8007,8008,8009...])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#tsai.data.core\n",
    "## slightly adapted version\n",
    "##NOTE TODO: Why does _ytype=TensorFloat not work (autograd fails)\n",
    "class TSDatasets3(NumpyDatasets):\n",
    "    \"A dataset that creates tuples from X (and y) and applies `item_tfms`\"\n",
    "    _xtype, _xdistype, _ytype = TSTensor, TSIntTensor, None # Expected X and y output types (torch.Tensor - default - or subclass)\n",
    "    def __init__(self, X=None, X_dis=None, y=None, items=None, sel_vars=None, sel_steps=None, tfms=None, tls=None, n_inp=None, dl_type=None,\n",
    "                 inplace=True, **kwargs):\n",
    "        self.inplace = inplace\n",
    "        if tls is None:\n",
    "            X = itemify(to3darray(X), tup_id=0)\n",
    "            X_dis = itemify(to3darray(X_dis), tup_id=0) if X_dis is not None else X_dis\n",
    "            y = itemify(y, tup_id=0) if y is not None else y\n",
    "            items = tuple((X,)) if y is None else tuple((X,y))\n",
    "            if X_dis is not None: items = tuple((X, X_dis, y)) if y is not None else tuple(X, X_dis,)\n",
    "            self.tfms = L(ifnone(tfms,[None]*len(ifnone(tls,items))))\n",
    "            \n",
    "#         if X_dis is not None: self.X_dis = X_dis\n",
    "       \n",
    "        self.sel_vars = ifnone(sel_vars, slice(None))\n",
    "        self.sel_steps = ifnone(sel_steps,slice(None))\n",
    "#         self.splits_help = splits\n",
    "        self.tls = L(tls if tls else [TfmdLists(item, t, **kwargs) for item,t in zip(items,self.tfms)])\n",
    "        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n",
    "        if len(self.tls[0]) > 0:\n",
    "            _tls_types = [self._xtype, self._ytype] if len(self.tls)==2 else [self._xtype, self._xdistype, self._ytype]\n",
    "#             print(_tls_types)\n",
    "#             print(len(self.tls))\n",
    "#             for tl,_typ in zip(self.tls, _tls_types):\n",
    "#                 print (len(tl), _typ, type(tl[0]), isinstance(tl[0], torch.Tensor))\n",
    "            self.types = L([ifnone(_typ, type(tl[0]) if isinstance(tl[0], torch.Tensor) else tensor) for \n",
    "                            tl,_typ in zip(self.tls, _tls_types)])\n",
    "    \n",
    "            self.types = L([ifnone(_typ, type(tl[0]) if isinstance(tl[0], torch.nn.Sequential) else tensor) for \n",
    "                            tl,_typ in zip(self.tls, _tls_types)])\n",
    "            if self.inplace and X and y: self.ptls=L(\n",
    "                [tensor(X), tensor(y)]) if not X_dis else L([tensor(X), tensor(X_dis), tensor(y)])\n",
    "            else:\n",
    "                self.ptls = L([tl if not self.inplace else tl[:] if type(tl[0]).__name__ == 'memmap' else \n",
    "                               tensor(stack(tl[:])) for tl in self.tls])\n",
    "\n",
    "    def __getitem__(self, it):\n",
    "        \n",
    "#         for i,(ptl,typ) in enumerate(zip(self.ptls,self.types)):\n",
    "#             print (i, typ)\n",
    "        \n",
    "#         return tuple([typ(ptl[it])[...,self.sel_vars, self.sel_steps] if i==0 else \n",
    "#                       typ(ptl[it]) for i,(ptl,typ) in enumerate(zip(self.ptls,self.types))])\n",
    "        ## do not enable slicing for now \n",
    "        return tuple([typ(ptl[it]) for i,(ptl,typ) in enumerate(zip(self.ptls,self.types))])\n",
    "    \n",
    "\n",
    "    def subset(self, i): \n",
    "        if self.inplace:\n",
    "            X = self.ptls[0][self.splits[i]]\n",
    "            y = self.ptls[-1][self.splits[i]]\n",
    "            X_dis = None if len(self.ptls)==2 else self.ptls[1][self.splits[i]]\n",
    "            print(X.shape, y.shape, X_dis.shape)\n",
    "            return type(self)(X=X, X_dis=X_dis, y=y, n_inp=self.n_inp, \n",
    "                                           inplace=self.inplace, tfms=self.tfms,\n",
    "                                           sel_vars=self.sel_vars, sel_steps=self.sel_steps)\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp, \n",
    "                                           inplace=self.inplace, tfms=self.tfms,\n",
    "                                           sel_vars=self.sel_vars, sel_steps=self.sel_steps)\n",
    "    @property\n",
    "    def vars(self): return self[0][0].shape[-2]\n",
    "    @property\n",
    "    def len(self): return self[0][0].shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4, 10) (10000, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "items, _ = df_to_items_discrete(df_small, [cols_c, cols_d], cols_y, n_train)\n",
    "Xc,Xd,y = items_to_arrays(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_test = L(L(range(7000)), L(range(7000,9000)), L(range(9000,10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [(#7000) [0,1,2,3,4,5,6,7,8,9...],(#2000) [7000,7001,7002,7003,7004,7005,7006,7007,7008,7009...],(#1000) [9000,9001,9002,9003,9004,9005,9006,9007,9008,9009...]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = TSDatasets3(X=Xc, X_dis=Xd, y=y, splits=splits_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7000, 4, 10]) torch.Size([7000]) torch.Size([7000, 2, 10])\n",
      "torch.Size([2000, 4, 10]) torch.Size([2000]) torch.Size([2000, 2, 10])\n",
      "torch.Size([1000, 4, 10]) torch.Size([1000]) torch.Size([1000, 2, 10])\n"
     ]
    }
   ],
   "source": [
    "ds = [dsets.subset(i) for i in range(dsets.n_subsets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TSDataLoaders.from_dsets(*ds, batch_tfms=[TSStandardize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.n_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.dataset.n_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets.n_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls[2].n_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtools.dataloader.TSDataLoader at 0x7f2b9779f110>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSDataLoaders??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoaders??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dls(df, cols_c, cols_y, splits, cols_d=None, bs=[256, 512], ds_type=TSDatasets3):\n",
    "    '''\n",
    "    create dataloaders\n",
    "    '''\n",
    "    if cols_d is not None:\n",
    "        items, _ = df_to_items_discrete(df, [cols_c, cols_d], cols_y, n_train)\n",
    "    else:\n",
    "        items, _ = df_to_items(df, cols_c, cols_y, n_train)\n",
    "        \n",
    "    if cols_d: Xc,Xd,y = items_to_arrays(items)\n",
    "    else: (Xc,y), Xd = items_to_arrays(items), None\n",
    "    print(ds_type)\n",
    "    dsets = ds_type(X=Xc, X_dis=Xd, y=y, splits=splits)\n",
    "    print(dsets.n_subsets)\n",
    "    ss = TSStandardize()\n",
    "    ds = [dsets.subset(i) for i in range(dsets.n_subsets)]\n",
    "    dls = TSDataLoaders.from_dsets(*ds, bs=[128]+[256]*len(splits), batch_tfms=[ss])\n",
    "#     dls = TSDataLoaders.from_dsets(dsets.train, bs=[128,128])\n",
    "    \n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4, 10) (10000, 2, 10)\n",
      "<class '__main__.TSDatasets3'>\n",
      "3\n",
      "torch.Size([7000, 4, 10]) torch.Size([7000]) torch.Size([7000, 2, 10])\n",
      "torch.Size([2000, 4, 10]) torch.Size([2000]) torch.Size([2000, 2, 10])\n",
      "torch.Size([1000, 4, 10]) torch.Size([1000]) torch.Size([1000, 2, 10])\n",
      "CPU times: user 1.08 s, sys: 64.8 ms, total: 1.15 s\n",
      "Wall time: 609 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dls = get_dls(df_small, cols_c, cols_y, splits_test, cols_d=cols_d, ds_type=TSDatasets3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.n_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtools.dataloader.TSDataLoader at 0x7f2b99f4a190>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TSTensor([[[-0.3181,  1.3520, -0.2926,  ...,  1.3520, -0.2613,  1.3520],\n",
       "          [-1.5024,  1.3520, -1.0529,  ...,  1.3520,  1.3520,  0.0159],\n",
       "          [-0.2714,  1.3520, -0.1850,  ...,  1.3520, -0.3939,  1.3520],\n",
       "          [-1.4054,  1.3520, -1.4363,  ...,  1.3520, -1.6336,  1.3520]],\n",
       " \n",
       "         [[-1.3615, -0.3423,  1.3520,  ...,  1.3520, -0.3521, -0.5039],\n",
       "          [-1.4054,  0.0159,  1.3520,  ...,  1.3520, -1.3202, -1.4054],\n",
       "          [-0.8930,  1.3520, -0.6799,  ..., -2.8886, -0.1749, -0.8297],\n",
       "          [-1.2688,  1.3520,  0.0159,  ..., -1.2445,  1.3520, -1.5938]],\n",
       " \n",
       "         [[-0.5316,  1.3520, -0.2823,  ..., -0.3140, -0.1644,  1.3520],\n",
       "          [ 0.0159,  0.0159,  0.0159,  ...,  0.0159,  0.0159,  0.0159],\n",
       "          [-0.4864, -1.0040,  1.3520,  ..., -1.0198, -0.7305, -1.7655],\n",
       "          [ 0.0159,  0.0159,  0.0159,  ...,  0.0159,  0.0159,  0.0159]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.0883,  1.3520, -0.7347,  ...,  1.3520, -0.7519,  1.3520],\n",
       "          [-0.7182,  1.3520,  0.0159,  ...,  1.3520, -1.3337,  1.3520],\n",
       "          [ 1.3520, -0.9738,  1.3520,  ..., -0.3647, -1.2940, -0.6657],\n",
       "          [ 1.3520, -1.2940,  0.6840,  ...,  1.3520, -1.2940, -1.1770]],\n",
       " \n",
       "         [[-0.5294, -0.3794, -1.1163,  ..., -0.3691, -0.6026, -0.4664],\n",
       "          [ 0.0159, -1.0792,  0.0159,  ...,  0.0159,  0.0159,  0.0159],\n",
       "          [-0.1344, -0.5080, -0.5970,  ..., -1.5938, -0.4845, -0.2630],\n",
       "          [ 1.3520,  0.0159, -1.1359,  ..., -1.5938,  1.3520,  0.0159]],\n",
       " \n",
       "         [[-0.2031, -0.1578, -1.5024,  ...,  1.3520, -0.5970,  1.3520],\n",
       "          [-1.5747,  0.6840, -1.1163,  ...,  0.0159, -1.1260,  0.0159],\n",
       "          [ 1.3520, -0.3990, -1.0529,  ..., -0.5294,  1.3520,  1.3520],\n",
       "          [ 1.3520,  1.3520, -1.3905,  ...,  0.0159,  0.0159, -1.3337]]],\n",
       "        device='cuda:0'), TSIntTensor([[[-2,  1, -1,  ...,  1,  0,  3],\n",
       "          [-3,  3, -2,  ...,  1, -1,  2]],\n",
       " \n",
       "         [[ 0,  0,  2,  ...,  1, -1, -2],\n",
       "          [-1,  2,  0,  ...,  0,  0, -2]],\n",
       " \n",
       "         [[-1,  1,  0,  ..., -2, -1,  1],\n",
       "          [-1, -1,  1,  ...,  0, -1,  0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0,  2,  0,  ...,  4, -1,  1],\n",
       "          [ 3, -1,  1,  ...,  0, -1, -1]],\n",
       " \n",
       "         [[ 0, -2,  0,  ...,  0,  0, -2],\n",
       "          [ 0, -1, -1,  ...,  0,  0, -1]],\n",
       " \n",
       "         [[-3, -1,  0,  ...,  4, -2,  1],\n",
       "          [ 1,  0, -2,  ..., -2,  2,  1]]], device='cuda:0'), tensor([ -16.8067,  -27.6243,  100.0000,  -48.0769,  -37.0370,  -77.5194,\n",
       "          -67.1141,  -37.0370,  -38.7597,  -10.4167,  100.0000,  100.0000,\n",
       "          100.0000,  100.0000,  -32.0513,  -77.5194,  -67.1141,  100.0000,\n",
       "          -33.5570,  -20.1613,  -42.7350,  -20.1613, -133.3333,  -43.6681,\n",
       "          -12.8205,  -24.0964,  -44.2478,  -76.3359,  -27.2480,  -31.7460,\n",
       "          -67.5676,  100.0000,  -62.5000,  -19.5695,  -38.7597,  -14.3885,\n",
       "         -100.0000,  100.0000,  100.0000, -123.4568,  -24.2718,  -37.8788,\n",
       "          -25.0000,  -38.4615,  100.0000,  100.0000,  -87.7193,  -31.6456,\n",
       "          100.0000,  100.0000,  100.0000,  100.0000,  100.0000,  -96.1538,\n",
       "          -90.0901,  -46.2963,  -64.9351,  -44.0529,  -27.3973,  100.0000,\n",
       "         -263.1579,  -17.5439, -153.8462,  -22.8833,  100.0000,  100.0000,\n",
       "          -34.9650,  100.0000,  -50.0000,  -62.1118,  -42.7350,  100.0000,\n",
       "          -14.7059,  100.0000,  -27.8552,  100.0000,  100.0000,  -63.2911,\n",
       "          -29.9401,  100.0000,  -63.6943,  -85.4701,  100.0000,  -19.5312,\n",
       "          100.0000,  -15.5039,  -45.6621,  100.0000,  -62.5000,  -50.5051,\n",
       "          100.0000,  100.0000,  100.0000,  -28.1690,  100.0000,  -73.5294,\n",
       "         -156.2500,  100.0000,  100.0000,  -21.2766, -153.8462,  100.0000,\n",
       "          100.0000,  -55.5556, -128.2051, -131.5789,  100.0000,  100.0000,\n",
       "          -16.9492,  -22.2222,  -28.7356,  -46.2963,  100.0000,  -30.8642,\n",
       "          100.0000,  -35.0877,  -78.7402,  100.0000,  -12.0773,  -16.0000,\n",
       "          100.0000,  100.0000, -123.4568,  100.0000,  100.0000,  100.0000,\n",
       "          -37.7358,  -35.0877], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 4, 10) (200000, 2, 10)\n",
      "CPU times: user 13.1 s, sys: 1.1 s, total: 14.2 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dls = get_dls(df_main, cols_c, cols_y, splits, cols_d=cols_d, ds_type=TSDatasets3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 4, 10) (200000, 2, 10)\n",
      "[<class 'torchtools.data.TSTensor'>, <class 'torchtools.data.TSIntTensor'>, None]\n",
      "[<class 'torchtools.data.TSTensor'>, <class 'torchtools.data.TSIntTensor'>, None]\n",
      "[<class 'torchtools.data.TSTensor'>, <class 'torchtools.data.TSIntTensor'>, None]\n",
      "CPU times: user 1min 3s, sys: 508 ms, total: 1min 4s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#TSDatasets2\n",
    "dls = get_dls(df_main, cols_c, cols_y, splits, cols_d=cols_d, ds_type=TSDatasets2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _remove_augs(dls):\n",
    "    '''\n",
    "    remove augmentation transforms from dls.after_batch\n",
    "    '''\n",
    "    fs = [f for f in dls.after_batch.fs if not issubclass(type(f), AugTransform)]\n",
    "    print(fs)\n",
    "    dls.after_batch.fs.clear()\n",
    "    for f in fs: dls.after_batch.add(f)\n",
    "    print(dls.after_batch, dls.train.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4, 10) (10000, 2, 10)\n",
      "<class '__main__.TSDatasets3'>\n",
      "3\n",
      "torch.Size([8000, 4, 10]) torch.Size([8000]) torch.Size([8000, 2, 10])\n",
      "torch.Size([1000, 4, 10]) torch.Size([1000]) torch.Size([1000, 2, 10])\n",
      "torch.Size([1000, 4, 10]) torch.Size([1000]) torch.Size([1000, 2, 10])\n",
      "tfms None\n"
     ]
    }
   ],
   "source": [
    "dls = get_dls(df_small, data_params['cols_c'], data_params['cols_y'], splits_test, \n",
    "              cols_d=data_params['cols_d'])\n",
    "dls.after_batch.add(Augmix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtools.dataloader.TSDataLoader object at 0x7f87045ca210>\n",
      "<torchtools.dataloader.TSDataLoader object at 0x7f8704038fd0>\n",
      "<torchtools.dataloader.TSDataLoader object at 0x7f86ecb0b4d0>\n"
     ]
    }
   ],
   "source": [
    "for dl in dls:\n",
    "    print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: TSStandardize -> Augmix Pipeline: TSStandardize -> Augmix Pipeline: TSStandardize\n",
      "[TSStandardize: (TSTensor,object) -> encodes\n",
      "(NumpyTensor,object) -> encodes ]\n",
      "Pipeline: TSStandardize Pipeline: TSStandardize\n",
      "Pipeline: TSStandardize Pipeline: TSStandardize Pipeline: TSStandardize\n"
     ]
    }
   ],
   "source": [
    "print(dls.after_batch, dls.train.after_batch, dls.valid.after_batch)\n",
    "_remove_augs(dls)\n",
    "print(dls.after_batch, dls.train.after_batch, dls.valid.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_training(dls, arch=None, seed=1234, n_epochs=None, max_lr=None, wd=None, \n",
    "                 loss_fn_name=None, alpha=None, metrics=unweighted_profit, \n",
    "                 N=2, magnitude=0.1, pct_start=0.3, div_factor=25.0, aug='randaugment', **kwargs):\n",
    "    # model = ResNetSig(db.features, db.c).to(device)\n",
    "    '''\n",
    "    run a training cycle\n",
    "    parameterization important for keeping track\n",
    "    \n",
    "    '''\n",
    "    assert loss_fn_name and n_epochs, 'must pass loss_fn_name, and n_epochs'\n",
    "    \n",
    "    print(f'pct_start: {pct_start} div_factor: {div_factor}')\n",
    "    set_seed(seed)\n",
    "#     model = arch(db.features, db.c)\n",
    "    model = arch(6,1)\n",
    "    \n",
    "    _remove_augs(dls)\n",
    "    augs = RandAugment(N=N, magnitude=magnitude, verbose=True) if aug=='randaugment' else Augmix(\n",
    "        N=N, magnitude=magnitude, verbose=True)\n",
    "#     augs  = Augmix(verbose=True)\n",
    "    dls.after_batch.add(augs)       \n",
    "    loss_fn = get_loss_fn(loss_fn_name, alpha=alpha)\n",
    "    print(loss_fn)\n",
    "    \n",
    "    learn = Learner(dls, model, loss_func=loss_fn, metrics=metrics)\n",
    "\n",
    "    learn.fit_one_cycle(n_epochs, max_lr, wd=wd, pct_start=pct_start, div_factor=div_factor)\n",
    "#     learn.recorder.plot_losses()\n",
    "#     learn.recorder.plot_metrics()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#train params\n",
    "arch = InceptionTimeD\n",
    "n_epochs = 5\n",
    "max_lr = 1e-5\n",
    "wd = 0.03\n",
    "loss_fn_name = 'leaky_loss'\n",
    "alpha = 0.5\n",
    "metrics = [unweighted_profit]#, partial(unweighted_profit, threshold=0.2), \n",
    "           #partial(unweighted_profit, threshold=0.5)] #[weighted_profit, unweighted_profit_0, unweighted_profit_05]\n",
    "N = 3\n",
    "magnitude = 0.4\n",
    "bs = [64*4, 64*4*2]  ## the validation batch size is not a training parameter...\n",
    "# y_range = (-1, 1) # not sure yet about this one\n",
    "seed = 1234\n",
    "ds_name = dataset_name          # base dataset identifier, source csv, column config, split info\n",
    "ds_path = str(ds_full_path)\n",
    "pct_start=0.3                   #fastai default: 0.3\n",
    "div_factor = 25.0               #fastai default 25.0\n",
    "aug='augmix'\n",
    "\n",
    "#default dict?\n",
    "train_params = {'arch':arch, 'n_epochs':n_epochs, 'max_lr':max_lr, 'wd':wd, 'loss_fn_name':loss_fn_name, 'alpha':alpha,\n",
    "               'metrics':metrics, 'N':N, 'magnitude':magnitude, 'bs':bs, 'seed':seed, 'ds_name':ds_name,\n",
    "               'pct_start':pct_start, 'div_factor':div_factor, 'aug':aug}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pct_start: 0.3 div_factor: 25.0\n",
      "[TSStandardize: (TSTensor,object) -> encodes\n",
      "(NumpyTensor,object) -> encodes ]\n",
      "Pipeline: TSStandardize Pipeline: TSStandardize\n",
      "tfms None\n",
      "functools.partial(<function leaky_loss at 0x7fa0150f7830>, alpha=0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>unweighted_profit</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.100198</td>\n",
       "      <td>-0.123712</td>\n",
       "      <td>0.314173</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.063339</td>\n",
       "      <td>-0.414314</td>\n",
       "      <td>0.098768</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.128210</td>\n",
       "      <td>-0.423636</td>\n",
       "      <td>-0.081150</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.180203</td>\n",
       "      <td>-0.419215</td>\n",
       "      <td>-0.059166</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.193404</td>\n",
       "      <td>-0.417435</td>\n",
       "      <td>0.054366</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = run_training(dls, **train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _losses_from_recorder(r, metrics=False):\n",
    "    idx = slice(0,2) if not metrics else slice(2,None)\n",
    "    return r.values[-1][idx]\n",
    "def _minmax_values_from_recorder(r, metrics=False):\n",
    "    idx = [0,1] if not metrics else list(range(2, len(r.values[0])))\n",
    "    f = np.min if not metrics else np.max\n",
    "    return L([f(L(r.values).itemgot(i)) for i in idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_recorder_dict(recorder):\n",
    "    '''\n",
    "    return a dictionary containing train and validation loss and metrics values\n",
    "    '''\n",
    "    metrics = recorder.metrics\n",
    "#     loss_values = [recorder.losses[-1].item(), recorder.val_losses[-1].item()]\n",
    "    loss_values = _losses_from_recorder(recorder)\n",
    "#     loss_min_values = [np.min(recorder.losses), np.min(recorder.val_losses)] \n",
    "    loss_min_values = _minmax_values_from_recorder(recorder)\n",
    "    metrics_values = _losses_from_recorder(recorder, metrics=True)\n",
    "#     metrics_max_values = [np.max([m[i] for m in recorder.metrics]) for i in range(len(recorder.metrics[0]))]\n",
    "    metrics_max_values = _minmax_values_from_recorder(recorder, metrics=True)\n",
    "    recorder_keys = ['trn_loss', 'val_loss', 'trn_loss_min', 'val_loss_min', \n",
    "                     *[f'{m.name}_{i}_value' for i,m in enumerate(metrics)], *[f'{m.name}_{i}_max' for i,m in enumerate(metrics)]]\n",
    "    return dict(zip(recorder_keys, loss_values+loss_min_values+metrics_values+metrics_max_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = get_recorder_dict(learn.recorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trn_loss': -0.1934041678905487,\n",
       " 'val_loss': -0.41743481159210205,\n",
       " 'trn_loss_min': -0.1934041678905487,\n",
       " 'val_loss_min': -0.42363613843917847,\n",
       " 'unweighted_profit_0_value': 0.05436611920595169,\n",
       " 'unweighted_profit_0_max': 0.31417301297187805}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _to_flat_dict(train_params):\n",
    "    flat_dict={}\n",
    "    for key,value in train_params.items():\n",
    "        if key=='metrics':\n",
    "            for i,_ in enumerate(listify(value)):\n",
    "                flat_dict[f'metric_{i}'] = value[i].__name__\n",
    "        elif key=='arch': flat_dict[key] = value.__name__\n",
    "        else: flat_dict[key]=value\n",
    "    return flat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _write_results(df, fn):\n",
    "    if not os.path.isfile(fn):\n",
    "        df.to_csv(fn, index=False)\n",
    "    else:\n",
    "        print('not new')\n",
    "        df_old = pd.read_csv(fn)\n",
    "        df_new = pd.concat([df_old, df], ignore_index=True, sort=False)\n",
    "#         df.to_csv(fn, index=False, mode='a', header=False)\n",
    "        df_new.to_csv(fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arch': torchtools.models.InceptionTimeD,\n",
       " 'n_epochs': 5,\n",
       " 'max_lr': 1e-05,\n",
       " 'wd': 0.03,\n",
       " 'loss_fn_name': 'leaky_loss',\n",
       " 'alpha': 0.5,\n",
       " 'metrics': [<function torchtools.core.unweighted_profit(preds, y_true, threshold=0)>],\n",
       " 'N': 3,\n",
       " 'magnitude': 0.4,\n",
       " 'bs': [256, 512],\n",
       " 'seed': 1234,\n",
       " 'ds_name': 'bi_sample_anon_6chan_anon_discrete_y0_120_160_200',\n",
       " 'pct_start': 0.3,\n",
       " 'div_factor': 25.0,\n",
       " 'aug': 'augmix'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arch': 'InceptionTimeD',\n",
       " 'n_epochs': 5,\n",
       " 'max_lr': 1e-05,\n",
       " 'wd': 0.03,\n",
       " 'loss_fn_name': 'leaky_loss',\n",
       " 'alpha': 0.5,\n",
       " 'metric_0': 'unweighted_profit',\n",
       " 'N': 3,\n",
       " 'magnitude': 0.4,\n",
       " 'bs': [256, 512],\n",
       " 'seed': 1234,\n",
       " 'ds_name': 'bi_sample_anon_6chan_anon_discrete_y0_120_160_200',\n",
       " 'pct_start': 0.3,\n",
       " 'div_factor': 25.0,\n",
       " 'aug': 'augmix'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_to_flat_dict(train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dls, train_params, df_fn=None):\n",
    "        '''\n",
    "        could wrap the dataset parameters\n",
    "        '''\n",
    "        assert df_fn is not None, 'please specify results csv filename'\n",
    "  \n",
    "     \n",
    "        learn = run_training(dls, **train_params)\n",
    "\n",
    "        rec_dict = get_recorder_dict(learn.recorder)\n",
    "        df_dict = dict()\n",
    "        #     param_values = [arch.__name__, n_epochs, bs, seed, max_lr, wd, loss_fn_name, alpha, *[m.__name__ for m in metrics], N, magnitude]\n",
    "        #     param_key = ['architecture', 'n_epochs', 'bs', 'seed', 'max_lr', 'wd', 'loss_fn', 'alpha', *[f'metric_{i}' for i,_ in enumerate(metrics)], 'N', 'magnitude']\n",
    "        #     df_dict.update(train_params)\n",
    "        df_dict.update(_to_flat_dict(train_params))\n",
    "        df_dict.update(rec_dict)\n",
    "        df_dict['Timestamp'] = str(datetime.now())\n",
    "        ## store prediction in a separate file as tensors, but add filename\n",
    "#         self._save_preds(test=True)\n",
    "#         if self.save_model: self._save_model()\n",
    "        _write_results(pd.DataFrame([df_dict], index=[0]), df_fn)\n",
    "        return df_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fn_test = Path('~/coding/python/betting/experiments/test_results.csv').expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(df_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>max_lr</th>\n",
       "      <th>wd</th>\n",
       "      <th>loss_fn_name</th>\n",
       "      <th>alpha</th>\n",
       "      <th>metric_0</th>\n",
       "      <th>N</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>bs</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_start</th>\n",
       "      <th>div_factor</th>\n",
       "      <th>aug</th>\n",
       "      <th>trn_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>trn_loss_min</th>\n",
       "      <th>val_loss_min</th>\n",
       "      <th>unweighted_profit_0_value</th>\n",
       "      <th>unweighted_profit_0_max</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.128333</td>\n",
       "      <td>-0.711629</td>\n",
       "      <td>-0.133325</td>\n",
       "      <td>-0.738477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.912173</td>\n",
       "      <td>2020-05-21 14:36:42.977545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.181375</td>\n",
       "      <td>-0.683619</td>\n",
       "      <td>-0.181375</td>\n",
       "      <td>-0.740593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.912173</td>\n",
       "      <td>2020-05-21 14:38:17.612064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.181424</td>\n",
       "      <td>-0.655837</td>\n",
       "      <td>-0.181424</td>\n",
       "      <td>-0.730417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.912173</td>\n",
       "      <td>2020-05-21 16:19:49.366239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.204159</td>\n",
       "      <td>-0.671866</td>\n",
       "      <td>-0.204159</td>\n",
       "      <td>-0.711356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020-05-21 16:20:01.197565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.282855</td>\n",
       "      <td>-0.568804</td>\n",
       "      <td>-0.282855</td>\n",
       "      <td>-0.694458</td>\n",
       "      <td>0.044329</td>\n",
       "      <td>0.044329</td>\n",
       "      <td>2020-05-21 16:20:12.393597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.222047</td>\n",
       "      <td>-0.438311</td>\n",
       "      <td>-0.222047</td>\n",
       "      <td>-0.443605</td>\n",
       "      <td>-0.107899</td>\n",
       "      <td>0.143756</td>\n",
       "      <td>2020-05-22 08:48:22.384761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.327031</td>\n",
       "      <td>-0.427454</td>\n",
       "      <td>-0.327031</td>\n",
       "      <td>-0.453769</td>\n",
       "      <td>0.055171</td>\n",
       "      <td>0.270344</td>\n",
       "      <td>2020-05-22 08:48:28.630306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.203794</td>\n",
       "      <td>-0.443539</td>\n",
       "      <td>-0.203794</td>\n",
       "      <td>-0.443539</td>\n",
       "      <td>-0.045778</td>\n",
       "      <td>0.040683</td>\n",
       "      <td>2020-05-22 08:48:34.883116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             arch  n_epochs   max_lr    wd loss_fn_name  alpha  \\\n",
       "0  InceptionTimeD         2  0.00001  0.03   leaky_loss    0.5   \n",
       "1  InceptionTimeD         5  0.00001  0.03   leaky_loss    0.5   \n",
       "2  InceptionTimeD         3  0.00003  0.03   leaky_loss    0.5   \n",
       "3  InceptionTimeD         3  0.00010  0.03   leaky_loss    0.5   \n",
       "4  InceptionTimeD         3  0.00030  0.03   leaky_loss    0.5   \n",
       "5  InceptionTimeD         4  0.00003  0.03   leaky_loss    0.5   \n",
       "6  InceptionTimeD         4  0.00010  0.03   leaky_loss    0.5   \n",
       "7  InceptionTimeD         4  0.00030  0.03   leaky_loss    0.5   \n",
       "\n",
       "            metric_0  N  magnitude          bs  ...  pct_start div_factor  \\\n",
       "0  unweighted_profit  3        0.4  [256, 512]  ...        0.3       25.0   \n",
       "1  unweighted_profit  3        0.4  [256, 512]  ...        0.3       25.0   \n",
       "2  unweighted_profit  3        0.4  [256, 512]  ...        0.3       25.0   \n",
       "3  unweighted_profit  3        0.4  [256, 512]  ...        0.3       25.0   \n",
       "4  unweighted_profit  3        0.4  [256, 512]  ...        0.3       25.0   \n",
       "5  unweighted_profit  3        0.4  [256, 512]  ...        0.3       25.0   \n",
       "6  unweighted_profit  3        0.4  [256, 512]  ...        0.3       25.0   \n",
       "7  unweighted_profit  3        0.4  [256, 512]  ...        0.3       25.0   \n",
       "\n",
       "      aug  trn_loss  val_loss  trn_loss_min  val_loss_min  \\\n",
       "0  augmix -0.128333 -0.711629     -0.133325     -0.738477   \n",
       "1  augmix -0.181375 -0.683619     -0.181375     -0.740593   \n",
       "2  augmix -0.181424 -0.655837     -0.181424     -0.730417   \n",
       "3  augmix -0.204159 -0.671866     -0.204159     -0.711356   \n",
       "4  augmix -0.282855 -0.568804     -0.282855     -0.694458   \n",
       "5  augmix -0.222047 -0.438311     -0.222047     -0.443605   \n",
       "6  augmix -0.327031 -0.427454     -0.327031     -0.453769   \n",
       "7  augmix -0.203794 -0.443539     -0.203794     -0.443539   \n",
       "\n",
       "   unweighted_profit_0_value  unweighted_profit_0_max  \\\n",
       "0                   0.000000                 1.912173   \n",
       "1                   0.000000                 1.912173   \n",
       "2                   0.000000                 1.912173   \n",
       "3                   0.000000                 0.000000   \n",
       "4                   0.044329                 0.044329   \n",
       "5                  -0.107899                 0.143756   \n",
       "6                   0.055171                 0.270344   \n",
       "7                  -0.045778                 0.040683   \n",
       "\n",
       "                    Timestamp  \n",
       "0  2020-05-21 14:36:42.977545  \n",
       "1  2020-05-21 14:38:17.612064  \n",
       "2  2020-05-21 16:19:49.366239  \n",
       "3  2020-05-21 16:20:01.197565  \n",
       "4  2020-05-21 16:20:12.393597  \n",
       "5  2020-05-22 08:48:22.384761  \n",
       "6  2020-05-22 08:48:28.630306  \n",
       "7  2020-05-22 08:48:34.883116  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arch': torchtools.models.InceptionTimeD,\n",
       " 'n_epochs': 5,\n",
       " 'max_lr': 1e-05,\n",
       " 'wd': 0.03,\n",
       " 'loss_fn_name': 'leaky_loss',\n",
       " 'alpha': 0.5,\n",
       " 'metrics': [<function torchtools.core.unweighted_profit(preds, y_true, threshold=0)>],\n",
       " 'N': 3,\n",
       " 'magnitude': 0.4,\n",
       " 'bs': [256, 512],\n",
       " 'seed': 1234,\n",
       " 'ds_name': 'bi_sample_anon_6chan_anon_discrete_y0_120_160_200',\n",
       " 'pct_start': 0.3,\n",
       " 'div_factor': 25.0,\n",
       " 'aug': 'augmix'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _dict_product(params):\n",
    "            values = list(itertools.product(*params.values()))\n",
    "            return [dict(zip(params.keys(), values[i])) for i in range(len(values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'ds_fn': 'bi_sample_anon.csv',\n",
       "             'ds_path': Path('data/custom'),\n",
       "             'trn_end': 120000,\n",
       "             'val_end': 160000,\n",
       "             'splits': ((#8000) [0,1,2,3,4,5,6,7,8,9...],\n",
       "              (#2000) [8000,8001,8002,8003,8004,8005,8006,8007,8008,8009...]),\n",
       "             'col_config': '6chan_anon_discrete',\n",
       "             'cols_c': [['x0_0',\n",
       "               'x0_1',\n",
       "               'x0_2',\n",
       "               'x0_3',\n",
       "               'x0_4',\n",
       "               'x0_5',\n",
       "               'x0_6',\n",
       "               'x0_7',\n",
       "               'x0_8',\n",
       "               'x0_9'],\n",
       "              ['x1_0',\n",
       "               'x1_1',\n",
       "               'x1_2',\n",
       "               'x1_3',\n",
       "               'x1_4',\n",
       "               'x1_5',\n",
       "               'x1_6',\n",
       "               'x1_7',\n",
       "               'x1_8',\n",
       "               'x1_9'],\n",
       "              ['x3_0',\n",
       "               'x3_1',\n",
       "               'x3_2',\n",
       "               'x3_3',\n",
       "               'x3_4',\n",
       "               'x3_5',\n",
       "               'x3_6',\n",
       "               'x3_7',\n",
       "               'x3_8',\n",
       "               'x3_9'],\n",
       "              ['x4_0',\n",
       "               'x4_1',\n",
       "               'x4_2',\n",
       "               'x4_3',\n",
       "               'x4_4',\n",
       "               'x4_5',\n",
       "               'x4_6',\n",
       "               'x4_7',\n",
       "               'x4_8',\n",
       "               'x4_9']],\n",
       "             'cols_d': [['x2_0',\n",
       "               'x2_1',\n",
       "               'x2_2',\n",
       "               'x2_3',\n",
       "               'x2_4',\n",
       "               'x2_5',\n",
       "               'x2_6',\n",
       "               'x2_7',\n",
       "               'x2_8',\n",
       "               'x2_9'],\n",
       "              ['x5_0',\n",
       "               'x5_1',\n",
       "               'x5_2',\n",
       "               'x5_3',\n",
       "               'x5_4',\n",
       "               'x5_5',\n",
       "               'x5_6',\n",
       "               'x5_7',\n",
       "               'x5_8',\n",
       "               'x5_9']],\n",
       "             'cols_y': 'y0',\n",
       "             'ds_full_path': Path('data/custom/bi_sample_anon.csv'),\n",
       "             'dataset_name': 'bi_sample_anon_6chan_anon_discrete_y0_120_160_200',\n",
       "             'dsds': None})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_preds_fn(prefix='val'):\n",
    "    return f'{prefix}_preds_{abs(hash(datetime.utcnow()))}.pt'\n",
    "\n",
    "def _get_model_fn(prefix='model'):\n",
    "    return f'{prefix}_{abs(hash(datetime.utcnow()))}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TSExperiments:\n",
    "    '''\n",
    "    Wrapper class for TS modelling experiments\n",
    "    '''\n",
    "    def __init__(self, train_params, save_model=False, preds_path=None, model_path=None, results_path=None):\n",
    "        \n",
    "        self.train_params = train_params #training params can change, e.g. when running grid search\n",
    "        self.save_model = save_model## models are big\n",
    "        self.preds_path = ifnone(preds_path, './experiments/preds')\n",
    "        self.model_path = ifnone(model_path, './experiments/models')\n",
    "        self.results_path = ifnone(results_path, './experiments/results')\n",
    "        \n",
    "    def setup_data(self, df_base, data_params):\n",
    "        cols_c, cols_d, cols_y, splits = map(data_params.get, ['cols_c', 'cols_d', 'cols_y', 'splits'])\n",
    "        \n",
    "        self.bs = self.train_params['bs']\n",
    "        self.splits = splits\n",
    "        self.dls = get_dls(df_base, cols_c, cols_y, splits, cols_d=cols_d, bs=self.bs)    \n",
    " \n",
    "    def _save_preds(self, test=False):\n",
    "    #         val_preds_fn = _get_preds_fn()\n",
    "        preds_fn = _get_preds_fn()\n",
    "        preds, y_true = self.learn.get_preds(1)\n",
    "        torch.save(preds, Path(self.preds_path)/preds_fn)\n",
    "        self.df_dict.update({'val_preds':preds_fn})\n",
    "        if len(list(self.dls))==3:\n",
    "            preds_fn = _get_preds_fn('test')\n",
    "            preds, y_true = self.learn.get_preds(2)\n",
    "            torch.save(preds, Path(self.preds_path)/preds_fn)\n",
    "            self.df_dict.update({'test_preds':preds_fn})\n",
    "    \n",
    "    def _save_model(self):\n",
    "    #         val_preds_fn = _get_preds_fn()\n",
    "        model_fn = _get_model_fn()\n",
    "        self.learn.save(model_fn)\n",
    "        self.df_dict.update({'model_fn':f'{self.learn.model_dir}/{model_fn}.pth'})\n",
    "\n",
    "    \n",
    "    \n",
    "    def run_training(self, arch=None, seed=1234, n_epochs=None, max_lr=None, wd=None, \n",
    "                     loss_fn_name=None, alpha=None, metrics=unweighted_profit, \n",
    "                     N=2, magnitude=0.1, pct_start=0.3, div_factor=25.0, aug='randaugment', **kwargs):\n",
    "        # model = ResNetSig(db.features, db.c).to(device)\n",
    "        '''\n",
    "        run a training cycle\n",
    "        parameterization important for keeping track\n",
    "        '''\n",
    "        assert loss_fn_name and n_epochs, 'must pass loss_fn_name, and n_epochs'\n",
    "\n",
    "        print(f'pct_start: {pct_start} div_factor: {div_factor}')\n",
    "        set_seed(seed)\n",
    "    #     model = arch(db.features, db.c)\n",
    "        model = arch(6,1)\n",
    "\n",
    "        _remove_augs(self.dls)\n",
    "        augs = RandAugment(N=N, magnitude=magnitude, verbose=True) if aug=='randaugment' else Augmix(\n",
    "            N=N, magnitude=magnitude, verbose=True)\n",
    "    #     augs  = Augmix(verbose=True)\n",
    "        self.dls.after_batch.add(augs)       \n",
    "        loss_fn = get_loss_fn(loss_fn_name, alpha=alpha)\n",
    "        print(loss_fn)\n",
    "        learn = Learner(self.dls, model, loss_func=loss_fn, metrics=metrics, model_dir=self.model_path)\n",
    "\n",
    "        learn.fit_one_cycle(n_epochs, max_lr, wd=wd, pct_start=pct_start, div_factor=div_factor)\n",
    "    #     learn.recorder.plot_losses()\n",
    "    #     learn.recorder.plot_metrics()\n",
    "        return learn\n",
    "\n",
    "\n",
    "    def run_experiment(self, df_fn=None):\n",
    "        '''\n",
    "        could wrap the dataset parameters\n",
    "        '''\n",
    "        assert df_fn is not None, 'please specify results csv filename'\n",
    "  \n",
    "        self.learn = self.run_training(**self.train_params)\n",
    "#         rec_dict = get_recorder_dict(self.learn.recorder)\n",
    "        self.df_dict = dict()\n",
    "        self.df_dict.update(_to_flat_dict(train_params))\n",
    "        self.df_dict.update(get_recorder_dict(self.learn.recorder))\n",
    "        self.df_dict['Timestamp'] = str(datetime.now())\n",
    "        ## store prediction in a separate file as tensors, but add filename\n",
    "        self._save_preds(test=False)\n",
    "        if self.save_model: self._save_model()\n",
    "        _write_results(pd.DataFrame([self.df_dict], index=[0]), Path(self.results_path)/df_fn)\n",
    "#         return df_dict\n",
    "    \n",
    "    \n",
    "   \n",
    "    def run_grid_search(self, hypers:dict, df_results_fn=None):\n",
    "        '''\n",
    "        run hyper parameter grid search, note that this changes self.train_params\n",
    "        '''\n",
    "        hyper_configs = _dict_product(hypers) #list of dictionaries\n",
    "        if hasattr(self, 'hyper_configs'): self.hyper_configs+=hyper_configs\n",
    "        else: self.hyper_configs = hyper_configs\n",
    "        for config in hyper_configs:\n",
    "            self.train_params.update(config)\n",
    "            print(self.train_params)\n",
    "            self.run_experiment(df_fn=df_results_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_experiment = TSExperiments(train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params['splits'] = splits_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4, 10) (10000, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "ts_experiment.setup_data(df_small, data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {'max_lr':[3e-5, 1e-4, 3e-4], 'n_epochs':[7], 'N':[3], 'magnitude':[0.4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arch': torchtools.models.InceptionTimeD,\n",
       " 'n_epochs': 7,\n",
       " 'max_lr': 0.0003,\n",
       " 'wd': 0.03,\n",
       " 'loss_fn_name': 'leaky_loss',\n",
       " 'alpha': 0.5,\n",
       " 'metrics': [<function torchtools.core.unweighted_profit(preds, y_true, threshold=0)>],\n",
       " 'N': 3,\n",
       " 'magnitude': 0.4,\n",
       " 'bs': [256, 512],\n",
       " 'seed': 1234,\n",
       " 'ds_name': 'bi_sample_anon_6chan_anon_discrete_y0_120_160_200',\n",
       " 'pct_start': 0.3,\n",
       " 'div_factor': 25.0,\n",
       " 'aug': 'augmix'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arch': <class 'torchtools.models.InceptionTimeD'>, 'n_epochs': 7, 'max_lr': 3e-05, 'wd': 0.03, 'loss_fn_name': 'leaky_loss', 'alpha': 0.5, 'metrics': [<function unweighted_profit at 0x7fa0150f7d40>], 'N': 3, 'magnitude': 0.4, 'bs': [256, 512], 'seed': 1234, 'ds_name': 'bi_sample_anon_6chan_anon_discrete_y0_120_160_200', 'pct_start': 0.3, 'div_factor': 25.0, 'aug': 'augmix'}\n",
      "pct_start: 0.3 div_factor: 25.0\n",
      "[TSStandardize: (TSTensor,object) -> encodes\n",
      "(NumpyTensor,object) -> encodes ]\n",
      "Pipeline: TSStandardize Pipeline: TSStandardize\n",
      "tfms None\n",
      "functools.partial(<function leaky_loss at 0x7fa0150f7830>, alpha=0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>unweighted_profit</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.061447</td>\n",
       "      <td>-0.253833</td>\n",
       "      <td>0.193756</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.138268</td>\n",
       "      <td>-0.512078</td>\n",
       "      <td>-0.034383</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.173151</td>\n",
       "      <td>-0.464161</td>\n",
       "      <td>-0.178399</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.204135</td>\n",
       "      <td>-0.478039</td>\n",
       "      <td>-0.178399</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.268758</td>\n",
       "      <td>-0.478676</td>\n",
       "      <td>-0.157899</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.351980</td>\n",
       "      <td>-0.457422</td>\n",
       "      <td>-0.157899</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-0.336728</td>\n",
       "      <td>-0.456215</td>\n",
       "      <td>-0.157899</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not new\n",
      "{'arch': <class 'torchtools.models.InceptionTimeD'>, 'n_epochs': 7, 'max_lr': 0.0001, 'wd': 0.03, 'loss_fn_name': 'leaky_loss', 'alpha': 0.5, 'metrics': [<function unweighted_profit at 0x7fa0150f7d40>], 'N': 3, 'magnitude': 0.4, 'bs': [256, 512], 'seed': 1234, 'ds_name': 'bi_sample_anon_6chan_anon_discrete_y0_120_160_200', 'pct_start': 0.3, 'div_factor': 25.0, 'aug': 'augmix'}\n",
      "pct_start: 0.3 div_factor: 25.0\n",
      "[TSStandardize: (TSTensor,object) -> encodes\n",
      "(NumpyTensor,object) -> encodes ]\n",
      "Pipeline: TSStandardize Pipeline: TSStandardize\n",
      "tfms None\n",
      "functools.partial(<function leaky_loss at 0x7fa0150f7830>, alpha=0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>unweighted_profit</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.114760</td>\n",
       "      <td>-0.213118</td>\n",
       "      <td>0.253433</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.197599</td>\n",
       "      <td>-0.503396</td>\n",
       "      <td>-0.177498</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.293514</td>\n",
       "      <td>-0.422551</td>\n",
       "      <td>-0.006572</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.347330</td>\n",
       "      <td>-0.424305</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.381413</td>\n",
       "      <td>-0.524226</td>\n",
       "      <td>-0.103994</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.458140</td>\n",
       "      <td>-0.443198</td>\n",
       "      <td>0.119664</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-0.487051</td>\n",
       "      <td>-0.410117</td>\n",
       "      <td>0.120056</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not new\n",
      "{'arch': <class 'torchtools.models.InceptionTimeD'>, 'n_epochs': 7, 'max_lr': 0.0003, 'wd': 0.03, 'loss_fn_name': 'leaky_loss', 'alpha': 0.5, 'metrics': [<function unweighted_profit at 0x7fa0150f7d40>], 'N': 3, 'magnitude': 0.4, 'bs': [256, 512], 'seed': 1234, 'ds_name': 'bi_sample_anon_6chan_anon_discrete_y0_120_160_200', 'pct_start': 0.3, 'div_factor': 25.0, 'aug': 'augmix'}\n",
      "pct_start: 0.3 div_factor: 25.0\n",
      "[TSStandardize: (TSTensor,object) -> encodes\n",
      "(NumpyTensor,object) -> encodes ]\n",
      "Pipeline: TSStandardize Pipeline: TSStandardize\n",
      "tfms None\n",
      "functools.partial(<function leaky_loss at 0x7fa0150f7830>, alpha=0.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>unweighted_profit</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.100034</td>\n",
       "      <td>-0.314245</td>\n",
       "      <td>0.193756</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.264312</td>\n",
       "      <td>-0.551215</td>\n",
       "      <td>-0.416235</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.355112</td>\n",
       "      <td>-0.438786</td>\n",
       "      <td>0.075726</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.339290</td>\n",
       "      <td>-0.234057</td>\n",
       "      <td>0.053099</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.427105</td>\n",
       "      <td>-0.291003</td>\n",
       "      <td>0.130943</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.521998</td>\n",
       "      <td>-0.219214</td>\n",
       "      <td>0.025726</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-0.614366</td>\n",
       "      <td>-0.186999</td>\n",
       "      <td>0.130943</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not new\n"
     ]
    }
   ],
   "source": [
    "ts_experiment.run_grid_search(hypers, df_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(df_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arch</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>max_lr</th>\n",
       "      <th>wd</th>\n",
       "      <th>loss_fn_name</th>\n",
       "      <th>alpha</th>\n",
       "      <th>metric_0</th>\n",
       "      <th>N</th>\n",
       "      <th>magnitude</th>\n",
       "      <th>bs</th>\n",
       "      <th>seed</th>\n",
       "      <th>ds_name</th>\n",
       "      <th>pct_start</th>\n",
       "      <th>div_factor</th>\n",
       "      <th>aug</th>\n",
       "      <th>trn_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>trn_loss_min</th>\n",
       "      <th>val_loss_min</th>\n",
       "      <th>unweighted_profit_0_value</th>\n",
       "      <th>unweighted_profit_0_max</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>1234</td>\n",
       "      <td>bi_sample_6chan_anon_discrete_y0_120_160_200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.128333</td>\n",
       "      <td>-0.711629</td>\n",
       "      <td>-0.133325</td>\n",
       "      <td>-0.738477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.912173</td>\n",
       "      <td>2020-05-21 14:36:42.977545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>1234</td>\n",
       "      <td>bi_sample_6chan_anon_discrete_y0_120_160_200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.181375</td>\n",
       "      <td>-0.683619</td>\n",
       "      <td>-0.181375</td>\n",
       "      <td>-0.740593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.912173</td>\n",
       "      <td>2020-05-21 14:38:17.612064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>1234</td>\n",
       "      <td>bi_sample_6chan_anon_discrete_y0_120_160_200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.181424</td>\n",
       "      <td>-0.655837</td>\n",
       "      <td>-0.181424</td>\n",
       "      <td>-0.730417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.912173</td>\n",
       "      <td>2020-05-21 16:19:49.366239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>1234</td>\n",
       "      <td>bi_sample_6chan_anon_discrete_y0_120_160_200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.204159</td>\n",
       "      <td>-0.671866</td>\n",
       "      <td>-0.204159</td>\n",
       "      <td>-0.711356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020-05-21 16:20:01.197565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>1234</td>\n",
       "      <td>bi_sample_6chan_anon_discrete_y0_120_160_200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.282855</td>\n",
       "      <td>-0.568804</td>\n",
       "      <td>-0.282855</td>\n",
       "      <td>-0.694458</td>\n",
       "      <td>0.044329</td>\n",
       "      <td>0.044329</td>\n",
       "      <td>2020-05-21 16:20:12.393597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>1234</td>\n",
       "      <td>bi_sample_6chan_anon_discrete_y0_120_160_200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.222047</td>\n",
       "      <td>-0.438311</td>\n",
       "      <td>-0.222047</td>\n",
       "      <td>-0.443605</td>\n",
       "      <td>-0.107899</td>\n",
       "      <td>0.143756</td>\n",
       "      <td>2020-05-22 08:48:22.384761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>1234</td>\n",
       "      <td>bi_sample_6chan_anon_discrete_y0_120_160_200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.327031</td>\n",
       "      <td>-0.427454</td>\n",
       "      <td>-0.327031</td>\n",
       "      <td>-0.453769</td>\n",
       "      <td>0.055171</td>\n",
       "      <td>0.270344</td>\n",
       "      <td>2020-05-22 08:48:28.630306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>1234</td>\n",
       "      <td>bi_sample_6chan_anon_discrete_y0_120_160_200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.203794</td>\n",
       "      <td>-0.443539</td>\n",
       "      <td>-0.203794</td>\n",
       "      <td>-0.443539</td>\n",
       "      <td>-0.045778</td>\n",
       "      <td>0.040683</td>\n",
       "      <td>2020-05-22 08:48:34.883116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>1234</td>\n",
       "      <td>bi_sample_anon_6chan_anon_discrete_y0_120_160_200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.336728</td>\n",
       "      <td>-0.456215</td>\n",
       "      <td>-0.351980</td>\n",
       "      <td>-0.512078</td>\n",
       "      <td>-0.157899</td>\n",
       "      <td>0.193756</td>\n",
       "      <td>2020-05-22 09:45:35.273860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>1234</td>\n",
       "      <td>bi_sample_anon_6chan_anon_discrete_y0_120_160_200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.487051</td>\n",
       "      <td>-0.410117</td>\n",
       "      <td>-0.487051</td>\n",
       "      <td>-0.524226</td>\n",
       "      <td>0.120056</td>\n",
       "      <td>0.253433</td>\n",
       "      <td>2020-05-22 09:45:46.067508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>InceptionTimeD</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00030</td>\n",
       "      <td>0.03</td>\n",
       "      <td>leaky_loss</td>\n",
       "      <td>0.5</td>\n",
       "      <td>unweighted_profit</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[256, 512]</td>\n",
       "      <td>1234</td>\n",
       "      <td>bi_sample_anon_6chan_anon_discrete_y0_120_160_200</td>\n",
       "      <td>0.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>augmix</td>\n",
       "      <td>-0.614366</td>\n",
       "      <td>-0.186999</td>\n",
       "      <td>-0.614366</td>\n",
       "      <td>-0.551215</td>\n",
       "      <td>0.130943</td>\n",
       "      <td>0.193756</td>\n",
       "      <td>2020-05-22 09:45:56.925092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              arch  n_epochs   max_lr    wd loss_fn_name  alpha  \\\n",
       "0   InceptionTimeD         2  0.00001  0.03   leaky_loss    0.5   \n",
       "1   InceptionTimeD         5  0.00001  0.03   leaky_loss    0.5   \n",
       "2   InceptionTimeD         3  0.00003  0.03   leaky_loss    0.5   \n",
       "3   InceptionTimeD         3  0.00010  0.03   leaky_loss    0.5   \n",
       "4   InceptionTimeD         3  0.00030  0.03   leaky_loss    0.5   \n",
       "5   InceptionTimeD         4  0.00003  0.03   leaky_loss    0.5   \n",
       "6   InceptionTimeD         4  0.00010  0.03   leaky_loss    0.5   \n",
       "7   InceptionTimeD         4  0.00030  0.03   leaky_loss    0.5   \n",
       "8   InceptionTimeD         7  0.00003  0.03   leaky_loss    0.5   \n",
       "9   InceptionTimeD         7  0.00010  0.03   leaky_loss    0.5   \n",
       "10  InceptionTimeD         7  0.00030  0.03   leaky_loss    0.5   \n",
       "\n",
       "             metric_0  N  magnitude          bs  seed  \\\n",
       "0   unweighted_profit  3        0.4  [256, 512]  1234   \n",
       "1   unweighted_profit  3        0.4  [256, 512]  1234   \n",
       "2   unweighted_profit  3        0.4  [256, 512]  1234   \n",
       "3   unweighted_profit  3        0.4  [256, 512]  1234   \n",
       "4   unweighted_profit  3        0.4  [256, 512]  1234   \n",
       "5   unweighted_profit  3        0.4  [256, 512]  1234   \n",
       "6   unweighted_profit  3        0.4  [256, 512]  1234   \n",
       "7   unweighted_profit  3        0.4  [256, 512]  1234   \n",
       "8   unweighted_profit  3        0.4  [256, 512]  1234   \n",
       "9   unweighted_profit  3        0.4  [256, 512]  1234   \n",
       "10  unweighted_profit  3        0.4  [256, 512]  1234   \n",
       "\n",
       "                                              ds_name  pct_start  div_factor  \\\n",
       "0        bi_sample_6chan_anon_discrete_y0_120_160_200        0.3        25.0   \n",
       "1        bi_sample_6chan_anon_discrete_y0_120_160_200        0.3        25.0   \n",
       "2        bi_sample_6chan_anon_discrete_y0_120_160_200        0.3        25.0   \n",
       "3        bi_sample_6chan_anon_discrete_y0_120_160_200        0.3        25.0   \n",
       "4        bi_sample_6chan_anon_discrete_y0_120_160_200        0.3        25.0   \n",
       "5        bi_sample_6chan_anon_discrete_y0_120_160_200        0.3        25.0   \n",
       "6        bi_sample_6chan_anon_discrete_y0_120_160_200        0.3        25.0   \n",
       "7        bi_sample_6chan_anon_discrete_y0_120_160_200        0.3        25.0   \n",
       "8   bi_sample_anon_6chan_anon_discrete_y0_120_160_200        0.3        25.0   \n",
       "9   bi_sample_anon_6chan_anon_discrete_y0_120_160_200        0.3        25.0   \n",
       "10  bi_sample_anon_6chan_anon_discrete_y0_120_160_200        0.3        25.0   \n",
       "\n",
       "       aug  trn_loss  val_loss  trn_loss_min  val_loss_min  \\\n",
       "0   augmix -0.128333 -0.711629     -0.133325     -0.738477   \n",
       "1   augmix -0.181375 -0.683619     -0.181375     -0.740593   \n",
       "2   augmix -0.181424 -0.655837     -0.181424     -0.730417   \n",
       "3   augmix -0.204159 -0.671866     -0.204159     -0.711356   \n",
       "4   augmix -0.282855 -0.568804     -0.282855     -0.694458   \n",
       "5   augmix -0.222047 -0.438311     -0.222047     -0.443605   \n",
       "6   augmix -0.327031 -0.427454     -0.327031     -0.453769   \n",
       "7   augmix -0.203794 -0.443539     -0.203794     -0.443539   \n",
       "8   augmix -0.336728 -0.456215     -0.351980     -0.512078   \n",
       "9   augmix -0.487051 -0.410117     -0.487051     -0.524226   \n",
       "10  augmix -0.614366 -0.186999     -0.614366     -0.551215   \n",
       "\n",
       "    unweighted_profit_0_value  unweighted_profit_0_max  \\\n",
       "0                    0.000000                 1.912173   \n",
       "1                    0.000000                 1.912173   \n",
       "2                    0.000000                 1.912173   \n",
       "3                    0.000000                 0.000000   \n",
       "4                    0.044329                 0.044329   \n",
       "5                   -0.107899                 0.143756   \n",
       "6                    0.055171                 0.270344   \n",
       "7                   -0.045778                 0.040683   \n",
       "8                   -0.157899                 0.193756   \n",
       "9                    0.120056                 0.253433   \n",
       "10                   0.130943                 0.193756   \n",
       "\n",
       "                     Timestamp  \n",
       "0   2020-05-21 14:36:42.977545  \n",
       "1   2020-05-21 14:38:17.612064  \n",
       "2   2020-05-21 16:19:49.366239  \n",
       "3   2020-05-21 16:20:01.197565  \n",
       "4   2020-05-21 16:20:12.393597  \n",
       "5   2020-05-22 08:48:22.384761  \n",
       "6   2020-05-22 08:48:28.630306  \n",
       "7   2020-05-22 08:48:34.883116  \n",
       "8   2020-05-22 09:45:35.273860  \n",
       "9   2020-05-22 09:45:46.067508  \n",
       "10  2020-05-22 09:45:56.925092  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 99\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nbdev]",
   "language": "python",
   "name": "conda-env-nbdev-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
