{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "> refactor modelling, experiment functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torchtools.core import *\n",
    "from torchtools.data import *\n",
    "from torchtools.models import *\n",
    "from torchtools.datasets import *\n",
    "from torchtools.augmentations import *\n",
    "#from torchtools.datablock import *\n",
    "from torchtools.dataloader import *\n",
    "from torchtools.configs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai.basics import *\n",
    "#from fast_tabnet.core import *\n",
    "from fastcore.script import *\n",
    "from fastai.callback.tracker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.models.InceptionTimePlus import *\n",
    "from tsai.models.TSTPlus import *\n",
    "from tsai.models.utils import build_ts_model, transfer_weights\n",
    "import tsai.data.transforms as tsai_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsai_ra = tsai_tfms.RandAugment(tsai_tfms.all_TS_randaugs, N=1, M=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worklflow with Discret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "## data config\n",
    "df_fn = 'bi_sample_anon.csv'\n",
    "df_dir = Path('./data/custom')\n",
    "df_path = Path(Path(df_dir)/df_fn)\n",
    "\n",
    "trn_end = 120000\n",
    "val_end = 160000\n",
    "test_end = 200000\n",
    "splits = (L(range(trn_end)), L(range(trn_end, val_end)))\n",
    "df_config = f'{int(trn_end/1000)}_{int(val_end/1000)}_{int(test_end/1000)}'\n",
    "\n",
    "col_config = '6chan_anon_discrete'\n",
    "cols_c, cols_d, cols_y, n_train = get_discrete_config()\n",
    "\n",
    "df_source = Path(df_fn).stem\n",
    "\n",
    "\n",
    "dataset_name = f'{df_source}_{col_config}_{cols_y}_{df_config}'\n",
    "data_params = defaultdict(lambda:None, {'df_fn':df_fn, 'df_dir':df_dir, 'df_path':df_path, 'trn_end':trn_end, \n",
    "                                        'val_end':val_end, 'splits':splits, 'col_config_id':col_config, \n",
    "                                        'cols_c':cols_c, 'cols_d':cols_d, 'cols_y':cols_y, 'ds_id':dataset_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows=200000\n",
    "df_main = pd.read_csv(data_params['df_path'], nrows=nrows)\n",
    "\n",
    "# cols_c, cols_d, cols_y, n_train = get_discrete_config()\n",
    "# splits = TSSplitter()(df_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df_main.iloc[:50000]\n",
    "splits_small = TSSplitter()(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items_from_df(df_small, cols_c, cols_y, n_train, cols_d=cols_d)\n",
    "Xc,Xd,y = items_to_arrays(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items_from_df(df_small, cols_c, cols_y, n_train)\n",
    "Xc,y = items_to_arrays(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_test = L(L(range(7000)), L(range(7000,9000)), L(range(9000,10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp Exploration Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc.shape, y.shape, Xd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " dsets = TSDatasets3(X=Xc, X_dis=None, y=y, splits=splits_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,verbose,shuffle_train=64,False,True\n",
    "ss = TSStandardize(by_var=True, verbose=verbose)\n",
    "#     augmix = AugmixSS()\n",
    "ds = [dsets.subset(i) for i in range(dsets.n_subsets)]\n",
    "dls = TSDataLoaders.from_dsets(*ds, bs=[bs]+[bs]*len(splits), batch_tfms=[ss], shuffle_train=shuffle_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xc,y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = get_loss_fn('leaky_loss', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1234)\n",
    "dls.train.rng = random.Random(random.randint(0,2**32-1))\n",
    "# bs,verbose,shuffle_train=64,False,True\n",
    "# ss = TSStandardize(by_var=True, verbose=verbose)\n",
    "# #     augmix = AugmixSS()\n",
    "# ds = [dsets.subset(i) for i in range(dsets.n_subsets)]\n",
    "# dls = TSDataLoaders.from_dsets(*ds, bs=[bs]+[bs]*len(splits), batch_tfms=[ss], shuffle_train=shuffle_train)\n",
    "learn = Learner(dls, InceptionTimeSgm(4,1), wd=0.001, loss_func=loss_fn)\n",
    "learn.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1234)\n",
    "dls.train.rng = random.Random(random.randint(0,2**32-1))\n",
    "# bs,verbose,shuffle_train=64,False,True\n",
    "# ss = TSStandardize(by_var=True, verbose=verbose)\n",
    "# #     augmix = AugmixSS()\n",
    "# ds = [dsets.subset(i) for i in range(dsets.n_subsets)]\n",
    "# dls = TSDataLoaders.from_dsets(*ds, bs=[bs]+[bs]*len(splits), batch_tfms=[ss], shuffle_train=shuffle_train)\n",
    "learn = Learner(dls, InceptionTimeSgm(4,1), wd=0.001, loss_func=loss_fn)\n",
    "learn.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1234)\n",
    "dls.train.rng = random.Random(random.randint(0,2**32-1))\n",
    "# bs,verbose,shuffle_train=64,False,True\n",
    "# ss = TSStandardize(by_var=True, verbose=verbose)\n",
    "# #     augmix = AugmixSS()\n",
    "# ds = [dsets.subset(i) for i in range(dsets.n_subsets)]\n",
    "# dls = TSDataLoaders.from_dsets(*ds, bs=[bs]+[bs]*len(splits), batch_tfms=[ss], shuffle_train=shuffle_train)\n",
    "learn = Learner(dls, InceptionTimeSgm(4,1), wd=0.1, loss_func=loss_fn)\n",
    "learn.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1234)\n",
    "dls.train.rng = random.Random(random.randint(0,2**32-1))\n",
    "# bs,verbose,shuffle_train=64,False,True\n",
    "# ss = TSStandardize(by_var=True, verbose=verbose)\n",
    "# #     augmix = AugmixSS()\n",
    "# ds = [dsets.subset(i) for i in range(dsets.n_subsets)]\n",
    "# dls = TSDataLoaders.from_dsets(*ds, bs=[bs]+[bs]*len(splits), batch_tfms=[ss], shuffle_train=shuffle_train)\n",
    "learn = Learner(dls, InceptionTimeSgm(4,1), wd=0.001, loss_func=loss_fn)\n",
    "learn.fit_one_cycle(3, wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def emb_sz_rule(n_cat):\n",
    "    \"Rule of thumb to pick embedding size corresponding to `n_cat`\"\n",
    "    return min(600, round(1.6 * n_cat**0.56))\n",
    "\n",
    "def _one_emb_sz(classes, n, sz_dict=None):\n",
    "    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n",
    "    sz_dict = ifnone(sz_dict, {})\n",
    "    n_cat = len(classes[n])\n",
    "    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n",
    "    return n_cat,sz\n",
    "\n",
    "def get_emb_sz(to, sz_dict=None):\n",
    "    \"Get default embedding size from `TabularPreprocessor` `proc` or the ones in `sz_dict`\"\n",
    "    return [_one_emb_sz(to.classes, n, sz_dict) for n in to.cat_names]\n",
    "\n",
    "def get_mod(dls, arch='inception', dropout=None, fc_dropout=None, pretrained=None):\n",
    "    '''\n",
    "    architectures:\n",
    "    - inception\n",
    "    - transformer\n",
    "    - tst\n",
    "    - inception_gb, transformer_gb\n",
    "    - transformer_dl (for double loss, preds 1d, y 2d)\n",
    "    - pretrained: specify pretrained model path, make sure it corresponds to the chosen architecture\n",
    "    '''\n",
    "    if dls.classification and not dls.mixed:\n",
    "        model = InceptionTime(dls.n_channels, dls.c)\n",
    "    \n",
    "    elif arch=='inception_gb': #hack, works only for continuous channels and 1 target\n",
    "        model = InceptionTime(dls.n_channels, 2)\n",
    "    \n",
    "    elif arch=='transformer_gb': #hack, works only for continuous channels and 1 target\n",
    "        model = TST(dls.n_channels, 2, 10)\n",
    "    \n",
    "    elif arch=='transformer_dl': #hack, works only for continuous channels and exactly 2 targets with double_loss\n",
    "        #return TST(dls.n_channels, 1, 10):\n",
    "        model = TSTPlus(dls.n_channels, 1, seq_len=10, res_dropout=dropout, fc_dropout=fc_dropout, y_range=(-1,1))\n",
    "    \n",
    "    elif dls.n_channels==0:\n",
    "        assert dls.cols_cat is not None or dls.cols_cont is not None, 'no tabular columns'\n",
    "        emb_szs= [_one_emb_sz(dls.voc, c) for c in listify(dls.cols_cat)] \n",
    "        model = TabNetTT(emb_szs=emb_szs, n_cont=len(dls.cols_cont), out_sz=dls.n_targets)\n",
    "    \n",
    "    elif dls.mixed:\n",
    "        emb_szs= [_one_emb_sz(dls.voc, c) for c in listify(dls.cols_cat)] \n",
    "        \n",
    "        if dls.classification:\n",
    "             model = InceptionTime_Mixed(dls.n_channels_c, dls.n_channels_d, dls.c, \n",
    "                                    len(dls.cols_cont), emb_szs=emb_szs)\n",
    "        else:\n",
    "            model = InceptionTimeD_Mixed(dls.n_channels_c, dls.n_channels_d, dls.n_targets, \n",
    "                                    len(dls.cols_cont), emb_szs=emb_szs)\n",
    "    else:\n",
    "        if dls.dataset.has_x[1]: ##discrete channels\n",
    "            if arch=='transformer':\n",
    "                model = TransformerSgmD(dls.n_channels, dls.n_targets, res_dropout=dropout)\n",
    "            else:\n",
    "                model = InceptionTimeD(dls.n_channels, dls.n_targets)\n",
    "        else:\n",
    "            if arch=='tst':\n",
    "                #return TransformerSgm(dls.n_channels, dls.n_targets, res_dropout=dropout)\n",
    "                model = TSTPlus(dls.n_channels, dls.n_targets, seq_len=10, res_dropout=dropout, y_range=(-1,1))\n",
    "            if arch=='transformer':\n",
    "                model = TransformerSgm(dls.n_channels, dls.n_targets, res_dropout=dropout)\n",
    "            else:\n",
    "                model = InceptionTimeSgm(dls.n_channels, dls.n_targets)\n",
    "                \n",
    "    if pretrained:\n",
    "        transfer_weights(model, pretrained, exclude_head=True) #this works only for tsai plus models\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dls(df, cols_c, cols_y, splits, cols_d=None, bs=64, ds_type=TSDatasets5, shuffle_train=True,\n",
    "           verbose=False, ss_dis=True, cols_cont=None, cols_cat=None, classification=False, stats=None):\n",
    "    '''\n",
    "    create dataloaders\n",
    "    handling of discrete channels with cols_d and ss_dis\n",
    "    NOTE: continuous tab cols 3d, cat tab cols 2d, legacy....\n",
    "    args:\n",
    "        stats: (means ,stds) (items_from_df expects (means, stds, medians))\n",
    "    '''\n",
    "    stats_arg = None if stats is None else (*stats, None)\n",
    "    items = items_from_df(df, cols_c, cols_y, len(splits[0]), cols_d=cols_d, tab_cols_c=cols_cont, stats=stats_arg)\n",
    "    \n",
    "    print(len(items), len(items[0]))\n",
    "    ars=items_to_arrays(items)\n",
    "    has_col=[cols_c is not None, cols_d is not None, cols_cont is not None]\n",
    "    Xc, Xd, X_conts = map_xs(ars[:-1], has_col)\n",
    "    \n",
    "    y=ars[-1].astype(np.float)\n",
    "    if classification:\n",
    "        y, y_vocab = cats_from_df(df, listify(cols_y), len(splits[0]), add_na=False)\n",
    "        y=y.squeeze()\n",
    "        y=y.astype(np.long)\n",
    " \n",
    "    if cols_cat is not None:\n",
    "        X_cats, cat_maps = cats_from_df(df, cols_cat, len(splits[0]))\n",
    "    else: X_cats, cat_maps = None, None\n",
    "\n",
    "    _ytype=TensorCategory if classification else TensorFloat\n",
    "    print(ds_type)\n",
    "    dsets = ds_type(X_c=Xc, X_d=Xd, y=y, splits=splits, X_tcont=X_conts, X_tcat=X_cats, _ytype=_ytype)\n",
    "#     dsets = ds_type(X=Xc, X_dis=Xd, y=y, splits=splits, X_tabc=X_conts, X_tabcat=X_cats, _ytype=_ytype)\n",
    "    print(dsets.n_subsets)\n",
    "\n",
    "    ##standardization: continuous channels always, discrete channels optional\n",
    "    batch_tfms=[]\n",
    "    print(has_col)\n",
    "    ###!!!!HACK!!!!!\n",
    "    if stats is not None: batch_tfms+=[TSStandardize(by_var=True, verbose=verbose).from_stats(*stats)]\n",
    "    else:\n",
    "        if has_col[0] and ss_dis: batch_tfms+=[TSStandardize(by_var=True, verbose=verbose)]\n",
    "        if has_col[1] and ss_dis: batch_tfms+=[TSStandardize(by_var=True, verbose=verbose, discrete=True)]\n",
    "#     augmix = AugmixSS()\n",
    "#     print(batch_tfms)\n",
    "#     return dsets\n",
    "    ds = [dsets.subset(i) for i in range(dsets.n_subsets)]\n",
    "#     return ds\n",
    "#     dls = TSDataLoaders.from_dsets(*ds, bs=[bs]+[bs]*len(splits), batch_tfms=batch_tfms, shuffle_train=shuffle_train)\n",
    "    dls = TSDataLoaders.from_dsets(*ds, bs=bs, batch_tfms=batch_tfms, shuffle_train=shuffle_train)\n",
    "#     dls = TSDataLoaders.from_dsets(dsets.train, bs=[128,128])\n",
    "    dls.n_channels = len(listify(cols_c)) + len(listify(cols_d))\n",
    "    dls.n_channels_c = len(listify(cols_c)) \n",
    "    dls.n_channels_d = len(listify(cols_d))\n",
    "    \n",
    "    dls.n_targets = len(listify(cols_y))\n",
    "    dls.cols_cat, dls.cols_cont = cols_cat, cols_cont\n",
    "    if cols_cat is not None:\n",
    "        dls.voc=cat_maps\n",
    "    \n",
    "    dls.mixed = dls.cols_cat is not None or dls.cols_cont is not None\n",
    "        \n",
    "    if classification:\n",
    "        dls.y_vocab=y_vocab\n",
    "        dls.c = len(dls.y_vocab[list(dls.y_vocab.keys())[0]])\n",
    "    dls.classification = True if classification else False\n",
    "    ##ToDO: for mixed input, store category info in dl\n",
    "  \n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cont=['x0_0', 'x0_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items_from_df(df_small, cols_c, cols_y, len(splits[0]), cols_d=cols_d, tab_cols_c=cols_cont)\n",
    "ars=items_to_arrays(items)\n",
    "has_col=[cols_c is not None, cols_d is not None, cols_cont is not None]\n",
    "Xc, Xd, X_conts = map_xs(ars[:-1], has_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets=TSDatasets4(X=None, X_dis=None, y=y, X_tabc=X_conts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dls = get_dls(df_small, cols_c, cols_y, splits_test, cols_d=cols_d, ds_type=TSDatasets5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSDatasets4??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dls = get_dls(df_small, None, cols_y, splits_test, cols_d=None, ds_type=TSDatasets4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.n_subsets ## not set correctly with TSDatasets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dls = get_dls(df_main, cols_c, cols_y, splits, cols_d=cols_d, ds_type=TSDatasets3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#TSDatasets2\n",
    "dls = get_dls(df_main, cols_c, cols_y, splits, cols_d=cols_d, ds_type=TSDatasets2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _remove_augs(dls):\n",
    "    '''\n",
    "    remove augmentation transforms from dls.after_batch\n",
    "    '''\n",
    "    fs = [f for f in dls.train.after_batch.fs if not issubclass(type(f), AugTransform)]\n",
    "    print(fs)\n",
    "    for dl in dls:\n",
    "        dl.after_batch.fs.clear()\n",
    "    #dls.train.after_batch.fs.clear()\n",
    "    for f in fs: dls.add_tfms(f, 'after_batch') #fastcore 1.3.20\n",
    "    #since fastai 2.29 dls.after_batch no longer automatically equal to dls.valid.after_batch etc..\n",
    "    #for dl in dls:\n",
    "    #    dl.after_batch=dls.after_batch\n",
    "    print(dls.after_batch, dls.train.after_batch, dls.valid.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(df_small, data_params['cols_c'], data_params['cols_y'], splits_test, \n",
    "              cols_d=data_params['cols_d'])\n",
    "dls.add_tfms(Augmix(), 'after_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dl in dls:\n",
    "    print(dl.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = dls.after_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dls.after_batch, dls.train.after_batch, dls.valid.after_batch)\n",
    "_remove_augs(dls)\n",
    "print('remove')\n",
    "print(dls.after_batch, dls.train.after_batch, dls.valid.after_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_training(dls, arch=None, seed=1234, n_epochs=None, max_lr=None, wd=None, \n",
    "                 loss_fn_name=None, alpha=None, metrics=unweighted_profit, \n",
    "                 N=2, magnitude=0.1, pct_start=0.3, div_factor=25.0, aug='randaugment', **kwargs):\n",
    "    # model = ResNetSig(db.features, db.c).to(device)\n",
    "    '''\n",
    "    run a training cycle\n",
    "    parameterization important for keeping track\n",
    "    \n",
    "    '''\n",
    "    assert loss_fn_name and n_epochs, 'must pass loss_fn_name, and n_epochs'\n",
    "    \n",
    "    print(f'pct_start: {pct_start} div_factor: {div_factor}')\n",
    "    set_seed(seed, reproducible=True)\n",
    "#     model = arch(db.features, db.c)\n",
    "#     model = arch(6,1)\n",
    "    \n",
    "    model = get_mod(self.dls, arch=arch)\n",
    "    \n",
    "    _remove_augs(dls)\n",
    "    augs = RandAugment(N=N, magnitude=magnitude, verbose=True) if aug=='randaugment' else Augmix(\n",
    "        N=N, magnitude=magnitude, verbose=True) if aug=='augmix' else None\n",
    "#     augs  = Augmix(verbose=True)\n",
    "    if augs: dls.add_tfms(augs, 'after_batch')       \n",
    "    loss_fn = get_loss_fn(loss_fn_name, alpha=alpha) if not dls.classification else get_loss_fn\n",
    "    print(loss_fn)\n",
    "    \n",
    "    learn = Learner(dls, model, loss_func=loss_fn, metrics=metrics)\n",
    "\n",
    "    learn.fit_one_cycle(n_epochs, max_lr, wd=wd, pct_start=pct_start, div_factor=div_factor)\n",
    "#     learn.recorder.plot_losses()\n",
    "#     learn.recorder.plot_metrics()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#train params\n",
    "arch = InceptionTimeD\n",
    "n_epochs = 5\n",
    "max_lr = 1e-5\n",
    "wd = 0.03\n",
    "loss_fn_name = 'leaky_loss'\n",
    "alpha = 0.5\n",
    "metrics = [unweighted_profit]#, partial(unweighted_profit, threshold=0.2), \n",
    "           #partial(unweighted_profit, threshold=0.5)] #[weighted_profit, unweighted_profit_0, unweighted_profit_05]\n",
    "N = 3\n",
    "magnitude = 0.4\n",
    "# bs = [64*4, 64*4*2]  treated as a data_param\n",
    "# y_range = (-1, 1) # not sure yet about this one\n",
    "seed = 1234\n",
    "# ds_name = dataset_name #data_param inferred\n",
    "# ds_path = str(ds_full_path) #data_param\n",
    "pct_start=0.3                   #fastai default: 0.3\n",
    "div_factor = 25.0               #fastai default 25.0\n",
    "aug='augmix'\n",
    "\n",
    "#default dict?\n",
    "train_params = {'arch':arch, 'n_epochs':n_epochs, 'max_lr':max_lr, 'wd':wd, 'loss_fn_name':loss_fn_name, 'alpha':alpha,\n",
    "               'metrics':metrics, 'N':N, 'magnitude':magnitude, \n",
    "                #'bs':bs, \n",
    "                'seed':seed, \n",
    "                #'ds_name':ds_name,\n",
    "               'pct_start':pct_start, 'div_factor':div_factor, 'aug':aug}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = run_training(dls, **train_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _losses_from_recorder(r, metrics=False):\n",
    "    idx = slice(0,2) if not metrics else slice(2,None)\n",
    "    return r.values[-1][idx]\n",
    "def _minmax_values_from_recorder(r, metrics=False):\n",
    "    idx = [0,1] if not metrics else list(range(2, len(r.values[0])))\n",
    "    f = np.min if not metrics else np.max\n",
    "    return L([f(L(r.values).itemgot(i)) for i in idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_recorder_dict(recorder):\n",
    "    '''\n",
    "    return a dictionary containing train and validation loss and metrics values\n",
    "    '''\n",
    "    metrics = recorder.metrics\n",
    "#     loss_values = [recorder.losses[-1].item(), recorder.val_losses[-1].item()]\n",
    "    loss_values = _losses_from_recorder(recorder)\n",
    "#     loss_min_values = [np.min(recorder.losses), np.min(recorder.val_losses)] \n",
    "    loss_min_values = _minmax_values_from_recorder(recorder)\n",
    "    metrics_values = _losses_from_recorder(recorder, metrics=True)\n",
    "#     metrics_max_values = [np.max([m[i] for m in recorder.metrics]) for i in range(len(recorder.metrics[0]))]\n",
    "    metrics_max_values = _minmax_values_from_recorder(recorder, metrics=True)\n",
    "    recorder_keys = ['trn_loss', 'val_loss', 'trn_loss_min', 'val_loss_min', \n",
    "                     *[f'{m.name}_{i}_value' for i,m in enumerate(metrics)], *[f'{m.name}_{i}_max' for i,m in enumerate(metrics)]]\n",
    "    return dict(zip(recorder_keys, loss_values+loss_min_values+metrics_values+metrics_max_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = get_recorder_dict(learn.recorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _to_flat_dict(train_params):\n",
    "    flat_dict={}\n",
    "    for key,value in train_params.items():\n",
    "        if key=='metrics':\n",
    "            for i,_ in enumerate(listify(value)):\n",
    "                flat_dict[f'metric_{i}'] = value[i].__name__\n",
    "        #arch parameter should be string, but used to be <class model>\n",
    "        elif key=='arch' and not isinstance(value, str): flat_dict[key] = value.__name__\n",
    "        else: flat_dict[key]=value\n",
    "    return flat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _write_results(df, fn):\n",
    "    if not os.path.isfile(fn):\n",
    "        df.to_csv(fn, index=False)\n",
    "    else:\n",
    "        print('not new')\n",
    "        df_old = pd.read_csv(fn)\n",
    "        df_new = pd.concat([df_old, df], ignore_index=True, sort=False)\n",
    "#         df.to_csv(fn, index=False, mode='a', header=False)\n",
    "        df_new.to_csv(fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_to_flat_dict(train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dls, train_params, df_fn=None):\n",
    "        '''\n",
    "        could wrap the dataset parameters\n",
    "        '''\n",
    "        assert df_fn is not None, 'please specify results csv filename'\n",
    "  \n",
    "     \n",
    "        learn = run_training(dls, **train_params)\n",
    "\n",
    "        rec_dict = get_recorder_dict(learn.recorder)\n",
    "        df_dict = dict()\n",
    "        #     param_values = [arch.__name__, n_epochs, bs, seed, max_lr, wd, loss_fn_name, alpha, *[m.__name__ for m in metrics], N, magnitude]\n",
    "        #     param_key = ['architecture', 'n_epochs', 'bs', 'seed', 'max_lr', 'wd', 'loss_fn', 'alpha', *[f'metric_{i}' for i,_ in enumerate(metrics)], 'N', 'magnitude']\n",
    "        #     df_dict.update(train_params)\n",
    "        df_dict.update(_to_flat_dict(train_params))\n",
    "        df_dict.update(rec_dict)\n",
    "        df_dict['Timestamp'] = str(datetime.now())\n",
    "        ## store prediction in a separate file as tensors, but add filename\n",
    "#         self._save_preds(test=True)\n",
    "#         if self.save_model: self._save_model()\n",
    "        _write_results(pd.DataFrame([df_dict], index=[0]), df_fn)\n",
    "        return df_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fn_test = Path('~/coding/python/betting/experiments/test_results.csv').expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(df_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = Path('data/custom/bi_sample_anon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _dict_product(params):\n",
    "            values = list(itertools.product(*params.values()))\n",
    "            return [dict(zip(params.keys(), values[i])) for i in range(len(values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimal data configuation:\n",
    "- path to source(dataframe)\n",
    "- column configuration: explicit or implicit\n",
    "- splits: explicit splits or function\n",
    "- batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments Configuration:\n",
    "- data_params: dictionary, passed to ts_experiments.setup_data(df_path, columns, splits)\n",
    "- train_params: dictionary, passed to ts_experiments.setup_training: training parameters\n",
    "- (col_config: column configuration as a dictionary, passed to build_data_params)\n",
    "- (build_data_params is a helper function to create basic data_params)\n",
    "- batch_size: treated as data parameter, e.g. cannot be used as a grid search parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_preds_fn(prefix='val'):\n",
    "    return f'{prefix}_preds_{abs(hash(datetime.utcnow()))}.pt'\n",
    "\n",
    "def _get_model_fn(prefix='model'):\n",
    "    return f'{prefix}_{abs(hash(datetime.utcnow()))}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _id_from_splits(splits):\n",
    "    return '_'.join([(str((l[-1]+1)//1000)) for l in splits])\n",
    "    \n",
    "def _get_ds_id(data_params, splits):\n",
    "    return f\"{data_params['df_path'].stem}_{data_params['col_config_id']}_{_id_from_splits(splits)}\"\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_ds_id(data_params, splits=splits_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_experiment = TSExperiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([1., 10., 1, .10, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTsaiTensor(Transform):\n",
    "    def encodes(self, o:TSTensor): return tsai_tfms.TSTensor(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# def get_loss_fn_class(loss_fn_name, weight=None):\n",
    "# #     weights = tensor([1., 10., 1, .10, 1.])\n",
    "#     print(f'crosse entropy weigts {weight}')\n",
    "#     return CrossEntropyLossFlat() if weight is None else CrossEntropyLossFlat(weight=weight.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TSExperiments:\n",
    "    '''\n",
    "    Wrapper class for Timeseries modelling experiment\n",
    "    needed: data_params, train_params for setup\n",
    "    provides:\n",
    "        - `run_experiment(df_results)`: run one modelling run using `train_params`\n",
    "        - `grid_search(hypers, df_results)`: update `train_params` with each possible configuration of `hypers` \n",
    "        and run the respective experiment\n",
    "    experimental results and all necessary parameters for reproducibility is stored in `df_results`\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, save_model=False, preds_path=None, model_path=None, results_path=None):\n",
    "        ##reproducibility\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "        #self.train_params = train_params #training params can change, e.g. when running grid search\n",
    "        self.save_model = save_model## models are big\n",
    "        self.preds_path = ifnone(preds_path, './experiments/preds')\n",
    "        self.model_path = ifnone(model_path, './experiments/models')\n",
    "        self.results_path = ifnone(results_path, './experiments/results')\n",
    "        \n",
    "    def setup_data(self, data_params):\n",
    "        #read in dataframe\n",
    "        self.data_params=data_params\n",
    "        self.df_base = pd.read_csv(data_params['df_path'], nrows=data_params['nrows'])\n",
    "\n",
    "        #get continuous, discrete, and dependent columns\n",
    "        cols_c, cols_d, cols_y, splits, ss_dis = map(data_params.get, ['cols_c', 'cols_d', 'cols_y', 'splits', 'ss_dis'])\n",
    "        cols_cat, cols_cont= map(data_params.get, ['cols_cat', 'cols_cont']) ## tabular data\n",
    "        \n",
    "        prune = data_params.get('prune', None)\n",
    "        if prune is not None:\n",
    "            assert prune in ['hcodds_col', 'overodds_col']\n",
    "            prune_col = data_params.get(prune)\n",
    "            print(prune_col)\n",
    "            assert prune_col is not None, 'prune value has to be a valid columns'\n",
    "            self.df_base.drop(self.df_base[self.df_base[prune_col]<=1].index, inplace=True)\n",
    "            self.df_base.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        #get splits\n",
    "#         print(splits, callable(splits))\n",
    "        self.splits = splits(self.df_base) if callable(splits) else splits\n",
    "#         print(self.splits)\n",
    "        \n",
    "        ##store some of the data parameters for later use\n",
    "        self.bs = data_params['bs']\n",
    "        self.ds_id = _get_ds_id(data_params, self.splits)\n",
    "        self.classification = data_params.get('classification', False)\n",
    "        self.prune = prune\n",
    "        self.stats = data_params.get('stats')\n",
    "#         self.dls = get_dls(self.df_base, cols_c, cols_y, self.splits, cols_d=cols_d, bs=self.bs, \n",
    "#                            ss_dis=ss_dis)\n",
    "        self.dls = get_dls(self.df_base, cols_c, cols_y, self.splits, cols_d=cols_d, bs=self.bs, \n",
    "                           ss_dis=ss_dis, cols_cont=cols_cont, cols_cat=cols_cat, ds_type=TSDatasets5,\n",
    "                          classification=self.classification, stats=self.stats)\n",
    "        \n",
    "        \n",
    "    def setup_training(self, train_params):\n",
    "        assert hasattr(self, 'data_params'), 'setup_data first'\n",
    "        self.train_params = train_params\n",
    "        self.train_params['bs']=self.bs\n",
    "        self.train_params['ds_id'] = self.ds_id\n",
    "        self.train_params['classification'] = self.classification\n",
    "        self.train_params['prune'] = self.prune\n",
    "        \n",
    "        if self.train_params['classification']:\n",
    "            assert self.train_params['loss_fn_name'] in [\"crossentropy\", \"rww\"]\n",
    "    \n",
    " \n",
    "    def _save_preds(self, test=False):\n",
    "    #         val_preds_fn = _get_preds_fn()\n",
    "        preds_fn = _get_preds_fn()\n",
    "        preds, y_true = self.learn.get_preds(1)\n",
    "        torch.save(preds, Path(self.preds_path)/preds_fn)\n",
    "        self.df_dict.update({'val_preds':preds_fn})\n",
    "        if len(list(self.dls))==3:\n",
    "            preds_fn = _get_preds_fn('test')\n",
    "            preds, y_true = self.learn.get_preds(2)\n",
    "            torch.save(preds, Path(self.preds_path)/preds_fn)\n",
    "            self.df_dict.update({'test_preds':preds_fn})\n",
    "    \n",
    "    def _save_model(self):\n",
    "    #         val_preds_fn = _get_preds_fn()\n",
    "        model_fn = self.model_fn\n",
    "        self.learn.save(model_fn)\n",
    "        self.df_dict.update({'model_fn':f'{self.learn.model_dir}/{model_fn}.pth'})\n",
    "\n",
    "    \n",
    "    \n",
    "    def run_training(self, arch=None, seed=1234, n_epochs=None, max_lr=None, wd=None, \n",
    "                     loss_fn_name=None, alpha=None, metrics=unweighted_profit, \n",
    "                     N=2, magnitude=0.1, pct_start=0.3, div_factor=25.0, aug='randaugment', \n",
    "                     verbose=False, weight=None, save_best=False, aug_params=None, **kwargs):\n",
    "        # model = ResNetSig(db.features, db.c).to(device)\n",
    "        '''\n",
    "        run a training cycle\n",
    "        parameterization important for keeping track\n",
    "        '''\n",
    "        assert loss_fn_name and n_epochs, 'must pass loss_fn_name, and n_epochs'\n",
    "\n",
    "        print(f'pct_start: {pct_start} div_factor: {div_factor}')\n",
    "       \n",
    "        ## reset dls.rng --> consistent shuffling\n",
    "        \n",
    "#         huffle_fn(self, idxs): return self.rng.sample(idxs, len(idxs))\n",
    "#         print(self.)\n",
    "    #     model = arch(db.features, db.c)\n",
    "       \n",
    "\n",
    "        _remove_augs(self.dls)\n",
    "        if aug=='randaugment':\n",
    "            tfms = None\n",
    "            if aug_params is not None:\n",
    "                if aug_params=='noise': tfms = all_noise_augs(magnitude=magnitude)\n",
    "                if aug_params=='erasing': tfms = all_erasing_augs(magnitude=magnitude)\n",
    "                if aug_params=='zoom': tfms = all_zoom_augs(magnitude=magnitude)\n",
    "                if aug_params=='nodim': \n",
    "                    tfms = all_augs(magnitude=magnitude)\n",
    "                    tfms = [tfm for tfm in tfms if not isinstance(tfm, Dimout)]\n",
    "            augs=RandAugment(N=N, magnitude=magnitude, verbose=verbose, tfms=tfms)\n",
    "#         elif aug=='augmix': augs=Augmix(N=N, magnitude=magnitude, verbose=verbose)\n",
    "        elif aug=='rand_tsai':\n",
    "            augs = [ToTsaiTensor(), tsai_tfms.RandAugment(tsai_tfms.all_TS_randaugs[:2], N=N, M=int(magnitude*10))]\n",
    "        elif aug=='augmix':\n",
    "            _augsmixtype=AugmixSS if kwargs.get('augmixss') is not None else Augmix\n",
    "            augs=_augmixtype(N=N, magnitude=magnitude, verbose=verbose)\n",
    "            print(f'augmix order {augs.order}')\n",
    "        else:\n",
    "            print(f'no augmentation with value {aug}')\n",
    "            augs=None\n",
    "        print(augs)\n",
    "        print(augs is None)\n",
    "        if augs: \n",
    "            self.dls.add_tfms(augs, 'after_batch')\n",
    "            if not is_listy(augs): augs=[augs]\n",
    "            for aug in augs:\n",
    "                aug.setup(self.dls[0])\n",
    "            ## Pipeline.add does not reorder the transforms, but we want the augmentation before the standardisation\n",
    "            self.dls.after_batch.fs = self.dls.after_batch.fs.sorted(key='order')\n",
    "        cbs = [SaveModelCallback(fname=f'{self.model_fn}_best_val'),\n",
    "               #SaveModelCallback(fname=f'{self.model_fn}_best_combo_profit', monitor='combo_profit'),\n",
    "              ] if save_best else None\n",
    "            \n",
    "#         loss_fn = get_loss_fn(loss_fn_name, alpha=alpha)\n",
    "        loss_fn = get_loss_fn(loss_fn_name, alpha=alpha) if not self.dls.classification else get_loss_fn_class(\n",
    "            loss_fn_name, weight=weight)\n",
    "        print(loss_fn)\n",
    "        \n",
    "         \n",
    "        set_seed(seed)\n",
    "        self.dls.train.rng = random.Random(random.randint(0,2**32-1))\n",
    "        pretrained = None\n",
    "        if self.train_params.get('pretrained') is not None:\n",
    "            pretrained = Path(self.model_path)/'pretrained'/self.train_params.get('pretrained')\n",
    "        \n",
    "#         model = arch(self.dls.n_channels, self.dls.n_targets)\n",
    "        model = get_mod(self.dls, arch=self.train_params['arch'], dropout=self.train_params.get('dropout'),\n",
    "                       fc_dropout=self.train_params.get('fc_dropout'), pretrained=pretrained)\n",
    "        learn = Learner(self.dls, model, loss_func=loss_fn, metrics=metrics, model_dir=self.model_path,\n",
    "                       wd=wd, cbs=cbs)\n",
    "        print(learn.dls.after_batch)\n",
    "        \n",
    "#         print(f'wd: {wd} {learn.wd}')\n",
    "        learn.fit_one_cycle(n_epochs, max_lr, wd=wd, pct_start=pct_start, div=div_factor)\n",
    "#         learn.fit_one_cycle(n_epochs, max_lr, wd=wd)\n",
    "    #     learn.recorder.plot_losses()\n",
    "    #     learn.recorder.plot_metrics()\n",
    "        return learn\n",
    "\n",
    "\n",
    "    def run_experiment(self, df_fn=None):\n",
    "        '''\n",
    "        could wrap the dataset parameters\n",
    "        '''\n",
    "        assert df_fn is not None, 'please specify results csv filename'\n",
    "        self.model_fn = _get_model_fn()\n",
    "  \n",
    "        self.learn = self.run_training(**self.train_params)\n",
    "#         rec_dict = get_recorder_dict(self.learn.recorder)\n",
    "        self.df_dict = dict()\n",
    "        self.df_dict.update(_to_flat_dict(train_params))\n",
    "        self.df_dict.update(get_recorder_dict(self.learn.recorder))\n",
    "        self.df_dict['Timestamp'] = str(datetime.now())\n",
    "        ## store prediction in a separate file as tensors, but add filename\n",
    "        self._save_preds(test=False)\n",
    "        if self.save_model: self._save_model()\n",
    "        if self.save_model or self.train_params.get('save_best'):\n",
    "            self.df_dict.update({'model_fn':f'{self.learn.model_dir}/{self.model_fn}.pth'})\n",
    "        _write_results(pd.DataFrame([self.df_dict], index=[0]), Path(self.results_path)/df_fn)\n",
    "#         return df_dict\n",
    "    \n",
    "    \n",
    "   \n",
    "    def run_grid_search(self, hypers:dict, df_results_fn=None):\n",
    "        '''\n",
    "        run hyper parameter grid search, note that this changes self.train_params\n",
    "        '''\n",
    "        hyper_configs = _dict_product(hypers) #list of dictionaries\n",
    "        if hasattr(self, 'hyper_configs'): self.hyper_configs+=hyper_configs\n",
    "        else: self.hyper_configs = hyper_configs\n",
    "        for config in hyper_configs:\n",
    "            self.train_params.update(config)\n",
    "            print(self.train_params)\n",
    "            self.run_experiment(df_fn=df_results_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def build_data_params(df_path, trn_end=None, val_end=None, test_end=None, splitter_fn=TSSplitter(), \n",
    "                      col_config=None, col_fn=None, bs=64, nrows=None, ss_dis=True, classification=False):\n",
    "#     assert col_config or col_fn, 'need to pass either cont. cols and y cols, or a col_fn'\n",
    "\n",
    "    assert col_config, 'need to pass columns configuration'\n",
    "    \n",
    "    if trn_end and val_end:\n",
    "        splits=L(L(range(trn_end)), L(range(trn_end, val_end)))\n",
    "        if test_end: splits.append(L(range(val_end, test_end)))\n",
    "    else:\n",
    "        splits = splitter_fn\n",
    "    \n",
    "    cols_c, cols_d, cols_y, cols_config_id, cols_cont, cols_cat, prune, hcodds_col = map(\n",
    "        col_config.get, ['cols_c', 'cols_d', 'cols_y', 'id', 'cols_cont', 'cols_cat', 'prune', 'hcodds_col'])\n",
    "\n",
    "#     dataset_name = f'{df\n",
    "    \n",
    "    data_params = defaultdict(lambda:None, {'df_path':df_path, 'splits':splits, 'col_config_id':cols_config_id, \n",
    "                                            'cols_c':cols_c, 'cols_d':cols_d, 'cols_y':cols_y, 'cols_cont':cols_cont,\n",
    "                                             'cols_cat':cols_cat, 'hcodds_col': hcodds_col, 'bs':bs, 'prune':prune, \n",
    "                                            'nrows':nrows, 'ss_dis':ss_dis,'classification':classification})\n",
    "#                'ds_full_path':ds_full_path, \n",
    "                 #'dataset_name':dataset_id, \n",
    "              \n",
    "    return data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## column configuration as dict, eventually move to separate configuration file\n",
    "col_config = defaultdict(lambda:None)\n",
    "col_config['cols_c'] = [[f'x{i}_{j}' for j in range(10)] for i in [0,1,3,4]]\n",
    "col_config['cols_d'] = [[f'x{i}_{j}' for j in range(10)] for i in [2,5]]\n",
    "col_config['cols_y']= 'y0'\n",
    "col_config['id']='anon10hc_4c_2d_y'  ## put it all in one config file with this unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = build_data_params(df_path, cols_config=col_config, nrows=10000, trn_end=5000, val_end=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params['col_config_id'], data_params['bs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_experiment = TSExperiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_experiment.setup_data(data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {'max_lr':[3e-5], 'n_epochs':[7], 'N':[3], 'magnitude':[0.4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_experiment.setup_training(train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_experiment.run_grid_search(hypers, df_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(df_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 99\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_experiment.train_params['cols_config_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_arch(arch:str, with_discrete=False):\n",
    "    if arch.lower()=='inception': return InceptionTimeSgm if not with_discrete else InceptionTimeD\n",
    "    elif arch.lower()=='resnet': return 'ResNet not implemented'\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_CONFIG = 'config2.json'\n",
    "\n",
    "@call_parse\n",
    "def main(n_epochs:Param(help=\"n_epochs list\", nargs='+', type=int)=[10],\n",
    "         max_lr:Param(help=\"max_lr list\", nargs='+', type=float)=[1e-5],\n",
    "         wd:Param(help=\"wd (weight decay): hyperparameter list of floats\", nargs='+', type=float)=[0.03],\n",
    "         div_factor:Param(help=\"div_factor hyperparameter list\", nargs='+', type=float)=[25.0],\n",
    "         seed:Param(help=\"seed hyperparameter list\", nargs='+', type=int)=[1234],\n",
    "         N:Param(help=\"N hyperparameter list\", nargs='+', type=int)=[3],\n",
    "         magnitude:Param(help=\"augmentation magnitude: hyperparameter list of floats\", nargs='+', type=float)=[0.3],\n",
    "         alpha:Param(help=\"alpha hyperparameter list\", nargs='+', type=float)=[0.5],\n",
    "         aug:Param(help=\"augmentation policy\", choices=[None, 'randaugment', 'augmix'], type=str)=None,\n",
    "         nrows:Param(help=\"n_epochs list\", type=int)=None,\n",
    "         bs:Param(help=\"batch size\", type=int)=128,\n",
    "         trn_end:Param(help=\"n_epochs list\", type=int)=None,\n",
    "         val_end:Param(help=\"n_epochs list\", type=int)=None,\n",
    "         test_end:Param(help=\"n_epochs list\", type=int)=None,\n",
    "         df_fn:Param(help=\"dataframe filename\", type=str)='bi_sample_anon.csv',         \n",
    "         df_dir:Param(help=\"dataframe dir\", type=str)='./data/custom',\n",
    "         df_results:Param(help=\"results dataframe filename\", type=str)='results_script.csv',\n",
    "         config_fn:Param(help=\"json column configuration filename\", type=str)=COL_CONFIG,    \n",
    "         config_id:Param(help=\"column configuration id\", type=str)='anon2hc_4c_2d_y',\n",
    "         arch:Param(help=\"model architecture\", choices=['inception', 'resnet'], type=str)='inception',\n",
    "         upper:Param(\"Convert to uppercase?\", bool_arg)=False):\n",
    "#     print(msg.upper() if upper else msg)\n",
    "    \n",
    "   \n",
    "    \n",
    "    train_params['aug']=aug\n",
    "    \n",
    "    df_path=Path(df_dir)/df_fn\n",
    "    print(df_path)\n",
    "    \n",
    "    col_config=read_config(config_id, config_fn)\n",
    "    data_params = build_data_params(df_path, col_config=col_config, nrows=nrows, trn_end=trn_end, val_end=val_end,\n",
    "                                   test_end=test_end, bs=bs)\n",
    "#     print(data_params)\n",
    "\n",
    "    train_params['metrics']=[unweighted_profit, unweighted_profit_05]\n",
    "    train_params['arch']=_get_arch(arch, col_config['cols_d'] is not None)\n",
    "    ts_experiment = TSExperiments()\n",
    "    ts_experiment.setup_data(data_params)\n",
    "    ts_experiment.setup_training(train_params)\n",
    "                                 \n",
    "    hypers = {'n_epochs': n_epochs, 'max_lr':max_lr, 'wd':wd, 'seed': seed, 'div_factor':div_factor,\n",
    "             'N':N, 'magnitude':magnitude}\n",
    "    print(hypers)\n",
    "    \n",
    "    ts_experiment.run_grid_search(hypers, df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ttools]",
   "language": "python",
   "name": "conda-env-ttools-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
