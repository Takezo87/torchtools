---

title: Title

keywords: fastai
sidebar: home_sidebar

summary: "summary"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_models.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="models">models<a class="anchor-link" href="#models">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;1.4.0&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">2.</span><span class="p">]))</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="o">-</span><span class="mi">1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([-0.7616])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@gmail.com based on:</span>

<span class="c1"># Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... &amp; Petitjean, F. (2019). InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.</span>
<span class="c1"># Official InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime</span>


<span class="k">def</span> <span class="nf">noop</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">shortcut</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="n">c_out</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> 
                           <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">c_out</span><span class="p">)])</span>
    
<span class="k">class</span> <span class="nc">Inception</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c_in</span><span class="p">,</span> <span class="n">bottleneck</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">ks</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">nb_filters</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="n">bottleneck</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">bottleneck</span> <span class="ow">and</span> <span class="n">c_in</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">noop</span>
        <span class="n">mts_feat</span> <span class="o">=</span> <span class="n">bottleneck</span> <span class="ow">or</span> <span class="n">c_in</span>
        <span class="n">conv_layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">kss</span> <span class="o">=</span> <span class="p">[</span><span class="n">ks</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
        <span class="c1"># ensure odd kss until nn.Conv1d with padding=&#39;same&#39; is available in pytorch 1.3</span>
        <span class="n">kss</span> <span class="o">=</span> <span class="p">[</span><span class="n">ksi</span> <span class="k">if</span> <span class="n">ksi</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">ksi</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">ksi</span> <span class="ow">in</span> <span class="n">kss</span><span class="p">]</span>  
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kss</span><span class="p">)):</span>
            <span class="n">conv_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">mts_feat</span><span class="p">,</span> <span class="n">nb_filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kss</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="n">kss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">conv_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="n">nb_filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">nb_filters</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
            <span class="n">out_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">out</span> <span class="o">=</span> <span class="n">out_</span>
            <span class="k">else</span><span class="p">:</span> <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">out</span><span class="p">,</span> <span class="n">out_</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">mp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">))</span>
        <span class="n">inc_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">out</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">inc_out</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">InceptionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">c_in</span><span class="p">,</span><span class="n">bottleneck</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">ks</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span><span class="n">nb_filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">residual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">depth</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">depth</span> <span class="o">=</span> <span class="n">depth</span>

        <span class="c1">#inception &amp; residual layers</span>
        <span class="n">inc_mods</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">res_layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
            <span class="n">inc_mods</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">Inception</span><span class="p">(</span><span class="n">c_in</span> <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">nb_filters</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">bottleneck</span><span class="o">=</span><span class="n">bottleneck</span> <span class="k">if</span> <span class="n">d</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span><span class="n">ks</span><span class="o">=</span><span class="n">ks</span><span class="p">,</span>
                          <span class="n">nb_filters</span><span class="o">=</span><span class="n">nb_filters</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="ow">and</span> <span class="n">d</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">res_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shortcut</span><span class="p">(</span><span class="n">c_in</span> <span class="k">if</span> <span class="n">res</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">nb_filters</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">nb_filters</span> <span class="o">*</span> <span class="mi">4</span><span class="p">))</span>
                <span class="n">res</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span> <span class="n">res_layer</span> <span class="o">=</span> <span class="n">res_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inc_mods</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">inc_mods</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">res_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">res_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">depth</span><span class="p">)):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inc_mods</span><span class="p">[</span><span class="n">d</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="ow">and</span> <span class="n">d</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">res_layers</span><span class="p">[</span><span class="n">d</span><span class="p">](</span><span class="n">res</span><span class="p">)</span>
                <span class="n">x</span> <span class="o">+=</span> <span class="n">res</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">x</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="noop" class="doc_header"><code>noop</code><a href="https://github.com/Takezo87/torchtools/tree/master/torchtools/models.py#L16" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>noop</code>(<strong><code>x</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="shortcut" class="doc_header"><code>shortcut</code><a href="https://github.com/Takezo87/torchtools/tree/master/torchtools/models.py#L19" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>shortcut</code>(<strong><code>c_in</code></strong>, <strong><code>c_out</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Inception" class="doc_header"><code>class</code> <code>Inception</code><a href="https://github.com/Takezo87/torchtools/tree/master/torchtools/models.py#L23" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Inception</code>(<strong><code>c_in</code></strong>, <strong><code>bottleneck</code></strong>=<em><code>32</code></em>, <strong><code>ks</code></strong>=<em><code>40</code></em>, <strong><code>nb_filters</code></strong>=<em><code>32</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="InceptionBlock" class="doc_header"><code>class</code> <code>InceptionBlock</code><a href="https://github.com/Takezo87/torchtools/tree/master/torchtools/models.py#L54" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>InceptionBlock</code>(<strong><code>c_in</code></strong>, <strong><code>bottleneck</code></strong>=<em><code>32</code></em>, <strong><code>ks</code></strong>=<em><code>40</code></em>, <strong><code>nb_filters</code></strong>=<em><code>32</code></em>, <strong><code>residual</code></strong>=<em><code>True</code></em>, <strong><code>depth</code></strong>=<em><code>6</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="InceptionTime" class="doc_header"><code>class</code> <code>InceptionTime</code><a href="https://github.com/Takezo87/torchtools/tree/master/torchtools/models.py#L90" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>InceptionTime</code>(<strong><code>c_in</code></strong>, <strong><code>c_out</code></strong>, <strong><code>bottleneck</code></strong>=<em><code>32</code></em>, <strong><code>ks</code></strong>=<em><code>40</code></em>, <strong><code>nb_filters</code></strong>=<em><code>32</code></em>, <strong><code>residual</code></strong>=<em><code>True</code></em>, <strong><code>depth</code></strong>=<em><code>6</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>

<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="InceptionTimeSgm" class="doc_header"><code>class</code> <code>InceptionTimeSgm</code><a href="https://github.com/Takezo87/torchtools/tree/master/torchtools/models.py#L105" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>InceptionTimeSgm</code>(<strong><code>n_in</code></strong>, <strong><code>n_out</code></strong>) :: <code>Module</code></p>
</blockquote>
<p>add a sigmoid layer to InceptionTime to get the ouput in a certain range</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">InceptionTimeSgm</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>InceptionTimeSgm(
  (inc): InceptionTime(
    (block): InceptionBlock(
      (inc_mods): ModuleList(
        (0): Inception(
          (conv_layers): ModuleList(
            (0): Conv1d(6, 32, kernel_size=(39,), stride=(1,), padding=(19,))
            (1): Conv1d(6, 32, kernel_size=(19,), stride=(1,), padding=(9,))
            (2): Conv1d(6, 32, kernel_size=(9,), stride=(1,), padding=(4,))
          )
          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (conv): Conv1d(6, 32, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (1): Inception(
          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))
          (conv_layers): ModuleList(
            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))
            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))
            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))
          )
          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (2): Inception(
          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))
          (conv_layers): ModuleList(
            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))
            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))
            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))
          )
          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (3): Inception(
          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))
          (conv_layers): ModuleList(
            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))
            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))
            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))
          )
          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (4): Inception(
          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))
          (conv_layers): ModuleList(
            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))
            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))
            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))
          )
          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
        (5): Inception(
          (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,))
          (conv_layers): ModuleList(
            (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,))
            (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,))
            (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,))
          )
          (maxpool): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)
          (conv): Conv1d(128, 32, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (act): ReLU()
        )
      )
      (res_layers): ModuleList(
        (0): None
        (1): None
        (2): Sequential(
          (0): Conv1d(6, 128, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): None
        (4): None
        (5): Sequential(
          (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))
          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (act): ReLU()
    )
    (gap): AdaptiveAvgPool1d(output_size=1)
    (fc): Linear(in_features=128, out_features=1, bias=True)
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
</div>
 

